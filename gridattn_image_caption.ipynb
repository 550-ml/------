{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 实战案例：基于注意力的图像描述模型（ARCTIC）\n",
    "\n",
    "## 模型训练流程\n",
    "\n",
    "![模型训练的一般流程](img/cr-traning_process.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "在现代的深度学习框架基础下，模型训练的一般流程包括读取数据、前馈计算、计算损失、更新参数、选择模型五个步骤。每个步骤需要实现相应的模块。\n",
    "\n",
    "- 在读取数据阶段，我们首先需要下载数据集，然后对整理数据集的格式，以方便接下来构造数据集类，最后在数据集类的基础上构建能够按批次产生训练、验证和测试数据的对象。\n",
    "\n",
    "- 在前馈计算阶段，我们需要实现具体的模型，使得模型能够根据输入产生相应的输出。\n",
    "\n",
    "- 在计算损失阶段，我们需要将模型输出和预期输出进行对比，实现损失函数。\n",
    "\n",
    "- 在更新参数阶段，我们需要给出具体的参数更新方法，即优化方法；由于现代深度学习框架能够自动计算参数梯度，并实现了绝大多数优化方法，我们通常只需要从中进行选择即可。\n",
    "\n",
    "- 在选择模型阶段，我们需要实现具体的评估指标，选出在验证集上表现最优的模型参数。\n",
    "\n",
    "下面，我们将按照这个次序介绍一个图像编码器为CNN网格表示提取器、文本解码器为RNN+注意力的图像描述方法的具体实现。我们的实现大体上是在复现ARCTIC模型，但是在细节上有一些改变，下面的实现过程会对这些改变做具体说明。此外，[链接](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)给出了一个更接近原始ARCTIC模型的代码库，非常推荐大家阅读。本节的部分代码也是受到该代码库的启发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 读取数据\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "- 我们使用的数据集为flickr8k([下载地址](https://www.kaggle.com/adityajn105/flickr8k))。\n",
    "- 下载解压后，将其图片放在指定目录(本代码中将该目录设置为../data/flickr8k)下的images文件夹里。\n",
    "- 数据集包括8000张图片，每张图片对应5个句子描述。\n",
    "- 数据集划分采用Karpathy提供的方法([下载地址](http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip))，下载解压后，将其中的dataset_flickr8k.json文件拷贝到指定目录下。该划分方法将数据集分成3个子集：6,000张图片和其对应句子描述组成训练集，1,000张图片和描述为验证集，剩余的1,000张图片和描述为测试集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 整理数据集\n",
    "\n",
    "数据集下载完成后，我们需要对其进行处理，以适合之后构造的PyTorch数据集类读取。\n",
    "- 对于文本描述，我们首先构建词典，然后根据词典将文本描述转化为向量。\n",
    "- 对于图像，我们这里仅记录文件路径。\n",
    "- - 如果机器的内存和硬盘空间就比较大，这里也可以将图片读取并处理成三维数组，这样在模型训练和测试的阶段，就不需要再直接读取图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import random \n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def create_dataset(dataset='deepfashion',\n",
    "                   captions_per_image=5, \n",
    "                   min_word_count=5, \n",
    "                   max_len=30):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        dataset：数据集名称\n",
    "        captions_per_image：每张图片对应的文本描述数\n",
    "        min_word_count：仅考虑在数据集中（除测试集外）出现5次的词\n",
    "        max_len：文本描述包含的最大单词数，如果文本描述超过该值，则截断\n",
    "    输出：\n",
    "        一个词典文件： vocab.json\n",
    "        三个数据集文件： train_data.json、 val_data.json、 test_data.json\n",
    "    \"\"\"\n",
    "\n",
    "    karpathy_json_path='../data/%s/test_captions.json' % dataset  # 训练集的json文件\n",
    "    image_folder='../data/%s/images' % dataset  # 图片folder\n",
    "    output_folder='../data/%s' % dataset  # 输出文件夹\n",
    "\n",
    "    with open(karpathy_json_path, 'r') as j:\n",
    "        data = json.load(j)  # data是获取的json文件, data是一个字典\n",
    "    \n",
    "    image_paths = defaultdict(list)\n",
    "    image_captions = defaultdict(list)\n",
    "    vocab = Counter()\n",
    "\n",
    "    for img in data['images']:\n",
    "        split = img['split']\n",
    "        captions = []\n",
    "        for c in img['sentences']:\n",
    "            # 更新词频，测试集在训练过程中是未见数据集，不能统计\n",
    "            if split != 'test':\n",
    "                vocab.update(c['tokens'])\n",
    "            # 不统计超过最大长度限制的词\n",
    "            if len(c['tokens']) <= max_len:\n",
    "                captions.append(c['tokens'])\n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(image_folder, img['filename'])\n",
    "        \n",
    "        image_paths[split].append(path)\n",
    "        image_captions[split].append(captions)\n",
    "\n",
    "    # 创建词典，增加占位标识符<pad>、未登录词标识符<unk>、句子首尾标识符<start>和<end>\n",
    "    words = [w for w in vocab.keys() if vocab[w] > min_word_count]\n",
    "    vocab = {k: v + 1 for v, k in enumerate(words)}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = len(vocab)\n",
    "    vocab['<start>'] = len(vocab)\n",
    "    vocab['<end>'] = len(vocab)\n",
    "\n",
    "    # 存储词典\n",
    "    with open(os.path.join(output_folder, 'vocab.json'), 'w') as fw:\n",
    "        json.dump(vocab, fw)\n",
    "\n",
    "    # 整理数据集\n",
    "    for split in image_paths:\n",
    "        imgpaths = image_paths[split]\n",
    "        imcaps = image_captions[split]\n",
    "        \n",
    "        enc_captions = []\n",
    "\n",
    "        for i, path in enumerate(imgpaths):\n",
    "            # 合法性检查，检查图像是否可以被解析\n",
    "            img = Image.open(path) \n",
    "            # 如果该图片对应的描述数量不足，则补足\n",
    "            if len(imcaps[i]) < captions_per_image:\n",
    "                captions = imcaps[i] + \\\n",
    "                    [random.choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "            # 如果该图片对应的描述数量超了，则随机采样\n",
    "            else:\n",
    "                captions = random.sample(imcaps[i], k=captions_per_image)\n",
    "            assert len(captions) == captions_per_image\n",
    "            \n",
    "            for j, c in enumerate(captions):\n",
    "                # 对文本描述进行编码\n",
    "                enc_c = [vocab['<start>']] + [vocab.get(word, vocab['<unk>']) for word in c] + [vocab['<end>']] \n",
    "                enc_captions.append(enc_c)\n",
    "        # 合法性检查\n",
    "        assert len(imgpaths) * captions_per_image == len(enc_captions)\n",
    "        \n",
    "        # 存储数据\n",
    "        data = {'IMAGES': imgpaths, \n",
    "                'CAPTIONS': enc_captions}\n",
    "        with open(os.path.join(output_folder, split + '_data.json'), 'w') as fw:\n",
    "            json.dump(data, fw)\n",
    "\n",
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "在调用该函数生成需要的格式的数据集文件之后，我们可以展示其中一条数据，简单验证下数据的格式是否和我们预想的一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取词典和验证集\n",
    "with open('../data/flickr8k/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "vocab_idx2word = {idx:word for word,idx in vocab.items()}\n",
    "with open('../data/flickr8k/val_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 展示第12张图片，其对应的文本描述序号是60到64\n",
    "content_img = Image.open(data['IMAGES'][12])\n",
    "plt.imshow(content_img)\n",
    "for i in range(5):\n",
    "    print(' '.join([vocab_idx2word[word_idx] for word_idx in data['CAPTIONS'][12*5+i]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 定义数据集类\n",
    "\n",
    "在准备好的数据集的基础上，我们需要进一步定义PyTorch Dataset类，以使用PyTorch DataLoader类按批次产生数据。PyTorch中仅预先定义了图像、文本和语音的单模态任务中常见的数据集类。因此，我们需要定义自己的数据集类。\n",
    "\n",
    "在PyTorch中定义数据集类非常简单，仅需要继承torch.utils.data.Dataset类，并实现\\_\\_getitem\\_\\_和\\_\\_len\\_\\_两个函数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import random \n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "def create_dataset(dataset='deepfashion',\n",
    "                   captions_per_image=1, \n",
    "                   min_word_count=3, \n",
    "                   max_len=100):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        dataset：数据集名称\n",
    "        captions_per_image：每张图片对应的文本描述数\n",
    "        min_word_count：仅考虑在数据集中（除测试集外）出现5次的词\n",
    "        max_len：文本描述包含的最大单词数，如果文本描述超过该值，则截断\n",
    "    输出：\n",
    "        一个词典文件： vocab.json\n",
    "        三个数据集文件： train_data.json、 val_data.json、 test_data.json\n",
    "    \"\"\"\n",
    "\n",
    "    karpathy_json_path='../data/%s/train_captions.json' % dataset  # 训练集的json文件\n",
    "    image_folder='../data/%s/images' % dataset  # 图片folder\n",
    "    output_folder='../data/%s/' % dataset  # 输出文件夹\n",
    "\n",
    "    with open(karpathy_json_path, 'r') as j:\n",
    "        data = json.load(j)  # data是获取的json文件,data是一个字典！！！！！！！！\n",
    "    \n",
    "    image_paths = defaultdict(list)  # 图片路径\n",
    "    image_captions = defaultdict(list)  # 图片标注\n",
    "    vocab = Counter()  # 每个词语出现的次数\n",
    "\n",
    "    # 假设data是你的数据集字典\n",
    "    train_keys, val_keys = train_test_split(list(data.keys()), test_size=0.1, random_state=42)\n",
    "    for img in data.keys():\n",
    "        captions = []\n",
    "        sentences = data[f'{img}']  # 1个句子\n",
    "        words = re.findall(r\"[\\w'-]+|[.,!?;]\", sentences)  # 将文本分割成单词和标点符号\n",
    "        vocab.update(words)\n",
    "        if len(words) <= max_len:\n",
    "            captions.append(words)\n",
    "        path = os.path.join(image_folder, img)\n",
    "\n",
    "        if img in train_keys:\n",
    "            image_paths['train'].append(path)\n",
    "            image_captions['train'].append(captions)\n",
    "        else:\n",
    "            image_paths['val'].append(path)\n",
    "            image_captions['val'].append(captions)\n",
    "\n",
    "    karpathy_json_path_t='../data/%s/test_captions.json' % dataset  # 训练集的json文件\n",
    "    with open(karpathy_json_path_t, 'r') as j:\n",
    "        data = json.load(j)  # data是获取的json文件,data是一个字典！！！！！！！！\n",
    "    for img in data.keys():\n",
    "        captions = []\n",
    "        sentences = data[f'{img}']  # 5个句子\n",
    "        words = re.findall(r\"[\\w'-]+|[.,!?;]\", sentences)\n",
    "        if len(words) <= max_len:\n",
    "                captions.append(words)\n",
    "        path = os.path.join(image_folder, img)\n",
    "        \n",
    "        image_paths['test'].append(path)\n",
    "        image_captions['test'].append(captions)\n",
    "        \n",
    "    # 创建词典，增加占位标识符<pad>、未登录词标识符<unk>、句子首尾标识符<start>和<end>\n",
    "    words = [w for w in vocab.keys() if vocab[w] > min_word_count]\n",
    "    vocab = {k: v + 1 for v, k in enumerate(words)}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = len(vocab)\n",
    "    vocab['<start>'] = len(vocab)\n",
    "    vocab['<end>'] = len(vocab)\n",
    "\n",
    "# 储存词典\n",
    "    try:\n",
    "        with open(os.path.join(output_folder, 'vocab.json'), 'w') as fw:\n",
    "            json.dump(vocab, fw)\n",
    "        print(\"Writing to 'vocab.json' successful!\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while writing to 'vocab.json':\", e)\n",
    "\n",
    "    # 整理数据集\n",
    "    for split in image_paths:\n",
    "        # print(split)\n",
    "        imgpaths = image_paths[split]  # 图片加载\n",
    "        imcaps = image_captions[split]\n",
    "        print(imcaps)\n",
    "        enc_captions = []\n",
    "\n",
    "        for i, path in enumerate(imgpaths):\n",
    "            # 合法性检查，检查图像是否可以被解析\n",
    "            img = Image.open(path) \n",
    "            # 如果该图片对应的描述数量不足，则补足\n",
    "            if len(imcaps[i]) < captions_per_image:\n",
    "                captions = imcaps[i] + [''] * (captions_per_image - len(imcaps[i]))\n",
    "            # 如果该图片对应的描述数量超了，则随机采样\n",
    "            else:\n",
    "                captions = random.sample(imcaps[i], k=captions_per_image)\n",
    "            assert len(captions) == captions_per_image\n",
    "            \n",
    "            for j, c in enumerate(captions):\n",
    "                # 对文本描述进行编码\n",
    "                enc_c = [vocab['<start>']] + [vocab.get(word, vocab['<unk>']) for word in c] + [vocab['<end>']] \n",
    "                enc_captions.append(enc_c)\n",
    "        # 合法性检查\n",
    "        assert len(imgpaths) * captions_per_image == len(enc_captions)\n",
    "        \n",
    "        # 存储数据\n",
    "        data = {'IMAGES': imgpaths, \n",
    "                'CAPTIONS': enc_captions}\n",
    "        with open(os.path.join(output_folder, split + '_data.json'), 'w') as fw:\n",
    "            json.dump(data, fw)\n",
    "\n",
    "\n",
    "#         for c in sentence:  # c是一个句子\n",
    "#             words = c.split()\n",
    "#             vocab.update(words)\n",
    "#             # print(vocab)\n",
    "#             if len(words) <= max_len:\n",
    "#                 captions.append(words)\n",
    "#                 # print(captions)\n",
    "#         if len(captions)==0:\n",
    "#             continue\n",
    "        \n",
    "#         path = os.path.join(image_folder, img)\n",
    "        \n",
    "#         if img in train_keys:\n",
    "#             image_paths['train'].append(path)\n",
    "#             image_captions['train'].append(captions)\n",
    "#         else:\n",
    "#             image_paths['val'].append(path)\n",
    "#             image_captions['val'].append(captions)\n",
    "#     # print(image_paths['valid'])\n",
    "    \n",
    "#     # 测试集\n",
    "#     karpathy_json_path_t='../data/%s/test_captions.json' % dataset  # 训练集的json文件\n",
    "#     with open(karpathy_json_path_t, 'r') as j:\n",
    "#         data = json.load(j)  # data是获取的json文件,data是一个字典！！！！！！！！\n",
    "#     for img in data.keys():\n",
    "#         captions = []\n",
    "#         sentences = data[f'{img}']  # 5个句子\n",
    "#         sentence = sentences.split('.')  # sentence 就是索引\n",
    "#         for c in sentence:  # c是一个句子\n",
    "#             words = c.split()\n",
    "#             if len(words) <= max_len:\n",
    "#                 captions.append(words)\n",
    "#         if len(captions)==0:\n",
    "#             continue\n",
    "#         path = os.path.join(image_folder, img)\n",
    "#         image_paths['test'].append(path)\n",
    "#         image_captions['test'].append(captions)\n",
    "#     # print(image_paths['test'])\n",
    "    \n",
    "#     # 创建词典，增加占位标识符<pad>、未登录词标识符<unk>、句子首尾标识符<start>和<end>\n",
    "#     words = [w for w in vocab.keys() if vocab[w] > min_word_count]\n",
    "#     vocab = {k: v + 1 for v, k in enumerate(words)}\n",
    "#     vocab['<pad>'] = 0\n",
    "#     vocab['<unk>'] = len(vocab)\n",
    "#     vocab['<start>'] = len(vocab)\n",
    "#     vocab['<end>'] = len(vocab)\n",
    "#     print(vocab)\n",
    "#     print(output_folder)\n",
    "#     # 存储词典\n",
    "#     try:\n",
    "#         with open(os.path.join(output_folder, 'vocab.json'), 'w') as fw:\n",
    "#             json.dump(vocab, fw)\n",
    "#         print(\"Writing to 'vocab.json' successful!\")\n",
    "#     except Exception as e:\n",
    "#         print(\"An error occurred while writing to 'vocab.json':\", e)\n",
    "\n",
    "#     # 整理数据集\n",
    "#     for split in image_paths:\n",
    "#         print(split)\n",
    "#         imgpaths = image_paths[split]  # 图片加载\n",
    "#         imcaps = image_captions[split]\n",
    "        \n",
    "#         enc_captions = []\n",
    "\n",
    "#         for i, path in enumerate(imgpaths):\n",
    "#             # 合法性检查，检查图像是否可以被解析\n",
    "#             img = Image.open(path) \n",
    "#             # 如果该图片对应的描述数量不足，则补足\n",
    "#             if len(imcaps[i]) < captions_per_image:\n",
    "#                 captions = imcaps[i] + \\\n",
    "#                     [random.choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "#             # 如果该图片对应的描述数量超了，则随机采样\n",
    "#             else:\n",
    "#                 captions = random.sample(imcaps[i], k=captions_per_image)\n",
    "#             assert len(captions) == captions_per_image\n",
    "            \n",
    "#             for j, c in enumerate(captions):\n",
    "#                 # 对文本描述进行编码\n",
    "#                 enc_c = [vocab['<start>']] + [vocab.get(word, vocab['<unk>']) for word in c] + [vocab['<end>']] \n",
    "#                 enc_captions.append(enc_c)\n",
    "#         # 合法性检查\n",
    "#         assert len(imgpaths) * captions_per_image == len(enc_captions)\n",
    "        \n",
    "#         # 存储数据\n",
    "#         data = {'IMAGES': imgpaths, \n",
    "#                 'CAPTIONS': enc_captions}\n",
    "#         with open(os.path.join(output_folder, split + '_data.json'), 'w') as fw:\n",
    "#             json.dump(data, fw)\n",
    "\n",
    "# create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def generated_sentences(data):\n",
    "    # 读取词典和验证集\n",
    "    with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    vocab_idx2word = {idx:word for word,idx in vocab.items()}\n",
    "    for i, text in enumerate(data):\n",
    "        print(' '.join([vocab_idx2word[word_idx] for word_idx in text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> The upper clothing has long sleeves , cotton fabric and pure color patterns . It has a v-shape neckline . The lower clothing is of long length . The fabric is cotton and it has solid color patterns . The outer clothing is with cotton fabric and complicated patterns . This woman wears a ring . <end>\n",
      "<start> The person is wearing a medium-sleeve shirt with striped patterns . The shirt is with cotton fabric . It has a suspenders neckline . The pants the person wears is of long length . The pants are with denim fabric and solid color patterns . This woman is wearing a ring on her finger . <end>\n",
      "<start> The lady is wearing a short-sleeve shirt with pure color patterns . The shirt is with chiffon fabric and its neckline is round . The lady wears a three-point shorts . The shorts are with denim fabric and pure color patterns . There is a ring on her finger . <end>\n",
      "<start> The T-shirt this female wears has short sleeves and it is with cotton fabric and graphic patterns . The neckline of the T-shirt is crew . This female wears a three-point shorts , with cotton fabric and solid color patterns . There is an accessory on her wrist . This female is wearing a ring on her finger . <end>\n",
      "<start> The tank shirt this person wears has no sleeves , its fabric is cotton , and it has graphic patterns . The tank shirt has a crew neckline . This person wears a three-point shorts , with denim fabric and pure color patterns . There is an accessory in his her neck . There is a ring on her finger . There is an accessory on her wrist . <end>\n"
     ]
    }
   ],
   "source": [
    "data = [[112, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 61, 12, 13, 14, 45, 4, 33, 19, 15, 14, 1, 20, 3, 18, 16, 5, 21, 14, 1, 9, 18, 8, 10, 17, 4, 11, 12, 13, 14, 1, 28, 3, 18, 29, 8, 9, 10, 30, 13, 14, 23, 51, 26, 33, 34, 14, 113], [112, 1, 47, 18, 32, 33, 101, 41, 29, 96, 13, 14, 1, 41, 18, 29, 8, 9, 14, 45, 4, 33, 71, 15, 14, 1, 48, 59, 47, 26, 18, 16, 5, 21, 14, 1, 48, 49, 29, 22, 9, 10, 11, 12, 13, 14, 23, 51, 18, 32, 33, 34, 35, 36, 37, 14, 113], [112, 1, 24, 18, 32, 33, 72, 41, 29, 61, 12, 13, 14, 1, 41, 18, 29, 43, 9, 10, 67, 15, 18, 46, 14, 1, 24, 26, 33, 60, 74, 14, 1, 74, 49, 29, 22, 9, 10, 61, 12, 13, 14, 53, 18, 33, 34, 35, 36, 37, 14, 113], [112, 1, 73, 80, 31, 26, 4, 76, 6, 10, 17, 18, 29, 8, 9, 10, 44, 13, 14, 1, 15, 16, 59, 73, 18, 75, 14, 23, 31, 26, 33, 60, 74, 7, 29, 8, 9, 10, 11, 12, 13, 14, 53, 18, 27, 55, 35, 36, 56, 14, 23, 31, 18, 32, 33, 34, 35, 36, 37, 14, 113], [112, 1, 40, 41, 80, 47, 26, 4, 42, 6, 7, 67, 9, 18, 8, 7, 10, 17, 4, 44, 13, 14, 1, 40, 41, 4, 33, 75, 15, 14, 23, 47, 26, 33, 60, 74, 7, 29, 22, 9, 10, 61, 12, 13, 14, 53, 18, 27, 55, 62, 63, 36, 64, 14, 53, 18, 33, 34, 35, 36, 37, 14, 53, 18, 27, 55, 35, 36, 56, 14, 113]]\n",
    "generated_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generated_sentences(data):\n",
    "    # 读取词典和验证集\n",
    "    with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    vocab_idx2word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    result = []  # 存储结果\n",
    "    for idx in data:\n",
    "        if idx in vocab_idx2word:\n",
    "            result.append(vocab_idx2word[idx])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generated_str(data):\n",
    "    # 读取词典和验证集\n",
    "    with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    vocab_idx2word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    result = []  # 存储结果\n",
    "    for inner_list in data:\n",
    "        inner_result = []  # 存储内部列表的结果\n",
    "        for idx in inner_list:\n",
    "            if idx in vocab_idx2word:\n",
    "                inner_result.append(vocab_idx2word[idx])\n",
    "        result.append(inner_result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'upper', 'clothing', 'has', 'long']]\n"
     ]
    }
   ],
   "source": [
    "result = generated_str([[1, 2, 3, 4, 5]])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> This female is wearing a short-sleeve shirt with graphic patterns and a long trousers . The shirt is with cotton fabric . The neckline of the shirt is lapel . The trousers are with cotton fabric and pure color patterns . There is an accessory on her wrist . The person is wearing a ring on her finger . There is an accessory in his her neck . <end>\n",
      "(750, 1101)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAAGiCAYAAAB6aXo1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAADkG0lEQVR4nOz9ebxl513eiX7faa219xmrTs2q0jzLko0lDwpgiEew04mD06EDIZBw7+fTtHAHfJsGpwm5Jp+0c+kkDImBNEkzBNwkDtBgxxjbAssYy5OMbMuWZFlTlapUc51pD2utd7h/vO9ae59STRJVJatqPeZwztl77bXX3kf7qd/w/J6fCCEEOnTo0OEygHyxL6BDhw4dLhY6wuvQocNlg47wOnTocNmgI7wOHTpcNugIr0OHDpcNOsLr0KHDZYOO8Dp06HDZoCO8Dh06XDboCK9Dhw6XDTrC69Chw2WDb2rCe9/73sfVV19NURS85jWv4XOf+9yLfUkdOnR4CeOblvD+83/+z7zrXe/in/2zf8YXv/hFXv7yl/OWt7yFw4cPv9iX1qFDh5coxDerecBrXvMaXvWqV/Hv/t2/A8B7z549e3jnO9/JT/3UT73IV9ehQ4eXIvSLfQGnQlVVPPDAA7z73e9ub5NS8sY3vpH777//lI8py5KyLNvfvfccP36cpaUlhBAX/Jo7dOjw4iGEwNraGrt27ULK0yeu35SEd/ToUZxzbN++fcPt27dv55FHHjnlY9773vfynve852JcXocOHb5JsW/fPnbv3n3a+78pCe+F4N3vfjfvete72t9XVla48sor2bdvH/Pz8y/ilXU4n4gFmKYKM1WNCQI2BPJh6n7B9J3Tt0IAMbm/ywVemlhdXWXPnj3Mzc2d8bhvSsLbsmULSikOHTq04fZDhw6xY8eOUz4mz3PyPH/O7fPz8x3hXUKIJecpygoQErnZcsTq8nGWjx9hsLLMaHWF4DxOSLZdsZst23cyO7+IzgsQEpBMVzvizx3lvZRxtvLVNyXhZVnGnXfeyb333svb3/52INbk7r33Xn70R3/0xb24DhcVYeqHJg6LBCcizfmatWPP8oVPfpzP/+nH2fv0XlZW1hiPxngfkEZT2xohBVu3b+eWW27hxpe9jJtffie7r7+V3tymyXM1wd7JT95x4CWDb0rCA3jXu97FD/7gD3LXXXfx6le/ml/4hV9gMBjwD//hP3yxL63DxUISEER6m05gBd7VHPjGI/zFh/+Az/35fTy9bz/DccXaYIgHiqJHZgwFgYX5eUKA1RMrfOqTn+KhL3+ZHffdx57rbua73v7fc83NtyG0iecP0+myQHRsd0nhm5bwvvd7v5cjR47wMz/zMxw8eJBXvOIVfOQjH3lOI6PDpY+m1haCADyrJw7zkff/R+77w//KMwePsTp2lLXDpnpcluV4IUEqhFQEBDrLsFLjqTl0Yo1B+RT79u1n5fhRXvYtr2LnlVdx0x3fwuzCEkJurPd1lHfp4JtWh/dXxerqKgsLC6ysrHQ1vJcopv/TDASCr9n/yFf47V/8OT53/2cY1J6RrakcWBePk0phpGB+ps/mhXmWNs2zMDfP2nDI0wcOMigtQUqUEvSMYtN8LHK74Pm273wTf+v7/hHbr9xDS3OCLsp7CeBcP+/ftBFehw4AAY9AEKzlwT/7IP/pF/81e/cdYr2yrJUVo3FNCJIgIM80Wipm+gWbFxdQApxz1LZmOBrhgVFZUXvH3Pwsq4MRCMnszAwHDx7mw3/4+2TK8H3vfBem6CHwEGQX4l1C6Aivwzc3QiB4x+c//kf83z//v/P0M4c4NqoYO0ftJUr3kCFgjMAYhTKGcVWx/9BhZvp9gspAVZiij8pKaj8AoN/vQ7/PkSNHyXqzqKzPyto6f/CB93PF9Tfwxr/1PSDVi/ziO5xvfNPO0na49BFCmKSt6efprwjB41/+DB943//Bk/uPcnhkWSsdwcdoDmEROlDkBgKsDsbUHoTJMb1ZrLNoKXBVicKRa4GzltFgwM7t23HeMRwOyfMMYTLWq5rf+Hf/hscf/vJU/yJdE4ENqpgOLzl0hNfhRUdDJun/AJ9ud6wcfJrf+5Vf5Imn93NkvWJY1kghUJmh9g5nLbN5jree2geKoofRhnI85tixYwRvsXWFDo6+VmitqXxgNBqzfdsS8/PzrA4G1M6xPio5fmKVbzz2GL/8c/87o7XleB0nX+9FfG86nF90hNfhouK5EdzkdtKwhEdCgHq8xkd/99d54Atf4Mj6GBtq8ixDZhnj8ZieFGybn8N5SyUEQSrq8Rprq8vUVUVPgyEwX2h271hiYW6WmV4fJSTeewiBG6+/jnI8ZlSWKK2QOmNoPff92X3c95E/ZsMkRkjiGNFR3ksVHeF1+OaAEElQHPkk4Hniy1/kk3/833j2xDqrtUXJjKquENaydb7PTD9jeTSkDgIRPPVoAEEw2+uxNDfDpn4OtuTQ0WPs3X+U8bim0IJeJqnKMc/sfZpX3/ly5mZ6WOcwxuCAvDeL84H3/+b/xXiwziSP7YjupY6O8DpccJwuqpuGCBNKEYCrR3z+3o/y+N79OK8oshxwzPd67FpawIfAicEIqSTBB8ajisJoFnoZfeVReA4ePcH+Y2sMS0t/JmN2Jsc7T6YNS4sLCFuzNNvj+mv2kGcGozX9IospszE8/NBDfO3Bv4SWipsXdCHfrQ4XEh3hdbhgOBvJbYRI/4sp48EnvsGn/+zjHFsv0UaRCcGW2YItCz2OrKwwLCvwkrr0lKMB83MF8zMFuQ4Myopnjx0nyyS7ty6yeW6G0bDk2PIaQikWZmfoGc3a8aM8/ujXuPWGa7HVmFxJ5vo9jJF469BK8rm/+AuCtx3HXSLoZCkdLihCCAgh2u+nhQiIIAgi4Ouar37hszy17yB5njNTaPq5QQrYe+gYQUoIEus9Es/CTJ+CgAqe5XFJaR27ti1RyEA5HNJThq1b+uzZPs+48gxKx4FDh/A24KuKm669jluv2MbK6pDSZGhvmDMLEDx7H/ky1jpMZppXRCfMe+miI7wO5x2niurO5mIxeYhgtL7Kww99ieFoxJbNC8z1CwC+sf8QQmvwntrWZEKwtDiHDA6pJKvDMbUNzOc5hSvZMd9j26YFtFLkpmZGa3rzcwhd8PLrd4N3bNk0z57FnL/1htfy2b98iIcf24c2GbWsWF0tWTl+lMHaGotLOR3RvfTREV6HC4rTEd0pSREgwPKRwzz2ta+wtGmO7Ztmqeqapw+dQJuMqqpACOZmCvpaowU4BOuDIUYpZgvFVVsX2TWXM5MLXD2kX/S54ZarufqWW+hv2k5WFMgAg5UVqtEavZmc+U1Xs2fXVv7iLx7gLx78GptnZ5EhsHL8CMtHDrG4tHX6Kju8RNERXofzhnOt153uuEAA7znw9OOsHjvMprk+mVYcOLqC0AY/GmIyQ2Y0oRyjlWZcVZTlmPkiZy6TXLFllhuv2IQBBoMhV93xMl7+6lexuGs3qreANAUejxKS3vwi45VjKOlxtiaXgtfcdTP9mT6f+dxfsntplv1HVnj84a9y1c23dRahlwA6wutwUXF6sosOd1Vdc/DppzDBM9ef4+DxEwwrR+UsyuSI4AjlmFxL6rpECMH2xRmWCsOebXO84tbrWDtxnJm5ee6+401su+oqVKGRWRYfr3KEEIgQUIUktxWhGsZn95bt27cgg+TYkcMcPnSQbfN9Hn7wL/nrb38HQkg6wntpoyO8DucFZ4vuzna/SJRXjoYc2PsU/aLHsBxzYjAmCI8DsDU9LViY7SOlYGWwTqEUuxZn2LWpx+233cD66gpX3XADV91wE9n8PNHpSYHMQRqklASgdg5CQGQG5xVCBhQ5wgvmFme47ZZrObL/Ga67cheH9z2Br0pk3pt6QXTc9xJER3gd/sp4IWQ3fVtT5gvAsWefYv+Tj+NQHFleoTCa1bHFEFicyVjIM2ovOLq6ipKSHQszFKJiy+adlOOK217xSjZv244uCoRQoBVCGqTUgCAEgcdHIvSBgESbnOAE+IAwkPUKtu/Yyp49uyirEj9cZn1thcW8f/7etA4vCjodXoe/Es5EdqcbIXvubRCI0pWDT3ydIweeYb0ck+mMuqyZkZ6d8wWzueH4aMy+o8eRSrK5b1jIJJvnZ5lfmOWW2+9gfmkbIushdZZGxQzKZLT7L0JABI8IDkHAhwBC4kOMMb2IjZZev891N1xP8DWhHDJYWWVDSNdFdy9JdBFeh28KiADBO5554nGM9GzuG5yw7JhZYH1cc2x1nRODIVILZvuKOeXZ2lcsLRRce/01XHfDjRRzs4jMII1BaI00Epn1kKZAqCzuKxUC7wLeO/AOKUQiO4FUmuA9SiqyPGfHnit59LFH0aMh48Fo6mJfvPepw18NHeF1eME4bQPiHG/fkNbKgKtrXFmyfWkeaWuOWMu+Y8fZd2wdhGCx0GRKslAYdi7OsGNpjutuuJYrrryS3vxCIjUFQiBNhjA5wsyAyhBSE5AoKRFCg/X44JFSIlAEqWP0JyVaZ2gv6M/l9Be2ssAKRb8gurjIrn73EkZHeB3OK85Wrzv9MTAaDtF4lLA89fR+Hjs65MTIMpNnLMwY5vOMQglyo9iytIVrr7+KHXt2IbMc7yTWgUHgrWPl+BpFX9BXUPQKfIgOKT54BJHYpJd4ZwGBlDrO8gaPMh4TFLUQ5LObcSdWmZmduUDvWIeLiY7wOrwgvHBie+4xUZwcGK0tM15fYd/BFfYdGeKCpq8Cm/oZRT9nVHpWxjWb5gw7rtjDpk1LSJ2B0iAU6ysDHv7qozz19LMcPb5OZjLmFme54cbrufPu1zK3ZQvCe3ywBOfARYsoKRVOCoQwKJFjZYm3Q5ACpwq86VONR3Ql75c+OsLrcEFwJvI7NREKxqMRmIJDx9d5693fwhu/89t44vAJ/rd/+x9YPXAMoyTF3By792xm0+ZNZHmBEBIpFSsrKzz1+F7mFzbzmm9/HXNblyBInn78KT736c/ywOc/x5vf9J3ceuedKBOt2wMBIQJCeKQQODyrgwEPf+1h/vyTn6IuJZt37uLVb/qbLG7bdWHfsA4XBR3hdXjeOFvkdi4/N7+3xgIIZJaj5jfTl5q3fcfd2Bx2Lihu3LXEQ4fXCAFyJLNZTp7noBRSaQigpeSOO25mdq5PZgy5sQgt2HHHFdyw50186YGv8uCn7ycXFdfcdhumP0cUGzt8XbN6YpUH//IhPv3Zv2Rce2649XpmZjax99gKL//2N6XjO7zU0RFeh+eFF0J2ZyM9gCA823btZjiq2Lm0ic1X7MbsuZJP/z9/yK1X7OTIoGK9Vsz3NbO9DGMUJtMQApnW1FScOHKY4bLiwLF1Hn76CMfWB2RGcMPmWW69cgfXvvIO6lHNaH0dlRcEFN7WjFdWePAzD/DMM4f5tlfeytXXXomSjpFZ4o7t1zO/ZVs7CTJZCd51LV6K6Aivw18J5xrNne62yZ0gdM7u66/jyBc344SmlxXsuuEWrnjiCd5w+/U8eWwtjpJtWkBrgVJxhaIUsLo25N5PPsjjh0/w2KFlfJ4jgazo88n1Y/zNu27mzd/2SrIsw9soR0EIButDDjy5l8X+LMX18zz17LP8xRc+zDXbN/Gyv/5mbn3ZK5BKIIOfKKQ7snvJoiO8DueMF2IOcC5RHgDCI3zcEbvzqqsQQePHJcf3H2D12ApbFucQm2HoPNs2zWJMhg+eXCp6ec6uXTt4/ev+GuWnPkttCl79hjexeugoX/3i57ntW74dGSpCvojUFjcaIGVObWuO7H+WtZWSA0PJH/3F5/nGU0+yPFjjms3z/K/f+l30ZmPqGzluspy7w0sTHeF1uGA4mezO2MUNAnBsu+Iq9i9uZnk0YN+Xv4pEceOtN7F45XY+++kH0NbSyyQ4BzJgCk1mNMpkXKkE/92330UQOUs33Uw+t8QXd8xw7XXXI+uKLTu3M149hPMOrzJsDd56BiFw3Wtey5u37+apX/i3vPbld7Fv75N88RtP8R0+oFVa4tPhJY+O8DqcFeciJD4duZ2J9DacV8R9ZbuuvIYD197El5/4CpnOuaI/B0sLbN6+yIwxhEwjBFTlGIVBiwKJJ+sVFLMzzCxuItQeKytUtcxrX3kHEkmxeQmTZ9TVCoIxtqpw1iN1wZW7r8SvnOD2hYzv/Y676M/Ps/Cdr+Fl3/EmpO4+IpcSur9mhxeEcyW7s0V57W3Bo4RCFDlXvfwVfPbeP8KUI4Yz8wxWjnGLCczPzjL2DucF3nqcqLGjEl9UyGIG1eth+gL0DGTzKAzVaECoLfnSEqEeUvQy7GCEq0oEBi1hfvMc1WjEYOU433rn9aA0vfmtbN0018V1lxg6wuvwV8Jpu67nQIjT8I0AOQiGgyGbN23ltiu3sWPTIgcOHGLvU98gm+1jhyUeRW0DGZ7x+oBepsmLGZAa05tHmTmC0nhbY2ZnyEwPlRsGhw8iQo3SBjzIvEDnGiE92XyGVnP06xxcQGqNkt3H41JD9xft8LxxrtKUkyO8MxGeAIL3eALa5Nz4ilezMw8sbpllz6vu4skv3M+XHngAo6KbiQ8C5wKurKiHI/zMCDEaIbNZpHEIJOQFQWkQGlsNcaNVFD7O1gaPkgLTn8WXNTN9g5/p432GHY8ZBYfQ6gK8ex1eTHSE1+GMOJeJibPV584ltRVAEA7vA3NLO8leWVAMjqPHh/D1iPnFeaSrkLKHCHGMvyZgncdbR1WOycZDxEyJYAYf4hYzbyuUkdT1CMYjfGmxvsbXFcZZtFI4O0TIDJ0VBFsRhiUBj1SGDpcWOsLr8IJwplT25J9Pdd9zzhPAK/C2ZmZmjjzvoTZt4qn7vsz1MxJ6fWpXIHoZaIGQ8UFBCHyAUNX40RA/XEdlBTLr4+oKJQUER1hfZfnwYfAanceF3NYFlMoIUiG0intxRTQCRRjMzOwFfQ87XHx0hNfhtDgX3d3ZorpzSWnj7xLhwXuPNhqhFEf3HefnfvU32JQLepnixquvY1Mxi1QKZQQygJCKIAQhOEJd4ssBbpyjpUQEiQ8SERzL+5/BDcfMLW1B9mYxcwsUC0uUVUnwY5TWhKrE1yWEgOrNkfU7wrvU0BFeh3PG85aaTN12cnR38nFeBGQApMQHR8Bz4tABhMp4dN9BlorAVbt2ofIdaCMwJqDxsYtqDEFKbAhoVxPsGF9r0AUehZEzOJUxGI14+qFH6c0tcEMxjynmoS4JdYUgI/gaOxpRWkG+fQ8qy8/vG9jhRUdHeB1Oiec7VXG6VPbkr9OfP6RObfSp05lhy64d3H7bzVSjFVRVUpV1TD2TZk8biRQCYTTCGNAaLyAEh7MVEonUPWzwbLnmWtxoSGmP8PjDD1GPRmy/6hrWThxi964tuF50bAk2UMsei7uvJQjVyVIuMXSE1+Gc8HwbFGcivFOTaUB4AEEQjnJU4YYn0GFAcBUmN5R1Cd6CMiBivU0bicwMZAUiyxFa471DeIuzY2Ra2qNUxs6bbmbb9Tdxw123s//Jx3DVOjt3bUX3CsqyJIRA5T1iy06KTVsQnUfAJYeO8Do8B2ccATvN7ecS1U0/5jnnDOCERAaLHY9ZP/YsT//ln+NGA7RU9GZn0VnGaDzGZIYgJEiBMhqpVWxkZAbyDCskvq6QwiKVxfsMTB+jc7Qx6N4mrp+5GVeOGQ3X8dZT+YpQ14xUn9ltO1AygHBAJ025lNARXodzxumiu7Olv2eK8CY/x5WJwQuqcsDxp7/E/fd+lL0Hj3P0xApblpbo93usrRxnbnYmTlsEjUt1vyDAEdBCg5AEW1KOBywfXWZhcStF4XCyJASJVIAtKdfWGZUjsrkedR0QIsPMbePE8aP0jjzL3NbdE4OUDpcEOsLrsAHPJ7p7Pqns6aQp7c8iIL2FAPXaUR76zJ9y8NBxVocVpYXagxSCarTOYH2V+fk5rHNYC9ZaMqPx1uHqmqwoEFpz6MgJ/vwPPsXmfI7F3jxzs3MUfYMtK+phTelq6p6mv3uOHTfsQhazLFx5E1dcdS1rKyeo64osKwjNFEiHlzw6wutwTjiTpOT5fJ18vgYirUoMfsSBR77A4SeeQKmAlNF6/ZlnD3L7tXvIe7MsnzhBLzcoDLVQVBIyo5FKYasx2kiEythz/fW8+a05x7/+LP06Q0nNYDBi5dgJMjKEVlTBs3NhDpNr6v5mZq+4BpHNsrClFx1cUh2vI71LAx3hdWjxfKO7UzUyzpXonnNuIREIyhPP8OxjX8JaT5FpvB0zkxsOHz1OUIY8U4yH66yuLCMX5pAoFA4l04JtBVWlkMohlWJuzzw6FwyfXqbn+uzafguuLjHWcXx1mdXNBbPbNWVQLO65CdPrpT6F6nb2XILoCK/DWXEukxJnivxO9/s0hPBgK/Z+5S/w9QilRDT59J7MaEIQ5FmGlA6tJOVoyEBDcDnC1SgCmYpyllDXBOewPmr7si19vIYTjx9i9cmj6KCxzjKYM8zvuomhcOiFXWzecy1phGPibiy6Ru2lhI7wOgBnj+6erwzl5MeeqY4nhCDgGR17mrUDj6FkoFcYRk6SZwZbefIsp8g0wnmEihQ0Ho/xtibYHLxDSRBaYTKD1BopJF4KlMqZ370JMTPP6oGDrKytUOQ5s5vnUcpizRw7bnkVKu8RNtBbc40d5V0q6AivwxnxQvV250KE0xDesbLvYXrGMAiB3uw8uhqQGU1pLQszfTIj8CHW9RCa4B3leAzO4X2N9w6PQEjJzNwsKjPoLMcUc5iFbcxdOc/Om8dUy4ew5Tpr60PWQ59NN9zG7JYdxN1pHS5ldITX4XlHd2cisOfbpW1QD5dZP7ofYTK0yZmZU4gTA4wSiBBYWpxF4BAiau+0EPi6onaWqhpDsLi6pi4r6qrCes/8pkVUBqgQM1Tr8bVFKIMXGkSP3tZr2XrtTQRxMtlNTA06Frx00BFeh9PifER0pyO9aAcFAkEgMF45BDiQimx2nqpax2hFr9/j6ErJzm1LKAnSKKSSGCkhMygpcbbEu5pybHFVhbeW8XhM8IFNsgfGQTVCVA5Xj/F1xbh22HyOnTe/HKX6MTKECcEJIHQx36WGjvAuc5ytCXGmNLS5/wXJUULAA4rodFINVsm0QSJZG5bIzOCdAyEp64ordyyRGYl30R3ZaI1SGSY3+HKMt2VMaauKcjzC2wpXlZTjivmlreiVNRBxDjd4x8paza5Xv4psbg7C1M7ZNE/W8F4X4F1a6Aivw1lxum7r2b6896d8vEDgiZZQhEDAU45HSFNgdECIZYL3jMcldVWTSdi+dRNFZrA2YL0nz3OKXo6UEjceUY9HBALeVtTjMfVwyMqxY6yeOEHWn6HozZIVBVrrqKfTMwxPHGLxqhsRaISoiWNkIc7pIunGLC49dITXocW51vKeb/r6nJ+TkJcQCEKAcyjhqRBIKcmMYbkcMxqOGQ9HbFmYY2G2oCgMVeWw4xKlNTOzs2RZTjnKKI0GAkIEfFUyWs0Yra4xHKyyenAFhGR2bpbFxQXmFhdRAY49dB/92T6L19+JQNNEdjIE2tiuI71LCh3hXcY4Xbp6ttrd6W733p+V/NrHJgFICAG8BzvGu9RlFYHB2hqjUcloVHLjdVdTFDEyU7nGDYYIAVmWkRUFSkWSpLGGqiqMycmLGcygR7+qKYqC2dke/X6BNpraego75NiDH8O5mqXr70IYQ0AREEgCZ07mO7wU0RFeh5hmnmMk81dtVrTnmZTLCM4RbAl4hFDYsmR9bY3xuGQ4GnHV7l3kWYaXCoInniaglCbPC7zWOK2SSDgQnMPNzmMXa+arJSQCrSTGKITwCAK6HANgQsXgkU8xPH6I7Te/inxxJ0KZCdmFUxBfU9xrf9l4e3u3eM4RHV5kdIR3meNMUd6pornpx5yqGXEu0R0iEIIiBBflIFKANjjvwAsGwzUG6wPGZc1sr2D3jiXymXmsdyhv0TpOXgghUVqjlETKgNIKKTXe2dgUCUShskyTHMGCswQ8SilsVaGUhuCoDj7MweN7KXbdxuK1LyNb2ELz8ZAyTl9MSNAjQpPuhvh/DYGfEh3lfbOgI7zLHE1kd7qu7Kl+f6ERXvP4RvURmUKCUFinqKsxWs+xenzAeFQxGNa8/I5bmJufQRmDCBpbS0yeEwhYZ0FKpIzRndIabfIYBTobzQiCRymBlAFnS4KtIXiU1EipU5lOYOoa/Bi770GOHHqMbOd13P/ws3zuwYe55dZbuerqq9i2fTtbtm5lafMSG8I3EZrqX/OqSEFoPKrju28adIR3meJscpOTjz2XGt7purOnIk1BiGYkAbRQKNPH1x6ZG9ZX1hmPSrJexh2330Qx04/nRRKUISt64C3OO4SUKJOhTIZUGVIZRHB4VyEEBClQMuC9RQqHUAJvPd4LZJFUgHWNEgohNBKHEzWj/Y/y/v/rv/CXjz0TCdEYhBDs2LmTv/v3vo83vOH1zM7OsGnL1njfSawmWtqT7evv3FZefHR+EB2eg1Pq5jg78XnvW7Jrfp5uZEx+huCb3z1BSETRp67jXou1tRVG45LXf8dr2Lp1EyYvIMSanZSaoigQxJWOhIDUGqEyVFagsh4y76HzHjovMJlGao02OqW/GiFVJMksj2YBQiKURBqBMoqeLvjUF7/Bg488hQzgaxs99OqaJx5/nH/1L/4Ff/ftf4u/+/a/yZ9+5EPgKryrkqh5BL6C4OlCu28+dBHeZYizpa3NbWcivuljztSdPd1ziRDwIqDSd9lfYFwF9KhiOBzx6rvv5OYbb6Q/M4fQOa4co2IbFqMUTklcXeNshRBzCCmRSiG0QaAQIkBw+ADgEdIgNfhQITMNlSXY2ADxAbQyCDROSyyOD977aWrvkEiMjjbyUiq895jMMHaW6tB+sqOPc+JLH0mzvRZPQJge+ebd9JZ2o4pZRHJgadL5+AZ0ZPhi4LxHeO9973t51atexdzcHNu2bePtb387jz766IZjxuMx99xzD0tLS8zOzvKOd7yDQ4cObThm7969vO1tb6Pf77Nt2zZ+4id+Amvt+b7cDlM4m0zlbGnsmep2ISTdXYrwfCP7SEWu/uxmbIDV5WXufNWrues1r2Zmfo7ezAxKKoSQOGsRCHq9Pv1eH63i5ISQEp0VCJ0leyeJD+C9i92EIAgokBnonODBB49PzQclNTLLIYsR4IMPH+DLTx1BSI2Q4ERAqlgn7PV6SKGoS0cmFFtnQI6OoMZHkNUyWbWMXD3A6MkvcuzBj7H2+Bexo5W22xu/Qid6eZFw3gnvvvvu45577uEzn/kMH/vYx6jrmje/+c0MBoP2mB//8R/ngx/8IB/4wAe47777OHDgAN/zPd/T3u+c421vextVVfHpT3+a3/zN3+Q3fuM3+Jmf+ZnzfbkdzoIzRWpnq+udTII+kR0hEERAChm1dVJSzCxSzG5iz84F7njFbcz0ZpmZXwClwXu8D4iQ6mBSoU0GgHMebXKkKZA6T2mvjI0Lb0FYAg4IsUnh4ypGGRRKmRi1CYkXEgIMxo7/+Ht/Qm19vC94IFCWJc45yrJkPB5DCBSFZn5xBo/AuRpfl9i6BgHGQBZGjPc/xOG/vJf1Zx8DbyMRd6nuiwYRnk/1+gXgyJEjbNu2jfvuu4/Xve51rKyssHXrVt7//vfzd/7O3wHgkUce4ZZbbuH+++/nta99LX/8x3/M3/gbf4MDBw6wfft2AH71V3+Vn/zJn+TIkSNkWXbW511dXWVhYYGVlRXm5+cv5Et8SeFM6ezZorfputzJX9P3n4oMo+cdgECKKPVQSiGF4GN/8lGq9RO86RV70OURRJCgMlAaV1esr6xSjsaYLCcIgcoyQjUm1BVzS0ts3n0NqpiNBOUCwlvseJVQD9syWiBFftYSbIn3Na4c4csxwTlqZ0Eo7v/aM/zUL74frRRBZVhXUdcWrWLtMIRA8I6AYOes5Hf/7T+h3zN4ZwnOA3FtpDIZznv8cISzlgrF3NWvZObaV8SIFdFltecR5/p5v+BNi5WVFQA2b94MwAMPPEBd17zxjW9sj7n55pu58soruf/++wG4//77uf3221uyA3jLW97C6uoqX/3qV0/5PGVZsrq6uuGrw18dp4veznTfKaPCVL+SApSSeGf5+J/8Cf/zPf8T/+Jn38P7f/3XqFcPIlBx65hQMfFL55BSxggvgNEGk/XQJm9jJQd4IRFSEQBrHR6wzuGdxVYjbDXE1QN8NWC8fJzhiWOsHz3C+vFjuLLCiR6/+8d/QVAaJyRVWVLXFVLGHbh1XcfXRWzK5LnBaIOQUVrTkGoQCodAKIPMcpTOMH7E8mOfZ3DgG4jguhjvRcIFbVp47/mxH/sxvvVbv5WXvexlABw8eJAsy1hcXNxw7Pbt2zl48GB7zDTZNfc3950K733ve3nPe95znl/B5YNzDfRPRXYnP/7kc4kk0FVSYquKP/3Yn/FfP/ABHvnaw+3I6mN7V/n8l77B6//ay6ndGGkFzlX4IMgzjfAea20kNB/TWBccBEdwFcLa2CH2AWyF8DWhGkI9Yri+Rl1WBBewVUmoapz1CCXRWU5/YZFi81b+5LOP8MVH95JlGU4ojAHpA8bkhABKKUIIWOuihEVqpNIxcpUgtEQgkTpHmhwhFGBwYogLDjVe4cjXPsvM5isw/Vm6Lu7FxwUlvHvuuYeHHnqIT33qUxfyaQB497vfzbve9a7299XVVfbs2XPBn/dSgRDi1I0GTlXHC22kk35rj9vw+HReIeD4seM8+Y1v8Fu/+Vt8/dFH24itKApG5Ri05rf/6BN822tfgbGBIB1Ca6QPiKBilEVACqLDMSJFXgE7HmKEwQUP3mHHQ/x4jXq4yvD4MZSUZL1ZQmYQpkDMS7TJMHkPlfVQvVmWq8Cv/u4fIZWiKmuC8mgVraiGo1GM5ITA+6a2FyjHFUiDlKkJIYkWVCpH6AIlDKgCrzQaiR+u4o8fYGX/N1i64RVRhNj+ARr660jwQuKCEd6P/uiP8qEPfYhPfvKT7N69u719x44dVFXF8vLyhijv0KFD7Nixoz3mc5/73IbzNV3c5piTkec5eZ6f51fR4VQNC0JyPPFT3VcgimwtAp+OE0g86yvHeec97+T4wWcprUVrQ5ZlzC3MY20cFUMGHnnyWT73pYf51tuvRQoFKiBdtHVXEqzwSG9xozVscHE2NpvFjtbxdQVCEFyNHQ+xowF2PKZY2MTc4lZskNGZRSpkphGmQKkcqRU1hl9836/w5L6DBAQ2OGQIlJXF6Iy8F2vGkbxFkpd4rPOMPWjS7YhIeFIjhAatwXmEzhG6ROg+hOOs7H+MpetujwvEkwVqckPtcIFx3mt4IQR+9Ed/lD/4gz/gT//0T7nmmms23H/nnXdijOHee+9tb3v00UfZu3cvd999NwB33303X/nKVzh8+HB7zMc+9jHm5+e59dZbz/clXzY4U9p6SmI7zWM9cV+rDB4ZAsJLIsdZwEPwmHKF+dGzLPpjHHv6EQ4feAYfBFJlgKAsS0aj0VSTQ4DO+E9/8FFqkyNNDxkMePDOo7Sh15tBZzk6M+S9Pv35TfTmFhFKx1GyusTXFd45VFbQ27REb/MOxMwmZH8B1Zsjn10k621C57OgM6ww/Opv/Rd+748/jpAK59zUa46KAWtt+/qVUjgX9Xtl7bHeRaL1kfhjrTIQvI3XFGINUQBKK4xRDJaP4WwNdBx3sXHeI7x77rmH97///fzhH/4hc3Nzbc1tYWGBXq/HwsICP/zDP8y73vUuNm/ezPz8PO985zu5++67ee1rXwvAm9/8Zm699VZ+4Ad+gJ/7uZ/j4MGD/PRP/zT33HNPF8WdR5yu/nYmPV6joyNEJZ0j4FMHNgSBrIaI5Wcw9gR5Lyczmzj+7D4g4PBIKamqmjzLEEK0BFOXNQp46LED/Nbvf5z/6fv+Fq72CAFSRtGv8x7nXEyHlcEUfXSviBMXdYmvxyA0JlMEIZHGILIetfdxukJIRIrAAoHVcckv/Mrv8Psf+hgIBdCOkAFkJqOua5xzhBBQSqWUVhICrA3HrCwvs9jb0jjCExUxLsZtwcfo17uJpb0I2OEarhzFSQ9gQnsd/V1onHfC+5Vf+RUAvvM7v3PD7b/+67/OD/3QDwHw8z//80gpecc73kFZlrzlLW/hl3/5l9tjlVJ86EMf4kd+5Ee4++67mZmZ4Qd/8Af52Z/92fN9uR3OgNNPTAREEPggcMKDcGjncSvPotefIVcW1c+RWUaQASUFzjpIDiVaT4hjPB5H0lMyEhOa9//Bn3H7VXu46/br8SFEd5Q0SYE2SKVROkPqHBdk6o4qUDlCJJ89qUDGyEtIjwhEJxU83lk+/9Wv8/P//rf46sNPIKXGWo+SEM0GVEtuEDvEUso2Gs2MxvlofLC2uoLYtRWkSA4q4F3ciytV1BlKYi0QKeO1lUNsOSKbW2zf0Y7qLg7OO+GdS7evKAre97738b73ve+0x1x11VV8+MMfPp+XdlnjTH+XkxsWpzt+Qn6NJ51EBIeqS+yRp9D1Mv1ejslmCFLGyQdlWFraAsHjQ7Rm8kESrCXPc2ZnZxmPx1RVRa41oBjYmn/y87/Be+75+7zy1mvwyqKzDKF1FC9LlVLpOnZCAzGtVSpFVZHgotZNobRGSInQhmOrA/7P//R7/Jc//Bh1UAQBPjjyrEBrSZl88pxzLSnDhPTSG4FzNSCwzqW1kRIRZIp0AwQLXuEFSBEnRZoRM1xNXY2bd5WO7i4eulnaDqfFqXV1kQibiQFZjymfeRhdnyCf7UXphtCgdPqAB7TRGKUIpiAoh9KKclxirWUwGFAUBb2iwHtHVdeE4Dlh4V/8n7/LT/3w3+GuV9wMdYVwgSAEkgrvPU4plIrRH6KJpqJPnnPJVEBqMDmH19b54Ec+yu998KM8c/AYPsQOr0+vSQuPtXEiwzmP1nFiw7noymJdjVIKrWJ63XRsVwclSqYaZBD49I9BTJ1VFA4Gj8ACIUaczuKrcbcH90VAR3gdzim6m9yX/p90COsYPv0IcmUfomcI1uCNbyzigFhrK/p9CAJrLZLAeFShtUEphTGGqqrQWlPXFqWiTVNVVRxZr/jffvk/8z+87dv43re+jqXZAu9rhPd47xBegbNIYwhK44VEihhlqUJjfeDJZw/x+//tz/iTP72fEyvrCBFrkEprnIu1tizLCCFOiQghUErFNY9pekQphTaKzKSlQc614uMHvvgQb/jW21PNzuGDAyRSpR0bSEJw8TEpRRYE6uFg+l3tzAQuEjrCu0zxfEjuOY+NmhSGh59mbe9DzPUEFAt41xCRj3o5FWUaedEjyzTORyLURuNd+vAL0ZJIs1FM6/ifpXOesfP8xh9+nI9+6gu87q6X87LrruD6K3ewtGkRpSRlWROkYG52DqkVIwtPHTjKFx56lE995gs8+cxhyqpCK4M2Gc4HbF3RU4q6rttrcM4xHo/p9XoYY3DOtdeUpQZLcywCnK2RSvLEvkP44JA+pdHBJ/PPaE4glYkWVoQolBYeKQLj0XDKOeWv8Ifs8LzQEd5ljuffoY2aMekUh7/+EKwdpZdvppmUddYjVUBpmSYNBEqqZBMSd2dYW2NMjhCCqqraGqJzDqUUVVWlmplACQWix5HlMf/3hz+J955MK/pFhpBg62gBlWdR7uJCYFRWBCEISJSUCGlwITYvkFAUedTSiRjdee8py7L9DrHO3HSQpZTtNXrvqeuq3cexvDYg1FXkLCcSIaaJCyFS99ZD+kdASYEUgbocsaF+17ztHfldUHSE1+GUOHnK4mS4csiJ/fso7BA7O4urLSpziNC4oEhAIEOgGo/AO0IQOB9w3iOdwxiTUtlJpNWklN7HCMm7QJ7nWO8wRT8Ro2V5FElRpTphOaqRyiAFCG2QQsQRNCHxqVYnkCg5IS5jDCEEqqpqZSfNzGxDdk1kt8EUQQqUiHvNBoMRvh4TkvlAEAphNFJE8vXBEYJN7i0xZZZC4G29cRdQh4uCjvAuY5xKh3eq+6d/D8kEoKoGjAarFNJixyOcTRq5ZKdE0y1FUg7WCPgoHwkCrTTWWrIs6tyaCMqHGD0prXFpAiPPDc46BB4lo+g3MxlOqXaHjtYmeSUGtDE4b2NXVEqsq3GVJzM5Ahk98KJuGlvXuFRX08ZgtG4dX+rkptzM0J7crTVKYYNjdVCyvj6gqAZIJRD9OYTM8FJH8kv1OU8kvBDn7dLOjRThhS6su1joCO8yRjsmdQpZSoNT6fBCAGs9vq5xuaeqa6qyxBQ9IPrQeVuCtQSnOP7sfqwLCC1S7S6mklVVYYxpIyrrLEpKcA6pVLsIx1qbIi4Rt4wRSbOJj2J0ppMgOEaYKtXoqjKaCkgl0VIzHo2orY3pso4C5EZ3Fxq/PdJSHgkiJBur5HYshAAHTsXmxOqo4sTRZbbIMXa8zsyeq0B5gqqQJjZrmlZFPLVvn6P9O7RLv8/zH7jDc9AR3mWIszUsTpfGTpsHTI7zBOewVUk9HmKKnKB17FZKhfewcvQI3rkoV5GyHZpvUthmeqap48Gkbmatpa7rNtX03qN17KoKQUtU3k9kM9PjYDoN/Teps3MhNkWJKW/wAes8NTWZMW3qKmRMrY3Op54j7uBwzuN8QEpBbS37Dx5j2+45Vo8v09+xC3ohOS/H+p1IKXTwvp3lbGaQO5K7uOgI7zLAuXRgz0xwUz8nMpRKtZlYLMrbOLhfjhFaobQBFScLbDlGNqNdRhOcYzgcIqUkz/O2dtYgzqtG4W8jWWnIL9bcSpyz7WOMMShl2oaClJPVk01K2njZzczM4Fw9FdkK6rqEEKL9VOoaF0VcBSkQeD8dSarUf0kGCVLy2N5DvOqaLZFInYuPS3lzU8trX17zNqZUusPFRbe17DLGuUR2JwuPfYhWSI2o1/m07wGS0WaJG48JdYmrx4xXl1k5cSJOQoT4Qa+qqiW58XjMaDRiMBi0c7JlWbZprDGmbW4URdGuTMyyoiU856adluPOlObxcYwtTkMYY9LPCq01SsXoryh67QytlDI5aotWO+i9o64tZVlS1zVaK7QyKKFQJucLX36cOkTz0Wo4Ag8SlcxIJ2QnIDqgiqn3vovwLiq6CO8SxwvR253N3DNGeDpGeQGEFIg0XmWtxXgHKYUbLq9w6NBh6uARSlNVUfbRmGk2Ewvee9YHa8BkgH+6tthIWEIISEmquxUp0pJTc6+ireXlaWF3E/GNx8OW1JTSWFthTEYIUW8Xfw54H7C2bvWB8dyxWWFtrC/mJqcqa9CKp/afYFx7glR450j96fSPAyAaucqk4yvbiHnDt47/LjA6wrtMcTrZydnIDgIBj1AZ2hhEFWK9TQrisFlAyOg9F5xlfXmZ0foQW1uc8yhApSmLJk1tojMdmv8cJwajMbqadHJFakg0ZAkikaBI86qivc7YCIi/j0fD9jU09cB4bBQY53mBELHDrJqZXAJ53ttgIhBrjBLvA1pD7TzOCryL0aVOcpUgACGQXmJ9SnNlaog09bsOFx0d4XUANhLfqWZop62h2k5oVuDHaSQLUFK2DiGE2MhYX11BBI8WIa1mVO1zVlWF9w6TGeJyn1hh0Wnsa1oIHElNkedFe51SSuo61vXicYKiKMgyQwjRnkkJgd/QiQ1Tej8dJ0OS9k9KlSI7FcmsrmnIt4lEQ+xeUIcqioiVpnI1w/UBvq4ROkOqDKlM2okLBJEWj8dZ3Wl0Ed3FRUd4HTbgXOQpIcS9sro3g1+N3U7vXexIEut0vvb42lKXVeymSgguWkRVVY0QUWycFyZp3ETcF+t9K0SWUtLr9drfm05pk95GN5N4XBwHs6mZEfdNxBJ1oCrLluxipCiTtCYSkBAKax1aC6RsGhw+EeIkumuglSQIj5QZ1jkGZcmRIyfYkglkNkNQBk/SFopYl/TJOKBtVqT0tvNKubjomhaXMU43Vna66A6aCbEYrZmZBbxNQ/FNYV4QZRipgVHXNXUISJ0hECglUSkKjDOzgsFgwGB9QFmWlGXJcDhsd8A2JBXrbrJtRjTNC611akCo9nu8P6a5IYTUxVVtSixEc744+jZdL2zOrXWM/rwP6biNvnje++isLBWlDzz06NNk/RlE3osjdTJ+ibREPDqluDZVF6r76L0Y6CK8Sxjn0rA4XQ1v+ucNxJc2gwUC+cwcA8Cl6C4J3Ag4BAFb16yurrE6GFNaQEi8t+R5wXg0jiTjQUnTRlANubRd4eRJV9c1WZZhrU+d1SKlvbatA5qkowPwwVOVJbnJ4r/qicQa+UqzS6NJUxsCbCLBGPnFed5pi/eGHLVRyc3eI7Ie+w4uM7OwGS81sZYpEFLjvY3/SKT3LKQud2NC0OHioiO8yxBnGyU7cw0v4JuC/uw8PkSzzGAd+IAk1e/qGlvVrA1GLA+qtojvaovIaMXGjSNJE4k10hRrbbJ2j0RoTIa3Di0l1jnq0MzDqpa4msc2j9HGEBAorRiXJd4HhKgxWYbSCjF1rPM+dlPT6497Z/NkDzWJ/uIb5HEOcmOAQO0la6XHFDMQBM4LtIjmo40UhxCbFS4kMlSxbglxrgNoDQk6XDh0cXUH4NRkN/37yfd558n7s/igcNYRnE8jtIFgLaGuGQ1HDIclo9LFGh+x8RDCxBaqiayaqCvuvKhak83Gly5ON/gNo1mxORFrio2dU5OSCiGafWCsrQ/aWdvo1hxfT5UcUiA2XJrJjcYSCthQTzTGkOemNQCIA2iRzIq5Gbwnvg9JmNLKUdJAhXc+LTsSaJPTsdvFR0d4lylOJrJT3baB4BphL1F87IMg7/fx0rTCXx98qs+BsxWjwSA6GycyI8R6WFmWrVZueryscRnOsiyRoUrmnMk6SiuCFCAicVkbbdYBlJoIk+u6TmlufC29XtHW6LIswxhN8B5jNForpAStoxX89DU10WdDinVdU5YlQgq0NngXpTa1rTl8bI2qdgQ/2cXrcbHo6WPDwjtPcFHnp0x2cf/gHYAupb3scK51veb7c0jPN7W8gMwK0AXOrsXoykfhiVYqFvOHI6x12GQY4Gws8htjKMuyHRkbj8dtY6GJ0Jr0MQTfRlfWVgBt9BXrezqRWewUN42LEOKomFJygxWUlDK6rwhaQXJcuxiif12KOpuvhvystW0nWIiQRt6S3TuC9cpS1p48Rb+RJJtOTnSJaQzdfRDkvZnJ/clRpctoLzy6CK8DcOrmxemiQEKaMjUGXcxQlVWcQyXuugghfnfW460HKVHGEJxHirj4piGlJmWsqqqdZW0mKkC0twkhMFlGEGC9YzgaIWS0eyqrEpvSWWvjCNh4PG6jvcFgkKymckAme3mD1hlSabTOIBFh3TqzxBpgYyKgp6I/UgSnjcY6j9KKo2tDhmWFDxLvm/S1GR8TKcONbjEhCEyvf7H+tB2m0BFeh9MSXXP7BvPLFN2FEAhCMTO/KaaO3kfZhffJbVjgXExly7qCNMTvkki52SERdXUypbMxwotpp0lD/EUb8TnvMSaL6bGAwXBAVdcIKSmrso3kbNqI1tQJZ2Zm2jnYJj1uokSBwKUosiHFpmlSVxW2tm2K3AqURVwZ2XSHtdIMxjVrw3GMcp1HpIguCo49jSGq93Hrms6LjRLkZjikwwVFR3iXCc4kN5m+/Uy3TROfCx7vBfniUuyYNktqQpRzCGRsNIRA5Sb1v9rWOF+3cpNGi6d1lr4bAJyzVNVEhydEtHv3NnU5U9QX52olM/2ZNgrr9/ttSut9SGNgsSuqVJSaNKshrbVxZ22yizJTUpWmdtcs4m6uq41yfUAnUfG4thxfGeIDSOGiPMd7RBq3a4xHY20vmg5ECJpEtktnLzw6wutwSpI7+UN/coTnfezM9hYWsUFRpU5t8C6mbZDWFIpotpnExsFHQkSQ7J+i+0hzbmvdlDZOpDR1YrdujCYzBiUkWsZoqxyVlOON+2Sb69baUNvokBLNAWLNbuK0Es8du7ikup9qZS7TpNdcQ3OfSK/N2ppxVXPw6AoiyVziRg0/sYWiCeBi9KtM1opSOlw8dE2LSxSn09qd7v7TRXewMaX13scxqRC7tXp+E6geVVnHulUIadQsQNrNqkTc7xoI5FneWrLHlYYhzbPGUa5mCiJGVdHeKUZ+espoYBIdeg95rtJUhG8jO4jaPak1GTHCsy6tgUyvb3qfxmRTmmt/b5oV06SotULr2GFufPaMMVSlwOQ5wVbJsACCVoCLpqdA423slUZlTYQ39Tegi/IuNLoI7zLAyY2Hk+87U3R3MtnF75MoTxYzyHyGuvbYOhESEIKIPnlCxOXVQiCURBCbEkCrwZtMVAi8dzEVdr6VpgDPuZYoa9k47tVEZE3tr5H0KiFx1oIPOOtwLk5nTLajyVYmMz3dEUJopTKNrCUET13bDe+hEAKk4sTqoL1+AC9E2uMRWmOE4APCZEgzRXidN9RFQ0d4lylOV6ub/v3k+yakF7988AidUcwutFKMpt7lRUM3Ik1fxPPVto7kl6KnaTSGANPpY5M6NnswJs7IG62ilFLkeZ7Gz2x7/rqqknVU0yyNV9WQWENoTWe4uf4mtW3O2Ti4RAPQaE4g0uvy3iO15pkjx2NqD+Ad0qdtZY0cJdUytckRstnJ0Vqq0KW3Fx4d4V1mOBeiO9uXD8RaXcxg0QtbqFyUi8Rpi0QELgqUIRKFD7HmFdLEQ57nG6LPWBOLYuOmjiakTA7BPo15aYqihxCytYCHWKuTUmNMjlK69bsT6bEQkrjZJFlKnJuVSqKMhqTLi6mxS8YDHvA4F4k5iqBjlCaJTQklZCss3n/oMLaO74En7WkL8TUEHzvLQipMbza5RKeGhYAuvLs46Gp4HYBzI7729kaWEqJBZzG7yJjoTddEeraOMhKfmg+BgLMWozWIkOyYTFs7a2Cta4LB6K0nRZR5pIgtRlpNQ0K3tbZGShJCaK3cpycmmigOaE0GvI8GA60DzBRBeu/SAqGqTauFIDVYfBRLZzpqCtNcrvMiuqNAG5X6MPHTE4ALAdObSzIV0nk7srtY6CK8ywRna2I0xzyH3Ka+Jh3QSSfUB082M4tDRasoF5dnj8uSunb4EFNPknSldhYQSCXb9HF6XnVi4xRnUH3q0J76dYTUAY6ENG0pFS3cHePUvW2mMSCk7WV2Uhf0rt1Y1tTzmnQ7hEnTpiWmVI8MADKSWmUtqLS8CBE1xwGCACE8Qqpk+S4w/bkuoHuRcMlHeOfyQb9ccdoI7jRfk/tj5NasRlRFn6AN1sXGhfJx4bR1cdRMColRmiBiDlzXFZnJ21GyaZJrhcXQNiOan6OWb5LeWlunx09S2EZ8bEwiHu/o9XqUZZlS1ea8tMTYzMtGgfDkvWmuB0gjcCOsrVEq7r9oZDNxNlhxdHkV6+M1NuYFSIUIyTMgpBnk2UVa7V0X3V1UXPKEdzn+B3W65sPpjpm+7VQEd/J9PrhJpxaPUAahMkKoqK1FWUeIw6ptSpdlOcFZnK0mW84SyTRTFTH1nExgNI2F5rsxBhfiJENIRgWxa1q319aQZ3zOrCWz6NJSU9eOgEVpjdGaPMvbTWnABrcViNKWJoKcmBL4Nr11LiZJPoS4h5ckSaH5b0+lGdqmdhnI+rOpbHf5/bf5YuOSJ7wuwpvgdPKUk+t3J9926sbFRKKihEJkBdYN4/0i9WaFREkZ51+tjfUxP4nyRBq8bwbtY8RnaXa+NlFWO8cqBUZoHDHNraqyJbim29pEjU2qPB6P2+ZIU+8LhCnh8KSqM71/NkaLLk2OxPtF8vTzIdYgmwg0eI9UirJyBNeIs31r+olUuOZ9A2RRnOe/bIdzxSVPeJdjhHcmnIsU5azprSd2JkNyPxYCsgK3npyPQ0ASEMnR0nqfallxm1kTvTlrUUpTVVVyM2lkJh4hYlTVkB2ASyQDcS43y/KWSJVSKY1tJjgaM4LA+no9mYOVEiVV2i8xaSg0tbumwdFcI8FTV2W77jEKoJsaZtyHEYkUhuOasqpAgJIx0vMhkqBsDEERU2NlHS42LnnC63BqnC3VPa0kZVqUHHy7ktBkRZxbdZYQIvGFtJrQOgtCUKVdr/EpJvOwDUG1A/1TEZvWuu2uNgahQoj22GYC4+RaYHOeKEfJWrHwdKrcaOumSTTaSU1mZ+u6xqeIUEqJSfZT3vm03jEneIetK4a25PjqOldrleyeYie7sXOPEaJE6e5j92Khe+cvU0wvuX5+ouOUrqVCVUzaBCrLCMQRMm+jw4j3AecCRhu0KBHaMC5HpNbDhkmL5ivW3EIbjTXP26SgDWE128umU9LoURf/k47uxDlxdM2npkZI+zHKdLxv9XzN8u/mtTcRYVmXKKVbki7LMc26yOkRNkHAV461YRnTdudRRk3JiZMIW2mU7sw/Xyx0spTLFGdqWpx8/3Prd9PeeFForE2RZkZFS4ghiXRFaFK6NO0wRVwQNpBNrOdN7Y8gklev12t96YzRKe2Fuq7S8bGZUJYlVVVNpanNesZm6N+2TY5mtWQjuUmvNh0/6RBPOzKHEBiPRzTzvFGq46IXoJAcOrqMaFdBTpxQaGlepAivK7W8GOgI7zLAmdLX0xHfyfdvvK0hu9B+lyabjFCFNI0hU80OqJ3FOkuQoo3cmgipIZtG59ekpM39E8mKaScwQvBUVZkEwnVLlg15Nd3Uqqqo67qNBJsO8LRIWafaXxMNTs/VShmftyh6FEVBnucorYgRanxMXVeNKpkvPfJ49ANMXeogiC7KQsZQT6q2S93h4qNLaTsAZx8523jbJNILIdGcziB4PAI31RAghFaYYX3yntuwLcwhkMkufWKr7lyjq0sOJ8lAMzYZmshq0kFtanixQ+tTw4Q2ivNetHW75rhmf4YPMC7Hrdddkz5Hwo37acty3EaGWmnG4zGzs7Nt86JyFUh4+sAhXFUTcksI0TShcYEWIhoqRNusC/0X7XAqdIR3GeJURDb985nS2VMSIQGpdYzwAngXsHXSyzWyDOfRyqDSh90H39brhAhtnasZH5tEWDI5FKvWFy+SUtxC1rglT4S8EIKcyEqYLPJuIr2J7MSn9HcS0U30d6atHZZliRC29fSLzZRJg6XIc9bX1zC5SdKWNNUR0ia3EHA+JNuoTib1YqIjvEsMZ2o+nOnY5vezSlJOIr0Y4QmE0gQhU6c27qUVRLMAFzw+QJ7kGNN1MyllMgmVjMYjiryY6NumnqMR/U5LRkJgamkPyZoptNEezfMnXV6zLwPYsOhHCIm3LrkhT6o8DUlOSM4QgsPZuJtjsqFs0lW2LtU1A9BU7ZKri5AKITvCezHREd5lhJOjsybKOVs6ezqt3sZzxdnS1m24+dALSWlddPgNxEmJRHi2tszMzLTNgeAnpgBNE2PSGJh0Z5sIsEl/67qmrqMer6kBTusvp8XIca+GS6YGstXyGaPbxsfEgDRMESzJPy9tO/MB52ogxH25Mr7+sqqonaNIDRnwSCXwjVTm/PwpO7xAdE2LyxRna1hMH9Pq7k5l996eZ7KwBsAGh/NxTytCIlK3NBAX3Kjo8Y4ElIiLrXVapWitndpWJlHKtETXEFnTeGjkL1Kq5JgspgxBNzogN82R6fEz52xbD5yWwExHmM0oWVt3dBbnarIsur1IFW+3dU1W5Jg8+vY10yhJ0YLSGiGjNrDDi4OO8C5DnKlr23w/3depJldCCISmEC+ijTlBEnyMfoSI1lBlVRKCR2oBKi6/KauKkNYaNstumijNtu7EcWB/skwnTi1IqRAiEl0jcVFpmxhElUychJjspI0E6aMPnlbpuLQkSOnWCmoaDYkWRYExOVIpxnVJEFBZS4B2m9nacMywsggsBJdWNfq4m1ZKZPDRCr/Di4KO8C5znEp2cqr7p39/TvNCQFCyTUljhzZ2YK13MXpr7JQSmukDREDq2IWNAt5IqFF4HKMwnWQg09MUzRNPR54xoqMVCjcavCaVbZoXVVUxGo5aU4GJwDm00x2N5m/S1bWtlKWJRJ33OD9xZ9baMByXrK4PwTuCtQTn8M7GmVwpoyeerZsiX4eLjI7wLnGcS1fwVI2IU/1+8vmmb49zsjI2JpODSW1rvLPI4FFSkOVZe2zbaAihXaQd1ylGdxObTASmd1kAbcrZ6OymnVWi67Fq09sQmmv0bWMhy7LokCIEWunWyn0ih5lsKou1wbp9jrKMqx2D9/R6/YlrclvzA+cDx46fIIRkJx8c3kYPQKEMUmvqcvSc96/DxUFHeJcpzvXDdiqyO9XjROq2xuU4kSRi8glaiGQJb1FTHViZBuxjUBdaFxMAJVUbcTWpdBNxTRuFTltHSSmjkHhq8U4UK+vnkKMxpr2G6T0aUk5S9maHRkOKk0hzQrrTTsvW1vgQeOwbT8b+rLN4WyJCEh9rg9ISO1rvqngvEroubYcNOJOUZeNQPifdF2Ui1jos5WQRjwQtJVLE+2R6rNYGIcbYxlAAhy0rMpO13eOm9hYbDExFbJMFPw25jUZDlJJYG1q9XOzaNqsgJ2NiUgi0lDjrCIqpGdsaY9RU1BqvtYngGnOBeA1T0xhKkgmNsx5vK+YXFmIt09eR0KVBSFAaskxTr504r3+zDueOS57wupRhI15oVNdgmuymh+1JpORcXFNobWSo6D8n8Ai0MpGEXPS/k1IyLkuk0u2uiOb0IYjWRMCHaU+6aPoZIzjabm6WZUkmQjItsMRtZTEqm96Slmc5JOGzVDoaizbb1rxDiEbuUrUSlw26QWgNB+L+jpi+q/Rc/V4B3uOdAyQieAgKkGTGMFo9HhsayQIr2i90gpWLgS6lvQxwJpI7na6uwelcVaajPZE6syEIvAsE7wg+1tgQinUb8EKiVRYnLrROdTZDMkLHO09R9Ag0It6J/CSEuPw7prdxVMzaicddo8vTOsOYLEWPEmj2zcbX0hiBjssxlbU4H12RGxKLKbJuia9Jp6dfZxMpbnR5IUZ3RJOEpU0LyOCRaSFQ8DU+gJAGnRVUq0dx9ZhGmJzevE6tchFwyRNeZwB69vfgbCLjk8ntlCltqplZ59L+VXC1pQySg2sjgrOMxyOkUIi0J1ZOzbY2S3a00m001aSz0bAzLj6cLM5W7VhZU49rmgzTTivNORrd3rRDslKS8XhMWZbtIiFgyqBgYg4KEzMD7wPex65ws1JSa02R5WilMTojeI+30SortowhaI3K+0hXMl5dbvf2Qui47iLhkie8yxWnm4441ffT4XQ1u9MRX/O7c7Ez6UPg4OqQgQXwZJmJkxdlFRdyJ7PNqirbSKk5T2MZlWVZdCvJ8xgJpgbDtHh4ohEMycAzztk2UpJmnnaaCBtx86QL3Hy5lA5PmivTUd3EJDSRums6xDrqDp3n4KHDWGdxifCaVF1qhcoKCqNYP7x/+q91lr9mh/OFS76G1+GkWtsLQBPpnInkmuO8c8lzzlH7wBOHjsa5Vp9cSJRs63HNnOpEFjIZDTMmykxixufTnGxcci0QbUob7dYnaxphMhFBchxuam+NOeg08UWTUE9ZRunIdKraTm04h1DRxt2l2Vs1FaVWKYrz3uEDLK+txz21tiYEha8tUtUgLQ5QRjM6fABusrGT0dXvLho6wutwVpxMdNPk1xJpSE7I3uFcja0sg7LkxPqQEBRCxrGquipT/S6mchMzUI+UkeAaLZ5zHhECLkzSW9mMbPnmaT0wtYOCSB8+BJRUuDR21nR0G1fkprY3Ho/SaxMbDEFbOUwAESA415ocNNGlrWukMfEYBEorgoSlpc2xhhjAExiPx2RBkMsMTIYyhurEUex4gJlZ6FoWFxEd4V0GeKEC11MR3XOOSaRF8FF35jy2snERt3VkpiAIjUyqPNWT1FWNFBqtYoQWgseYLHZfhd4wR9tsMJtetg1s0L957zHapCkGS0gjZs7aZEOVt6mrcy7O8wJZ1kvzuTKJnCeNihBiQ6Oua5CiNfFUxsTJkhAQbQc5CZYJ4Cwra+vU3iOyHvVogAhJL6hktNHSBsUK47VlzMxC80Z2uAjoangdzorTkV4cD5MEBMHWeOeo65qyrKjKkqocI5K4uE73Bx9QyQq+kX/AxBR0esmOUpOdF9NNhKbpAJOtZlVVUtUVLgSQIs7KmiaFtW1TQ2tNryhSlAb9fo9eryG+eC1hisSa1xmS9bxKaa1zNr22SUSoZJSgSJE+VtIgpUZphW/EJyKSp1IwWjmezn8h/3odpnHBCe9f/st/iRCCH/uxH2tvG4/H3HPPPSwtLTE7O8s73vEODh06tOFxe/fu5W1vexv9fp9t27bxEz/xE22XrMP5wfPpYJ++didBSHw5xPvoXjIuS8pxFZ1/nY3zsioN+QuJybKWTBrCaFLYaZ3btC2U1poi7XO11lKWZdvYEEJgshyTFRS9HkWvH7V/IbSbyaanMqJJp0zmnhVVVW3oCk8/xvkaiGQHUJZxSU8TXQYfrd6VThMZAuZ6eYzqVEbe68cpFCYuKVIKtITRyuF07g4XCxeU8D7/+c/z7//9v+eOO+7YcPuP//iP88EPfpAPfOAD3HfffRw4cIDv+Z7vae93zvG2t72Nqqr49Kc/zW/+5m/yG7/xG/zMz/zMhbzcDpy6M3um24jxHW59GeEstraMxyWj0RjvHEUWa1wqza1CtFFqyKfJ5ZqF1s25p4f9mw7reDyeNBK8n2pwmFYaEgnNUddV2109eUlQM7KmlKIsy2Tfbtuocfo5BBIpY9rbWLwLKdo6no/+Vwhiii0QFEUfpTQqNV2U1G2q3LSitRa4wTJtMbLDRcEFI7z19XW+//u/n1/7tV9j06ZN7e0rKyv8x//4H/k3/+bf8PrXv54777yTX//1X+fTn/40n/nMZwD46Ec/yte+9jV++7d/m1e84hV893d/N//8n/9z3ve+97WOtR1eXEzSy0h4o5WjSO8QITAqK6o61vOkgOBi9JblBUjR2ik1e7wguY2YiSavIdRGp9c8J0x0ctPE12jwGl1ddDOxG8gwy7INc7PxfE1HNr6u6dS5saKqkwWUlLLdSZHnRasFjJZSAiFjvPbgl74SZ+qm9tsSQlxa7tOUiZDY0SAKtZv39ML+yTpwAQnvnnvu4W1vextvfOMbN9z+wAMPUNf1httvvvlmrrzySu6//34A7r//fm6//Xa2b9/eHvOWt7yF1dVVvvrVr57y+cqyZHV1dcNXh3PD6cTEzX3PORYBU0P2ylZUy4fjFARgrWdc1SAhNxqET+mhQ0tFrygiESXCaMhFMNlm1qa01uFqC96DDxilMSZOVDgXo7VGaxe3l/mWwJqIb9pUIMuyDSLlPM/p93sURR6bFz4SdENUrZxFCoRS0dNONh1qj0ibyayzSKFASMZlhfQOgkeI2KEOQPBxSblIj3fVCO/q2AUOE/LvcOFwQbq0v/u7v8sXv/hFPv/5zz/nvoMHD5JlGYuLixtu3759OwcPHmyPmSa75v7mvlPhve99L+95z3vOw9Vfvjg5dT2VFCX+EmtvInVe7WgVMVolEBiMx9FAwDu8U1R1qoU5i8lyhFRpyD+KeyPJxdqfFJrIKZORrsZcAECpRnYiKIpeKyyOqxrt1ATFxMSzSXuHwyEQz92MmDnnMCZrU2YhfHpxbJCmNOl3JM9mvCzW8JDpfQoBozW2KimdQyQhs5CKuKuRuJ5RCEKqaXo/Iri6a9BeRJz3CG/fvn3843/8j/md3/mdtsh8MfDud7+blZWV9mvfvn0X7bkvJZwu0ts4UxpJh/R9fGw/M5mKxpiN9CMIRpVlWDus95TlKOrzXEw7m6U8Qk5WL07X2Zrr8IBLEpBmx62bShWFIE1jZGRZQZ4XZFneztPGrnHZpr5Ns2G6BjndMZZStBKRicPySYYKU2mvd9HVWKbILzCxygrp+oMQSGWgjWhV/OQ5h6uqdmNbhwuP8054DzzwAIcPH+aVr3xl619233338Uu/9Etordm+fTtVVbG8vLzhcYcOHWLHjh0A7Nix4zld2+b35piTkec58/PzG746vHCcaX5WIBBBIwMI7+D4QYyJDsUyzb4KKfGelgCttXjnYr2rSWHTsQB6yt+uieiaxkJzWxOVxVrdkKoqpzq5oq3tWWtbQ87m8ac6H8BoNGI4HFCW5WT1I5PGRQOfUmpX2zbtBlp/P+9D7LcKwf79h6itn9T3hIzpsIhEKNJ+3eAstirTexoFzh0uLM474b3hDW/gK1/5Cg8++GD7ddddd/H93//97c/GGO699972MY8++ih79+7l7rvvBuDuu+/mK1/5CocPH26P+djHPsb8/Dy33nrr+b7kDs8DUXuXPrRCwPoxdLWG0mmnhJYYo9GZQWtJ3yhC6l4iolYtS42EthM6JRlp0tJpe/WNKa5tu6/N9UxExTXWxq9IhvFYpRRZlrXOKI1HntIymQ3EpTzNzGusJ6ZusW7Ezz7N6Po0UeIJziGYGBFE09NAleZ0nXcgfIyElUI0I3TE5oV0Dm+reM74ai7yX/Pyw3mv4c3NzfGyl71sw20zMzMsLS21t//wD/8w73rXu9i8eTPz8/O8853v5O677+a1r30tAG9+85u59dZb+YEf+AF+7ud+joMHD/LTP/3T3HPPPWn2scOp8EKnKU532ymjvKaMJ+PwvF3eT6ajy0lmNHmvIM9LtA2Ecc1sLvG2Rmexq1nXNUZrer0edV0zGKQdslPXMZ1yNoQ2aT40hMWG+wkhTVnEqDK6HLskcm7Ew40RQMAHF0fJAFBJmxdF0SFdQzQrjbNlzWPj7lqFTb5/SvkmKyaE+HyHjx5lOK6Y6WdImWZlxWSGuOnUhuBx1jEhukBHehcWL8po2c///M8jpeQd73gHZVnylre8hV/+5V9u71dK8aEPfYgf+ZEf4e6772ZmZoYf/MEf5Gd/9mdfjMu95HAuZgKnbVzQkF5AOU+5coSZNEMqkAityXKDGJbUBHwIuOAptMEHi3e0gl1nkzBZqbZDO01wp0ynxcapDO89Ki3SbghZAC5p/ULwiBBnaxutn9Qy7qb1gV7Rb4XGNhEmgBKNbo5IZumagg8oo9CJSIMQsfOaIkOjJAeOrfPkvoPcdsPuJExuGhciGtkHnwK/RpfXzdJeLFwUwvvEJz6x4feiKHjf+97H+973vtM+5qqrruLDH/7wBb6yDtM42fnkdPc16WkoR4hyDdPT1N6BUkiiENiojNXxOk8fWQZlUFKA8yiZXEdSQ0BrHVNjaAv/vnFWmSLd6W7pyaNmhKkucqojeusmjQYRmwdNJAiNlm9i+dSctyxLJFF1I6TAWo/SkxoiqVHhpzzsdCJWrXSUpVQlX330cW66ehdaWoSOnWkRJITYeAne40KYnEV00d3FQGce0OGUOF2qG6O8qHOzgxVMKEH244dZCLQ0SGOolOHrB49hZcZs0Y8EIiQIFRdXu0DtoiFnEJO004c0dTrV2JhMSPhWctLU8aSUSCBIiSOg0/1Ki7Zh4kNakSjjRIQPnuAg+NBOb0xHkipZWIUQosbOu9i9dSGlwQEh41Kf4Hzc1pai1iA9VdrBSwioEFczkqykwBH8GFePsK6m1d6FieSnw4VDZx7QATh1mnuqtDbdgMDj1o5GEXAWRcRZlpPlUWt3aG3A0VHNzMwcWkiaNY4o0ZJG8J7aWrRp/t0NNEacjWlnMyUx7WfXRG7OOWob90k0JNkIlmPK6htZ3YT4hIAg0jrJiRlBu5BHyqidY0KCzrkoNZFxQkJIgVBEd2cRktuLSRFhjZISKQLOxSVFWBfJN7YrkK4iOEvtkk6PZkCvY7sLjS7C6wCcuuFx+jqfQAaHWzuBKXqYPAehKIoaZcYE6yhrj0chJUk2ksX0LZGRIE1eKIUUEp/CoDA1Wzqdwm6o2WkJIaSoUSR3ZRDJWiqE5MiSUtBpM894/GQ5N0wi1yZqDD7W7lw8abvRTEvVprHW1mhlQASs9yiv4lRFcORaYcuSwfoaYU5ickMQCmYX2P/4Yzz8hc/y1JNPc+VNt3LH/KZUJkyi5470Lii6CK/DC4MFWa6gJTG9MwZd5AghqCwcWF5DBhu7md5h6xJbVSBkHAsLrhUTN8P4TZ3Me49LDsfW2nYZdmxKBHxaBNRaSSVfOkRAa4lWAqMlwafVikIhgwAfsHUFOEQUEU46pyGghEgNDo9L1bUm7lJKpTngyQhY9NWTaGkwOiNL9UhP4OtPPsPTew9z/+e/yu+8/4/4+t7DzGy7irL29Ho5N95wPYszGeVo0L6lHdVdeHQRXocWZ5upnb7PV2O8HSPVHMoYhIhaMu8Dw9pxeHklqsuaoMXHaYqYioZJRzVNQyhp2k+8UgrvIqlMa+5i2umRUmNrh1YTGydksnInNi2sTSsVQ0gzrUkUrTRVsoevqgqj45hZZgx1Vcd0XQpcEhaLMJn6iPZPEqUVUiQdoYvRoLWOcV0hREBLyee/foDDR/8br37lHbz52+/i2l2LDJ76GtdcuZurdy5SnTjC0WPHcOUwvexmUK/DhURHeB2eN4QAN1pFENDGxCkCB1JIaus4NhgzLGsQKunZJEGmva7I9lOtk/5Otk2DqREuETuYE7t31xqCTvbainYONs7eC1RmwPs4ASE8Pkx57IVYS5NSxtqhjuacglink0rGnRWp0xx3aCQ9XhqBw0dDdtFMbcQTRyKOXlCEEJgvJD/yd9/EzTdfz8ymJZTWiFCjhKQW8TgRwFf1xje2wwVFl9J2ADY2LU4WHJ90YOx0jlfR7aB/TPyCkFgfOLyyzqiOg/lAclCxCBmobYXwnqi8g2DjLKqzdVqCPXnOZlC/ka8gxCSNFc3i7+Shl1g0Wq3HeVapFCZNbMTpiebcTT1PTiKqNEHSnsP7VMsLre6vgXcuBq1pssInkwClDa721BYWez1mZgoqWye9occLgRWB4AW+cUiZimA7XHh0EV6H03djT7ofaEeu7GCFQk0ThsQ6TzkuWRuXOO8pUmooRYyGnPfoLA71h7RJrDXjbAk3uZU4F2UsQSKJ59EqLvBurNInqxc9Lk1XQCThOq1alIF27WIjJA4h1vZifXBi/NmQaTNlobXGh0TWKlpM+TqmunVV4wnR3FM0tlYOqWLk6oTAGB2jP0JMjUmd6hCFzUrJpEZpktkuqb3Q6CK8DmfFxm6tgOBxowFKCZTRCKkIAcrKMi5rBuMKISVGmbhYO0VpUoi4nLscEwhpgc9kr2zjNOKSyYBPnViAzGTJgbgZzZqIl21jDS/S8p4UoXnnonRk6jl8CDjvqKoqWs7LaM0efBoRa0mxEQWn+V6p2miv6fBqrVFpl0UIkcwDcW3jgeV11tcHBFcTvAMfU95E1yS9NEJ2BHcx0RFehzOOmT1Xmwf4Gj8aonTjACJjulY7hpVleThMTQk5Gclq3VEkSImdmqSIqWHU3vrgCQJcEiAHohOKcy7Nq04WdkNyQJEqkmka2m8iNgJT8xBJwiImY2txU1kgMxlFnpMbQ3C+lbPIZCAqiLb0wU1s5X3S+TXLu62tY4ruPa62DMclew8tR3OCqsI6m2qIrgmT47V1dbuLio7wOpwRJ8+0BiDYimBLlEjLaaTEOU9dVhxeH3FsdUiR5Ugx8bZrlmErpaJ9FJMdr0ql6CnExodpBMZMrOAbsowOJpNRsEajB6I1CmjXPKqJL13MlCOZNeeSQhC8QwI+kRnQCqGbrnBjBdVo9ZrU2DtHbR3Bxxqi9w6PByGRSnNseZ1gbbSU8i6aGwjaedrYnGkW/HS4GOhqeJcJzsUw4OTjp7+3DQ3AlgM0FVL1UDqL7r0Bqqrk2GCMVwaZlmM3HnXNgH5d1+g8w9UuEs70hIOI42A2JEsnY3BpSkGK6LHnrMelPbbTjYTYSJW4IDaQbDOuhgPwyORFhxAxmhOBsh63zRihVWxW+OioEgKtLXwrTE6kqVVM5+OGMokNliIrKMcjFIIrtsxHoo8e7m20KBLhxebJxJ25w4VHF+FdRjhdU+JUOJXTbwM7WEcLi8pMXCwdoK4dg3LE8dV16tphXVylaFMq6kNol+FUVRUbAFOLeBo5rwsTsqrquk3/XIgCZmQ0DW0mKhrIVLsTxC1lzeudRGnVxFZeiLgPAwhStIu2EZNdtIQQTQKSu3JRFBRF0dqTNXttfXAxFfcOozMIkBV5snCPr725LkHsyjZ/Be98WuHY4WKhi/AuM5xpZvZ0t01LRQRgRwO0EugsQ+oMHwLluGJ9NGJ1MCTPMurgMUKhjI4pKZ7go8uIFDLaLKV0NW4Di9q4JiKcHvWKAuI4xO+cI8sKhA8tmQDQ1Pa8p3HL83FgNg7+EyOpZi429ic8IoiUAss4kSHjAvB+v8fWLVsYDNfZvn07R44cYTgctlMfMzMzFEXBytoa1lmqsqInRTIaUFSu5rFnjnDztTtbW6rgPZ6o7ROykcF06ezFREd4lwGmSe75zMxOPooSiPbnEoEdrtBLol2koq4cw1HJcFgzKC2eQF/n0QIuxH0RXgqkEUitYqc1kIw80xTDVBOjuWbYuNBnut6HCCnCssnsM6a93lum914LoktJo82z3oF33HDddVx11VXMzs6ijObrjz2GLSvKsuTQoUOMyyHX33gdR48fZ3VlBWstWZaxbds2du66goWFBU4cP8ZXvvKVeH7n8SHQKwrGoxFJg0zwNXVd4WqLr0p0rw9Sx4G10NHdxUZHeJcxzqa/2yhGkUDsSDJepZdlKGXaKKuqStYHY9aGo7jfQXqCECgRl+4Abc2ukX7gJ8/gnacoCqQUVFXdSkNssK1wuOnARheUVEfTSZsXPEoInPUEF6PPudl5FhZm2bJlC/jAgQMHWB8OEErS6/V48qmn+NtvfztSSnZfcQXeOfbte4ZPfvKTrK+v85Uvf4Xvettbo/REyPa5tTb89m//NlJAOR6zafMm/AmLs4686FGVJbV0DMc2GhX4aA8vRGP4OfHp6xjv4qIjvMsM59q8mP5AilYKIuIOhnINNadBmSgh8TGlWx9X1Ckdtc6hUqOhEfkCLCwsUo3GjMfjmNKmGmFmDLau075a2da9Gmun6ebJtm3bWF4+Hlc7SomSglvuuJ3xeESWZ+zZcxVz8wssLixw4vhR7vvEJ1BCsrKywo6dOxhVJQ9+6Uv0ioJPfOITPPPMM+zatYvbbruNuq654YYbGI3HPPnEExw5coQ7X/lKRoMh999/P19/7DG00hw5fBhjNK959at5y3e/mV/7tf/AM8/sZ3FhgdnZWY4uH+fLX3+at979MpyzBFzb7IDmu9j4r0qHC46O8C5TnBPxpU6CSItoIBCqIdgxmAWQEu8ttbWMq4r10mF9NNCUKJSc7J992ctexpEjRxmVJWVZtbthZ2dn8d6zvr7eNhuuuOIKFuYX8SEwruLURlWWKKXZvfsKDh88xMqJ42TakGUZ6+sD9u7dy9u/5+1cccUeFjdtYv8z+/mLT/05x48e4ciRIxACb3rzm7jnR3+UE8vL/NS7383TTz7Jgw8+CMCJEyd45JFHqV3N//yP/zG33HIrP/ue9/CBD/xXPvmJ+1BScvz4cbz3VHVNnme8/g2v5w1veANXXnUlx5dXWVldRStNvz9DL8s5cGKNJ585yMJiD8KkwzvNcqFjvIuKjvA6nBJNBBKtPgOeuHjbjwZIPFJJFAFshSsryrLm8IllPJJca4zuUVdjIEZohw4d5uabb6WyNVuXtiCkYHZmlocf/hrLy8tY53HBMTMzgxeC3uwszjm+5c47KcuybRKUZcmhZw9SVRXf9V3fxbd+67fyxx/5CJ/9zOf48098kjd/13exd+9efu8D/4XxcERVlSAEr3r1q/hff+onmZ2fY+vOHbz3X/5LPvRHH+Tgs8/yuc9+jsrWLC1u5q7X3Ml3v/WtbNmylX/zS7/EB//w/+GxR7/OA5//Anmec+ttt/G673gdO3ft5IbrbyAET9GfwYXA3/+Bf8DXvvo1HnvsG2RSMA6aR57cy7e87BpE0iQGQjIHjWn+C1m81OGFoyO8yxSnkps8B6KJRpKoQgjc4AS5EdEDT2mcB1db6nLM4bUSKyXGe+pqxNz8Aq9/0xv48Ic/zOz8HLfedis7d13BcDTgwIEDPPH4N3j0649QlmN6vT6vfvWryfOcj3/84zyzdy+zs/Osrq6wsrLMli1b+c6//p08+KW/5LFvfIPvfutb+afv+f+yuLjIm777u/md//Tb/Pp/+DV++d/+EkJInLNobfiut343r7n7r/Hdb3srRa8Xl/sQuPXW27jttpdx+PBhDhw4gLWWm2+5mdnZ2bgEyFmuv+4a3vX/+XHWVtc4ePAgANu2bUdKQV3FdZDGZMzOzvOv/9W/YveuHTyzdz8f/NCH+JOP/AmF9Fy7aysieGxdkwcX//kIUUYj5Dn8DTqcV3SEdxnj7GYBzXeJjIIKqvVl+lpjshxpojuIrWvq8RjqEukdY+uZn53jf/nJn+Stf+Nt3HTzzbznPe9BSsVVV13NZz/zGapyjK1rfPBkmeEH/sEP8Pf/wT+AEHjNa17DL/3iL3Hs2FHW11ao64p9Tz/N1x95mOFwyBvf/Bb+yT/9p2zesg0EzC0u8o/+3/8vvuXOV3Ls2FEOHTzIocNHeN3rvoNXveousizDC0lVlfik32ucWbZv38aOHdujgWmWAwFbW6jKGNc6T1bkXHXNNW13WWuNrWuqqkJKRZbnvOIVr+DZfU+xe+c2/t7/8HfZc+UevvxnH+amq3YihUg2UmlsTSqkMlH87DvCu5joCK/DaRGXbcd9CyIEZPBxcY/WSG2QWYawJbaqKcdjdsxpept309+8gx/6oR/g2/766zF5zlvf9jf4zGc+w6c//WkeeeRRjIxjWL1+n8VNm/jev/d9/NA/+kdkec5osM6b3vJmbrr5Jv703j/FO8f+A89w+PARlpaWuOuuV/Hf/e2/zdKWrdHlOE1NZEXBK1/1avI8S7rBpE1JhgHCx8mO6dnVSHwBqaNrMkKmrWux2RKCj04rSTMYtYHRPl4bg9RRPB2EoOj3Wdi0RLm+wtYd23jXT/wvfGJ7QT54OnZnGy1eiNMVIk1ZdDW8i4uO8Do8B9OavWljKFGViHIFPR8/7CDSDO0Yby2FgO/7H/9Hbnv1tzM/PxOnH7xn09IS/7+f+z84eOgQn/zkn/PoIw9z9dVX8/KXv5wbbrqZbTt2tC4neb8PQnDtdddz/Q03IpKtunMOJTX9mRmUNngfCEQdnxBhiuRkrDuGSYOgMRwQUqJaK6hIlEIpVJK2kAzd49hYlNwYk20wK4g/B3ASY9LYmgChFAtLWxlnGTLPENqw/Yo9rD38eJzYcDb+o9GEicmmSoTOD+9ioiO8yxxnHDcTIa0PdHgBfriODiV5by5NOwh8bbG2RhCYX9zEjbfdQa9f4AM466hFjbOOLC+48aabueW22+OpZUznGi6Jkg2JNhlKqsl9IjqaxC1mTSrIxMqp3TwRkoOJAylpgr908onNuzYxkguTxT4xekurItv3RbY29NM2dd6ndBiJ0gaZtqz5EMiKHpk21K7Ce1i6+kaOPfQJgnPpGIstR6isgOBRUrRWV/E6py6gwwVBR3iXMM5Zb3cSNpBgY1ApPOPVY+Qasl6BNhk+CJx1+NRxvOH2V7K4bQcmy/BEk0udrJ0q75C1xiRikSHNkKYRseYqpGz21Io0LmaRqbjfkFIgEpFooqV0oc2CnehEklLx+CTxeESykpqQmxBxjja6H5+GbZqbp/zypBQ0Ts8ekETiE6leZ+uSxZ17cDKLBggEvKsj+fk0tSIk3tWnfs4OFwQd4XV4DloSDIKAJyDQAaqVZ1nMc6TKCErTpJOKaMl0yx2vjKNjUiFFlK5IqaMxZ0NQ3ifH48YeaWpOd+p7K3z2E1uqeGdI/JU6xzKkephA1FPngBQx+faxk/PLpC9sjhXPIbsgpk8SADkdMrapbZwcEUTnqRhh+hCwVc3M3CJ6bol6fITgHFgLQUSJj5JIqXB1dV7+Zh3ODZ1VQ4czollT6F1cvF30+ugsR6S1ilLGOlbtA1t37kQrg9AZWurWJUXIyU7YJolt98TKiV379Pd2EoEU/U2NpE3SztD80LqOtGTdRKlBJjqTCGLqKoWcPE/6fvrXLxAhLtxBikSEzVKgWEf0vk7R28Q5WUiJMhkLV1xDOazwlSW4gJCgpU41Rp8Ib/KedLiw6AjvEsYZP8jn8uFKw/8gcMM1Mj+mmJlBZQU6yxIROqqywknDzMKmVmoSGwdxwc30qkU5VRsLYuLEIk6K9JotZogYCQkR1/4019NsNWvsn6afA5j4z6VmRlOTkyKSLM1Xuo5wqrcjTBb/bLzG6UivqQ/GSNB7R21rtMkISJauvpnKBVw1wtkK56poaCBjuh9T2qn52g4XFF1Ke5nidBZRJ28qC2nWwp54ln4m0UUfnRcIoSLZjUvW14YsbN+NymYYj0cUKV3DqLihTKn2OaM91MbnPD2mDDLF9ND9xDFYRMsUfHCTR508NneKp2nmSALh9POsG/SIIdUUU2VQEGeFCfgg0+RYoDEZjUYJgk07r8KaDG9rfF0jrMOFELecCUlZN4TXxR4XA927fJniZLI5XXQREi2Uxw/Q6/fQvR6o6F7irWWwvs7q6io7r74eofPo5OvsxMKcyVRHmPq5IVZ5ipRywxRICO0SoEBoo7NmDWNb35u6/unn8kw9N016TOvGciqyC1PtjpCspZrHiUC7R5cmpQ5JBoNIwuRk/kmgt7BEb/M2nAv42iG9iKajRKt6710KL8MpibnD+UVHeB2eg4mde4ifxarErx+lmO2jTY7UJi6rGZcM1tc4MRiz85qbULlB6wwh0h6KRHrT9vCC0OrmmueK/JPuPSm3bB4Zfe0kQqho9y4a3dxUf3aD51/sLEe68hBc++XxcT44yUm8jymxDw4fYge1qTXGVHnj69jwmPQVUrMiEJAq2lUFQJqc+R1X44irI71wYON9XgqELemY7uKhI7wOZ0AMacq1o2TKk/dnEcbEmC94ymrM+uo6VRBsuWI3WZ4hlWzJYhJXbSSjliSmojhaQvEnEReNfqSNyNqI0cfobyKd2ThFcaqv1n04RG0cbfzmJ9c6dQ4h2l7JhvO2P8dLi3o+4jrHqCOcjCLPLS4hQpTaeKERwiCkhiDxVTmJTrsS3gVHV8Pr0OK5NTWBDILy+LMszPUw/XmUzmNhvioZjccsH19G9WZR/ZmTUtVoMCBl0809deo69dtJ9zVNCLHxiLS+ccP4Q7z457yOc9cYnnT/VB1zg0SnPV2KUokpeVS/RINUoRQQcKmZEqSit7iFlfRIbXKENgRbgwdfVQTvIgF2uODo3uUOp0RrD+U9bvUI/YVZdD4DQuFdjcAzHKxxYmWNLVe+jKWdu1hfGyCEQKmG6FT6uSG8KAmJUxFT+2pPeu5mjWHc8tWsWgypOStjmppui6t4Tr7uyc9tOn0asp0mxcnxsfMaryXdl35r3hefri89EC8AJVP0GlIEaREITH+GysfygDA6kXPsGNfjEd5ZlDYv8C/V4fmgS2k7PAfTH3xfDjB2gOkVaXjeIoTHW8v68WWOr66x5/qbUDJDGdPq2hoJSHO+OMkQ07pmYoIQJg5UbTchppYbRcHpHG23ND38NDngySTW3Dad1k4d0F7vVAAX0+UwuZ72f8ERpkhWCIFUKvoDyjiTK0IAV1OPBtTrx3HViPFoTFb0kaZHkDqWBXzAVeMUzXb57MVAF+F12IANHVIB1YlD9HTAzMzH7mw9irtjxyMGq2uMnWTn1ddSlmVahJ1GKqZSw0YC0giE42Lt5hlTkYwwlcYSj016O8TJKapPtkpTHV+SeCRNb8TIcapzO3m2JFKeurapSLPtzvpJ46LtOIdGytJ0aSfzGUKkZeJCgnPYcoQV8OX7/4xs+RlCCOS9HkJlCJ3hqzEE4rrItKO2w4VHF+FdZpiOfk5X42o3hgGjo8/Q7xWYYgapTSp7BUZra6ytrDO3eSv9+UWCjwt2tNEIJRFKJu6bEJ8Uop2smLqi9lpOlqM0Ql7v3dSQfaq5JeHxBs1dQ7QbX1EbbUoZx96EUEnMLGlspNpZ2iajZYokp+uSQrTnac5BEC2ht2Lm4FlbW2fL7t2sr52gwvPUk49z4sh+ytEAZyuEqJJHnqPDxUFHeJc5zij+tZbq2LNkRR+tMyRJ1wYM1tZYGwzYsms3yGjtFEI4yX1k47lF0/Jsfm8IJv5yyubG5DHTRO05de3tOa8uRZVqYhAgTj1KNpHHNClviMadIpoNSCFBKJDyOcHYRumNSKk/aG246rrbmN+2i21XXcuR5RH7nn6Wkegx8JKR6IO3ySxhcq4OFw4d4V1mONXu15PRpIluuEoYniArcoTUOG+TsLhmsLLC+tqQxW07CYlQolnAhLSaMTIhZTLUnIROsXY3aShIodKsa/yupE63RY+7EAQhuKiV85OFOM+ZGGm+2mhPNKqX9OJ4zmOnrytAitI2hnqTMbZJHTBKa6aj06lzkYxglObYgaPMyD7X3vxyZrddQbGwmeMnjlHsuBKKTQQn2n9ITve6OpwfdDW8DqfFaPkIhQapdTLcjBxSjUeM19cZDsZs2r4TnRcorVBq0rBoh/TV1OxsIrnTfZhjNDeJ2DZGcRBTTwD1nNS8ER5vuH3qp3hOWrJtub4htLZb3NTuJnXFVoQcSKslJ/8oNHOxjbtLcx1SSsblmGo4ZH5hkcUdu8jmZhABVp99gsHRI3zygfsp8jnC/fdy813fwezipq6Ud4HREV6HhI36NEFguHyMItMIraM7kg2IECgHA+pxybis6c3OY7KsrW9tGPyH2IlNTiYypactOaU5idQGiE8tSNbt0FDWpA5I1OFN6f28b8TB06nyhHimc2aR3JDbA2Ci6WuiuDb8POlk0EprJmqUxHBi0uxoIKWiHI1Yq9bpz8zgVQ/hDSjDwrymf9PNLM5kPPLwE+z9yudZXVnhr73tv6fo9V/Yn6/DOaEjvA4J8ZPdxDnKQ7lynAWtUDojiABBARJX1dHa3TqqyoLzk7HSqTRVpKaFlCktJY2HpWfZ6HDSPD+t5q65FUKcVAjJdDN1c6XUSJU0fdGQrtX3pYs5qaHRUGxMkaefo4kgQzP10V5W6iyHRJft6eKDfSOhESHJbCTIuB9j9dgRlo/vZ35uHlMUoCTB19HDL3i2Lm6i3D7PE6ueO177OvKiN3k3zlRb7fCC0RHeZYyNjiIuad4cQYAVMF5ZwfeTvMMFAhIfFNWgxNcebXLmNi3hhaC2FmV0O9Af9WnRg44Qu7NAKyehJUImJLMhIEv1teZa8WmkbdJYCE10JSfND2ttdCqZep2t7KTR9on0QBr9XRN5+vR9ktS2asDpdJuJBGai80uxqhSgBEpJhieOI9ZWKTbtQSgNSuBtjbAVvqqw1pFlBa94zV0s7drTkdxFQEd4lzGeI+kIEhEc0jt8XVKvrzDWgdoH/NoyIe8zHo4Z1RYXAovbt3HlddeTzy/iRfS/E1OFfSkkSstWF9dsDQsTzgOe04QlFsMEiGmBb/webZckQURLKIFoXZGFCATvsM4TpJjIRqAl18l0RNPtpVW6NB2PSXrdEGVqxIRJ1zhGp5NCoGgKnKm7G5QiE4I8yzHGRAJN/wi4ykYDAiFQxlBXFYiuf3gx0BFeByBlZIAXCoShGq3glGHdeR57+GvgLWu1R4ScTblBKMPc0gxexekKrSRZlgMxVXUuGmGCRKdxKp8mK1qa8KHV500iudBGWdNd3Mm9YSL8nb7+NMURkusJXia5x3RkN/WYk87dipwbPV1oZH0b52rbxkhgo1B66o0UQYBU9Ps9ZLWCCD42N4jvsTQK7zSUEpVllOvrUY+nTi2Z6XD+0BFehwRJEDVCeOq1wzz2+Xtxw2fRss+1N70SMzvP00/uRcqMfV/7CuP1ITfefjtZrwdp/WEjPZFpfhYa7vAtIU1DMEk1Y6DVjFidTHQna92ac7Wdh5jmuuhBp5TaEDCe3L2VMkadk3RVbkhl266tmE65J1c/TaCN64tAEIRHBpBBIKQm7/VxqwKCj1vSQsA5S8AhkRiTIZTEjQbU1ZisN9t2e5v3p8P5RRdHX4Y4bRQhBNKN2PulB8jWV9DliKVt23Eyx6oeu66+ns1z8yxt2c5sb47DDz/Eo39+L3U5IssMSmu0NiipY4omFUrF39tISQi8CO0SbREmEhQR0v4IqZFSt1MRApUEwFGXN0VNURMXPM7VWFdPGWpGTzq8A+9o7aDa6beJ9XuUkyQdnY87ZIO30cUkhKgEjEw2sYaXKWJNEWrqwkQCJNrIZ0WGqy1UluAstR3hawvOQbAInaHMLM6OGK8tpz/CFIlPJH0dzhO6CK9DQiSccjTGlSOOHj7EzqWtZPNbcKpAqh4Bh1NjbOWQQmKqIR/6hX+N+63f5tve8B3c/u3fyc4bb0blfYzUMT5pbY/icpsgQHpJkJPWgAgBNT35KiZNgia0anZJNGnkdE/XWYur4xIdLwLOepQyxOVkqW6IQGkm0pSw8bXHb9P6u0aDl66njQ0m0Wdc1dg0UWK67oVv6416bp6ytrhqjKsLpHeo4HG2pK5qlMnjVjdvGRw7yvy23fCca+twPtER3mWIM6v4BcvHjiC8oz+/jWx2E6o3j+nPY8sKlY2pRyVaSmbn59g232PfkQM89Pu/w9f++HfZfN0NXH3n3Vz3LXez69obyWbmktdbs482EFSMjiZmn5M9sRs6oE0XNYQNhqGNVUCs88VH18Fh6xKtNS51agUSDwihUJKk2fOEJt2eSotbfR4yNj/C5DqaFLe5yChrSY+cFkSHZvjOI4RiYfsVnOjNgItdWeEqrK8YD07gVcZMUSC0osgylg/tZ+fNL2+7M+2+jfad6XA+0BHeZYhTz57G34teHyMDJtPorACdEUT8gMsUhQXh6fULhBPMFz3mtubIapmCmuHTT/CVvU/ytT/+fea372DXjbex55aXs+PGO5jffgVSGxBRlSeCiOcWJ9XZ2jLWRuv26dpZkwIjBUplCAHLx48nt2FHKYf0en2kzlAqafeCaLV/jS/fxncgHtNMhjRE1uqLm1G1eFdLyBAIEhQSh8Q7hxQKLWcQJsONjkVZSvB4N+LEwSPMb91BNDEwGJMxOH4EW47RSYt3OuurDn81dIR3ieL0A/VnfBQ+CIyU6F6OygpMXhCkRgoZTS6T8eXMzAzlYMzM7AxLoqA6XmP8EJ3FVFJnkA1PMPj6A3z9yQf5+r0FMzuuYvuNL+eKW76Fua27UMVsnMFtpi+mr3nDWNnpjDwFQmikivfXtccSkMIBDq0NRmqUTqmwnzyHP2liY1ooTZqm2Pi8ze9T1xACgvh+eALWWySSGEB6tAiovGBw+ARz2kCWQwA39igUQmZkOfiiZrA2Yu3YETZdsZuutH7h0BHeJYrnQ3bNhzwIRzlYR9iaXq8HIm4Gk5lJKVzUomVKU8zkBCfQhWFucRsr4xXEcIiUDpMZipk+M7MzLC4tMjs/j8nyuMD7yKMcOPwkrphhZseVbL76Zua2X4nOZtLEQyMCbmp2xKgL3+a8TamtXeSNABwClZJdS9HLo2tL432Xanmx9qZTHS/lpc24mJgyJ006PSElwTuCc7h6xGj5GK4cgffYuoyEFzzjwRrVYBVX1VR1iatGlKvrfPgPP84bXnEjS1dcgVAKN3YceHo/i1u3I1AYbXA6Q4UR68vHWNy1K17Pef2voUODjvAuQ5zSBw+BFI7lZ54kVCVqro/0ceWiIi7c9nWFqCtmZ2fQRmLyQG5y1NZtuOUj1MOjKKI9lMkNvX6ByTLyXkHe62GKgqw/g8wKlNIEhgy//gArX/8S+eYdzO68mpnN2xFZka5paugfAd7hU1eVMJFteO+pqgohQXiPtRVgolsJAlRj6yRw3iKVAqmJm8+mzEhD07mFYMeU68usHd6PXTuGXTsO9Sg2eYUgzwxKS4K3+HKIGA2QoxHYiqweY8cj5Khixg04vO65RWeoPMeNS7788GPk/R63LCySz83gbE0Qgtn5hdRgaVL6jvbONzrCu0Rxckp7clp4yhpeCBx/+jFGw3X6WY5zDlyyOK9LVDlkOFil6PVx9RCkQyvN/I7d2LVlDu97FHJFU/9XaUIhPelUE9RFm3NlmC10jKDGh1h7ZB8rpke2aQezW68iX9iKyvLJ+JeMohTfOJskIbPDAg6lBNbWGK1x3mNyTaYEwqWxOR3XR3pACZCimRyWEBzl2jJrh59hePhpqtVD+PE6ubdkWUaWF0hj0EriQ0CpmCZLAUJprFBoY/AE8A4nK6RWbN06y4nBEHwFoiBoGZs2oyFhNELM9bHVkHzTdhZ2XEGsBE79TeKbd57/67h80RHeZYLn+MZNTRBA/GiV62uMjh+i1ysYrA/YtDRPEnaAq/Hr61SjIb25OUbLNSGMCAhmt+9iPFzl4GdTZzUAU4ag0zOnUeihkAGwZUwbTYE0kr4S4MbYY4+zduRxVqVBzm+n2H41/U070Fke0+zUERVph60KAW0MxmhwktmZHP//b+/N4yS5qjvf7703Ipfal+6q6mp1q1v7CggJhFgMWD2IxTbYPN7II89gzEM2lp7NeD7G5vNAfqMBBAzPI0vjQc/+DJstzJh5Y4wxCGSJRQLRWkAgoV1q1Fq6eq+uNTMj7j3vj3sjMrM6q7oaVXc13fGDVFVlREbeSFX9dO45v/M7KOJSCROVQPuii9EanMPgkLRBalOmdj7L9I4nmXruCWRmH2XjqESaijGYKEbiCBWBiRXKaB9hkneRAWCtYEolEIdNU5Q2RHGJJHWM9PXwxLYn2L/7DLpHYvbt2U9aa+DSBFyCajRI05ThU85Ax+Uj9wtQACgI77hCp8htucULpWBm4nm0TXCpkNYblIxGuxTlLNTm2fPcdnr6u4lKZZLZCkbNYaOIysAgA+vWk6IQFwjPtRcGnHWY2FdHjVg0oTNDA9IAZzBRhGiIVYKxKc7VcfueYHb3ExzQZaQ6RN/4qXQPjxNXe3yFN0Q/URRT7enGpV4EXasnGA2YoJlToJzFNmrsff5pdj35MAe2P0I1maKnrKkaTVwuo+Myog0SxUgUoaIYHccYbXDWhS6N5kwMEYWJIq/9i2KsbuC9W3xetKtaoksJe7Y/TRL3s//ANA1r2b13PyIpttFASl30jKzr/C+mub8tsAI4IuWg5557jt/6rd9ieHiYarXK+eefz7333psfFxGuueYa1q1bR7VaZcuWLTz++ONt19i3bx9XXHEFfX19DAwM8O53v5uZmZkjsdwTFm3tVsDubU+ypq8f7TT1mWmUS2nMTSP1eVSjRmN2ikqlRBRHdHX1UDJlyv29mGqF7qG1qLiEs029GqH1yjlpe09LNrwGlGhcAuK8KQCi0CYmjsuYQFSxhl5Vp7T/SaZ+/A22/8vf8OS/3Mzz99/O/qcfoj41SVprUCqVsECtnqC1oTZfJ5lvsOfJR/nJP3+J733uk3z///0zHvn//oLaQ7cz5PYz0BXnREfoC858AEQEg2+wcIKPFBF0YFJFiDIV4XXK5yZFgXgdXbUSs6Y7Jqpb0meeZFDmmarXeGLbs7j5OnGlgqr2ElV78o+tqctReZ6ywMpgxSO8/fv386pXvYrXv/71fP3rX2ft2rU8/vjjDA4O5ud84hOf4IYbbuBzn/scmzdv5kMf+hCXXXYZDz30EJWKT1hfccUV7Nixg1tvvZUkSXjXu97FlVdeyRe+8IWVXvIJg9Zo76D8XiOltm8Xg9UKA/0DPPHAA8yffQpdto7SipnpKcZO2kgclRGnSENBwXT3oHRE3NNPtXcQN72bVCKcxldbrR/pKKHyKiJoAXEOsSmSVUYlBfE+cpnJpzYGpctIapHUEmmNiS2VdJZ0ej+NnY+ya/8Ue2YSbNcIZ77xX2P6+kklGBKoCJc6vvoXH6Y6u5MNG9ewZmQtlCsopSmVSn6Drf3QIW1CwSBUflUQHLvUYpRFGW8KIAqa9lWuWf2NDGKNb9FT3v5KR4ZqTy8NK1R0HLbeMWt6++gaWosudxHb+kFymwJHBitOeB//+MfZsGEDn/nMZ/LnNm/enH8vIlx//fV88IMf5K1vfSsAn//85xkdHeXLX/4yl19+OQ8//DC33HIL99xzDxdddBEAN954I29+85v55Cc/yfj4+Eov+4TBwuJF3skAKINPvItj14491GZnfaRjIpSCUqUH16hDapHGHOn8HNX+Ef8HXirTM7aemak9PiJyGSG0dkn4Bn9tHCjtxxO6FKW9hTzifIdEkIRknRRNAbAfwu1UjNaOOFIM9VUolWL2pynaWd+q5VXAGK0x4ijHhoHuMtVS7P34lKCCbZVSGufEa+fEJ+a0+C24NhrnLFr5YkiktTf4dA50pucTxLm8iALBBj6QqClHWF1iaPwshk49hZ1PPcJZGzbSbywRFusa3qEma8FTLTFdwXsrjhXf0n7lK1/hoosu4h3veAcjIyNccMEF/PVf/3V+fNu2bUxMTLBly5b8uf7+fi6++GLuuusuAO666y4GBgZysgPYsmULWmu2bt3a8X3r9TpTU1NtjwKLY6GBgI4ieoZHKUURo+s3sH7DJib2TDI/W6dhLc899yxJo4FKaux9+ml2Pvko1f4ujBHA4rTQu+Fk0pC/w4mv8gaIC+aa4nDOkrkeZyShle9WdS5FxLueZDo5bzeVBiIk954TrRGlMApiY/wWWLy4OIu6jNFUu7v9L7oISSPBWU9QrbNns/YxsZnQz29Vs84MY0xzuykul6c0hxV58ycRQUcmGAwYtImJygZxCdpZ5mbmMaKZm55k3/PPMTdfY2pmxhsQFDjiWHHCe+qpp/jUpz7F6aefzje+8Q3e+9738gd/8Ad87nOfA2BiYgKA0dHRtteNjo7mxyYmJhgZGWk7HkURQ0ND+TkLcd1119Hf358/NmzYsNK39guNQ/msOa1Ye8rZ1OYbRGiqvV1Q6ebA1AHENujq7qYxtZ/JXRPMakPX8BiVvkHieg1bb2DRDG08Bae1j6JESNMUlxOVCy4kDo3CWT9r1ktOWvpUFX4L7Bw4LzwOo15btpkOcTa8wIAqYUpVVFwi70INOTBtYnoGh7HOYm1mP5V9zXplXS4ullBpddai89mzKmu1yE0NxNmcGPNuXOWFzsaY3OZd6Zi4v5sndz3PI489wjPPPcfs9CTlcolSpYSkQrmrG31EsukFFmLFP2bnHC996Uv56Ec/ygUXXMCVV17Je97zHm666aaVfqs2fOADH+DAgQP545lnnjmi7/eLjE7kp5zQO7qOGXFIOkdJUvY8+hR6ahJTm2LzKaeRNubpGh6id3SEORS16SnUzBTzzz+FwTAwOo4plxGtyGbHigtk4pz3q3M2FDJs3uLli56ZIi7IV1KLWMHZ8LpgJ+Wc4CQUOJxGxG8fTbULFWQdvvvBmxJYDN1DI9QaCWniCygKv64kqXs7qLA26zLTAe+EAn4YjzIRaI0TwvxdlxOlt7LP7LCyjhWNMd4wQUUG5Sy3fvUW7r71W1QizXBvxIWveBnV/j6mp2foXrcJpzN70AJHEitOeOvWreOcc85pe+7ss89m+/btAIyNjQGwc+fOtnN27tyZHxsbG2PXrl1tx9M0Zd++ffk5C1Eul+nr62t7FDgcKHS1C9PTgyR1evt6md27h/rkARq7J1CSUq5WqHT30RVHaBH2T0xQiTVTD9yHTO6ka3CY7sHREKn5SqML1kmIIC7k81waCMyFaFD57WU4x2SDr0PF1MM7GeetZ6IQB068oNiUe9EmDsXNUHQI28+B8Q0kzpGm3ovOJj6awzmvA8RLS4zS/meV6eyct2IP23FCHlFoVmY9MWY6Q41GgxO0NkTGX7Ncitg0vobzzjyN6V3Pc/aZp9M/NsoBaxg+/2WcdM6LyRxgChxZrDjhvepVr+LRRx9te+6xxx7j5JNPBnwBY2xsjNtuuy0/PjU1xdatW7nkkksAuOSSS5icnOS+++7Lz7n99ttxznHxxRev9JKPWyzHLjy3L0ehiOjqX0uK47Rzz6FU6eLZ7buZeOZZDuzdh9aayT17mN49gZ3ZT21+Fj0zy8j8ASa//iVqk/sYOuU0jDFoCYO0VeZO3Nos5YJ42G9bXepNN3EOcc5vPYNjSb6dFULBIeTOdCjAhN7YOBgRNN8luy9hYPxkrEDaSKnN17BpSpqkIXIMZ7lswLbFifMFlfyTCe/tJB80jlLBesp7yIjYZpTa0l1iFahSmVNPORk7O81QFLF2bIyG6WLg/FcwcvYFqMhQkN3RwYoT3r//9/+eH/zgB3z0ox/liSee4Atf+AJ/9Vd/xVVXXQX4X9L3ve99fPjDH+YrX/kKDzzwAP/u3/07xsfHedvb3gb4iPCNb3wj73nPe7j77rv53ve+x9VXX83ll19eVGhXCAs7LTILpOqadcw1avQP9XHBBRfw0E+fYN+UxVSHSdOIiccfYu9jD9OdJpx28gYq1ZjR9evRUqdWm2ftGecQD66BUhfO+NJE5gycTS5TtObhfPO90U3tmxOLTb3jsLd18jk5CVvRbDS2KIXSBicaXal6J5QW3sickftG1qNLPST1NFRUs4IJOOvatt2INxTFST5zIxu4nV819+ULBOlCFIvkbsjK+IKKMoZSqULFWfpocNaLzyet9KI2nMHQaef7LTAF3R0trLgs5WUvexn/8A//wAc+8AGuvfZaNm/ezPXXX88VV1yRn/P+97+f2dlZrrzySiYnJ3n1q1/NLbfckmvwAG6++WauvvpqLr30UrTWvP3tb+eGG25Y6eUWIJAfDqcslTWj7HaK+QP76R3qY2jdONNTdYwuQ1xi+rmdVLuqpFEFuoaYtTEHBjbRf+4rYHicuKuLyvB6ZGYfmjQ046u8v5ZAaj4+0p7AVIpyfv6rNgpx2ldrrcsrthKiNicu1B1CYUJpRJeIq91tZEd4L4BSzwDVNetJdv+MNE2x1qKs9d58oSibWkuUEVsW9ba0xGUFlTx3p1UuUM6jQOXvQRmNdgZtfAGjUY7p7etmbdcAun+UdOPprHvJJWAMTgladPPzKXBEcURay37lV36FX/mVX1n0uFKKa6+9lmuvvXbRc4aGhgqR8VGGAkxXH7ZnLfv3PE1/tcq/uux1/PDu+3n+8QfZ/eyT7N27j1PWn89eysxTIh3byMBpZ6PiHpzz5BL3DVFLZrHWkVpPVs5l9k6ZfXow4cwchYP8Q+U5MXJiy6qkSmtIbXOLiyAYnIqIurrzNqx24hB0bBg45SwmJ57EOUdqHco5tHXEyjQdlcPwH619AUJpPwVNtAPrixPZWvLhQ8GZWRvtfRa0j+okTUL+0c+jVf192NFT6P2ly+jfdBoqikOkqfMFF5HekUfRS3sCYDn9tIJCicIqzUkvfQXPfH+K/VMHMDbl5HVDuJ3bOGv9GLsHBlCnnkf3uo3EQ6M4DFYpXJB6RKUypYFB7ORuFCmB5docfIWs6CooDUa3sFTmhELQAWtPhn4MbJCAZGQTRsMqE2GCU/BBrCF+Sz1+5rnsvPOrJKklSh1RSfIqcZZrzFxYWl1eJOQVldFBv6d8BwhN41C/Pfd5ySweRUEUGRJTQ7uI6qkvYuMvv5Xq8Kgv6IQI1yPkLAu6O+Io1D8nGBafWOajDYXDDI2w6Zd/ne5zX8YBK1CJiUolyr199K47ie6zXoJeM06qNBYBsSjxkhKnNNWBEUxkUNoEOUc2lzXUIp2fBCZZB4YKUhHrq6fO2pALA7QX9erW6BBfpVVZhBjF6LjSrFWolpAJsGIY2XwartJDkjjEhrwg+E4J8cUHlC8eOHH41OGC/0ioludyVm4+n+UAfSrP6wuVeDfkyppRuobX5tVcaBYqFhZ2Chw5FBHeCYTF3FTAR1NKCVqE1BnScjfdZ72UDWMbmXr6SbAp+/uHoHcAabMxao3cvD9d1+AQB+Iy2tWC+iSTozQH8eRDevCFA2MWrK1txkWzfUuJYF0oW4RCiIlilPEE0laICQ/E0T04StfIetyen3lxsbUYkTzHqLUJ1VjnuYiwvc7bO4LMpaUDJJ/bnS/Zn6ebZZHw6Thso55dpi1P2LrWAkcexz3hHf5ch19sHM4si/xc5efBWiVYHVqnLL762TtE/7mDiBVSsaFo0G48AFmC35NYpXcQ4gokMwjO84NtRme5ls5ZxOlgo+7CVlWjlY+ylIh3X8mI0omfraEVLm1KVHxXg2lZRyvZAMpBVGXt6eexc+dTvnCR+i2lH6YtoEIUqiJc6ABRWre3j2VXFT9YXPKv5O+XdY640FZnRXzkm9S9DAcTSjAFxa0Gjvst7XK0aCc6FKBF+QdZBOIQbf2cVxwpvv/VLEKmWfyDaEy1G9PVHbaJTfGwl6fgt7At4l6xNrRqOS/2zcgPHaaLNYlEcHn+zBc0/ODurDdr4b9vvyZFKsK6M19MIpoktfl7WOuyJo6wvfWxmXPZ1jcUMcK6WiPULDOpQoFDZfq7FhMBpcOIyKRGNlO34LrVw3FPeAUORuf/CIQqKhrVzLahxRczFF7jJos0fWZFVUGhTEy5p893SiiwGZk41zw/dENkYl+bJmTmAl6Kku0cs+b8FmeXjCCza2nvTdd27TzKCzIWLGs3noqudGFt6J5QKmy3M10foTqrfRubbZJcPgM3Kx+rJumhsnuX0DbnmhXcsBUWm3rxMrRmAQocZRSEd4LiINLLlLj5P7NJXy3pJlFtr22ziFf47R2+46A8uBZREdkO2AE2byXLCg++squVCvNkw3F00zRUBdFxpr+TJlFKZiRgdFtKrH1LL7iwha70D1IZ3ej7Zq3X3rmgqUNAYxAJspOwdRUJ4ud8PWHLKi50WXjSzKQySikkk7QgGC1YEZytI5K+sH9pBV4wCsIrEJCRWLtCokluB+XZ2xHy+Zl0pKtvAEyEC0UGF8gsTUNnQu5cEiIladIs0OxfDZ0QeUdETsr+q9YmNO+33Elb4aJl5KGJGTn1bKz1ZOltp2z+/v49sq6JjOybOcrmusiCvNwUIZPZiIj3wwt9wSq81iYJLrX5qgqsDgrCO87RKRpbeLzTOYd6vlOE2KoGKff2+2ncSuXE4qxFhZ7UrPLa7J+1+VYQEbQCEZ83JMha8mm1IXfn283aixYH37vOsotYFGOnn0/iFNZabOrJzqbWE11L/rBVftLaFtdqnmqz7W5IAuaFCLegmAPgHM4VEd5qoyC8ExhLkeDC79vyYh0LF96rLvujN5Vuomp3iPiU98bLIz3Xks/LtrT+V9HmBYwQyQU5ioTiBiLBgp283Uxpb6t+0CQ2X7FABTspwTG44RSk2o9zftqYuDBz19pQjVWhj9avTyvdcr1mDjKL3rTWRFEUTAW8rVSTHD15Gh2cmltymAVWBwXhFQAOjgQ7RnEdzs3Py5L5WRuFKVHu7iFT24lIHuVl/naKdnLVbZ5wIcpTXrjbtoUMuj6fY9QoE3WuA6hmJdUTslDt66d7dIMfIuRcMAa1uHxLTdOd2TW3sVn0qbMWN0WT2PA2WL4AI22Oyt5peSHJF1va1UJBeAVyHM4W9mC9X5b38sykFZT61yAo327lmto7a5M8WhPrwFpUkKRk21cVSp8qk62EooUL0ZfPAQJa+9awBeuGZrTZWpBRUczazaeRpH4r61KLTbwzi00TrE3J5m/49dmgVcTr8nQQ7iiVc7OEvJ5kZOdvFW20d3MJzitNO6oCq4WC8I5DLKU97ERch3Nup+0uQNNWPYNQ7hv2ubUsya8IYmPrbZWClbvL7NXDrAtvEGrbcnz+iqC18n572ufsXFsXRId7yAUzfgvs0Iyedg6pMjTShCT1My7SYD/f3EoHiYyTQM7kpJcXM4SWyrInuCiOQtdGMCgNshrnrCfpFvItcPRREN4JiE7E1XGbusj5C6+Vfx8qm4IvEJR7+hATkVmiOycktkliOamI31qKCyMdrQsW79a7HEMQKGdJ/0CAEliowxax03qzAsPQyaejql1Ya33VGB/tZRVbCEOFlA45xGZfRL6VlswLz2GCLEbCvft8pW+zc9Z6rgzRbcF1q4uC8E5gHCraW0h+i32Flu1jts1TirirG1Wqhkin6ZfSHMzdHOGYWavngrZwPMuSqbDNFMmquVmrGj50XCCIXqy9TkSo9A/TO7YRmzok9TlF3/rW7IHNiyBtLw75OmfzggVIfp7YbBiQC1vj7PPIprW5Ttxc4CiiILwCwOJktlgx46Ctbet5oVqr4xJRpS8YR/l8nd/WSm4kkM2hVUG+kvnjAXkBwHdVZEwa+jlCVTTb0jZ1hMtgFGMYOfMlJNZXhZO07iUvwb7dBmLybWNNUbNIIF7JRNOE2bVhW+u87EYHIbOPXrPKc3YfBVYTBeGd4FhOBLccDV9rQOULtg6lI0o9faCMH1XrwghEl1Vr07zbIo+U8hmvoWIarKea4xSb2jeVRViHO/FLHOvOODfk8dIwwyJUZPNe2XBFnd275NvwfLsbIkyv6Uvybas4Gzg4/MdBIG85Kba0q4qC8E4AHIq4lnpN689La/Ja/5I9cTkUpb6+QFTW696sDcO2MwcU18zChdyey4oaIV+Wt5GRnZhFWwqDLwwcDpMop+gbW0/UPwwWlDVenhKIuCmPafbGNr3usmqrj958n7GX0PitdZDRaP8Z5eamYn3qURWMt5ooCO8EwXLyddnXQ25fD/G65kHfYmZDb6w4SNPmjNnm3rW168KTXfN4M0fmixdZS1nLeylfVlgusQtQqnYzNH4yifWtb0r8RDKRzMlFtchfVMt6vHTGWtsyT1fleUSlNNoYtNJhkHercLkgu9VGQXgnIJZVhGghu+UQH7T/QWfbvXLvAJhK3uivdMuWNWwBnbW+ZzbMis0KFIoFdkzSjAT9gO6santwpXYp0hMUog1rTz2bhhVsqJ5KID0kXDvXBmb5w0DG4SrZBDSCGFnw/nfWWVAQmaZl1eFE1wWOHArCO0GwWHRxWFq7DlXbpXKACETVXnSp6unLNZv2CTk58i2sa/adZpVNfPHDhrY0XwVVeUSYOZocDpk0LeJh7eYzSXVEkqbBFy/03KYpaZogzmZTF/0arGspIAtKfDXWOefNVFRoLUP7HGXw+DORLkjvGEFBeAWA5RHdwucXu07+VYEuVYir3cFdqbVVK+uecC0qN4BgGpoRSyb2hdwZ2ZNT5lKSCUkO9hDutFb/nN9S942sI652YVNL6pr+d855YnMt+cQswgw3kOcbtQgmFFp8wdibliJhwHhLz3AhOl59FIR3gmC50cVyiG4xIjn4eYXSEdX+tTin8qJDHu25zJUkG8WY/Tp60W62zRWbeskHLiT9vVOx98YLE7xVJk9ZGrmjsRbi7l6qa9eRJl72khFbpknxBRbJSSsnvyz6zDSDIefnTQ38VjxrvRMy4lve+gocWRSEd5xiOZ0Riz23WLS3VAS42GsBKsPDiGhS57CpDUn/NGz5svycz5s55/N4zbxZk4gy4a7WGoJvXUZAnXR4bYO0WyDiycjoiOGTNpHa9gjO5xhDZTjzx8sCtBAdZoTmr5dVcYPw2KVgU7ICh85yjAXhrToKwisALK3HW3jOcgsZmVikPDCE0iUszSJA87Uh4rIpyvn2sWy7mxNWy3pyDZ7Khnur5pstcj8HP6dzAfHQ+s1Y/PY1q9I6CZMzMulN8OvLkAmQsx2qd0SxSOofPseYIraBTRs+36hUmKxWYDVREN4Jik6ElT2/nK+dcFBhJKhPyr2DSFwJtuqpFxyHXJnLoiPXHOyTWSll52jVzM+J+GhJ8BXV7PlMlrKcCNaf78l46KSTsTpuieJU7ohig0VUswOkZbRki2OLa9EUZlc3SqNCdOe3xAoTRRzEzAWOKgrCO47x81QFD5WrW+rcg88XUEJc7sJ09+IsJKkjSZJQxfRkRpCwOJXFe3hXZJcN5/bklyY+esoKGVrrpiVTeLtO6z54a9sUMncPDmOqvaH/1fviZTm7jGa9ps5XWrN9ryLT6gWiCx0jWS7PBt++bOC40hpjjvupqMc8CsI7wdBJnrJcPd7C81uPd7oeeFJRxlDuH0REkaYu3xJqsjxY6FRoiaIyIlE0t5RZtOVs2BabUM111hc8FuHnzkTsdXOV7j5KfUO+Upv6nGJz6wyoMB5Smi1tItI0/Mxa4/JaR1hj1vqmsvGUGhPFS/2rKXAUUBDeCYblFjMWK1p0+r7153ZZSjNZ3z2wFqsAaXE/TtNgyeRlJkrwubVMtpKRSj4PNhQJbBjVGIhGXMJSv8oHkXxLVKlNTP/oOIlrvldm954RmCjCFDLy7amfh6vI+nv9nfq+WR0IXcTh0jAVzRiiuFSIUlYZBeGdgFhqq7qULGWpCHCp6/mt4xBEsRdtWNe0es+cRjJrpeB+YmI/iSzrfjDGeEeT/P3CdVyKpKmXgyySlzwYWb7O97wOjIyR2mzsY6tWMBvM3TQByEwMMi1e9k6ZiamzKdam6LDdTW0KIui4gjZRB7VggaOJIqlwAqFVSrHU8Y7RWodzltMbmu1Uy30D6LgLSWtYJ6T1hERnDsHaD/gWP9k2i6wyCYqQ5eFCZCjeX8//11qQpIHvzO9cBe1ctAikCfSuGfPCY9c0KnXOR2beHUV7jtMW7XRu85RZRKngnELLlteGSNE5h0sVOu7KZ2AUWD0UEd5xjsMpPCx1fDHZytIyFQjpfUy1G1OqgtJ55CSpxaWJn2eB5Dtg31ixoC83J+JQQNAajXcmtkm97fxO93Xw/A3yvFvfmjGseDF0GpyPQeUeB1krnPJ72iA78QezogvOhXvJ1umvr41GiSIqd+UdIwVWD8W/gRMYi+XjFj63LM1dx+1v2MAphYrLxNWebCQtToQ0TWjU6yRJEjoXfN4M5wsa3jlUeaGwU82ozDW3n0qBSxIO1bLVXkjJVcQAVPr6UdrkOcMsV6e1yYmrxQs576Bofi4h2guDevwQ7pb5tkphStWfq2peYGVRbGkLtCH7I24dON1pS5t9n6HTcwCiHEo02kSYrj4aJiJpzKOcw+gY0LlERAS0+JyfhC4y61Kv+KC5lYbmvIlUEiSp+Yhrmf/5Vmicf2cQTbmrB12uAIKKNDrS3jHAa09o9bDLtq5YC1qFYd6+I8Ozr0Ca+FyeC47HYqmUq77iqwol3mqiiPAKHBaWigYP7rTI/ud1c1G5hNJRcBVWELaRPs9lcdb3yGbDfZwICt0mE2luM0M/bealJ+3rWQoCZCPWBKh0dWPKVdJUmtvtUDmGZl4uI1xrbegOaXF5USr4Hqjc2VmyObzOEVUqFFS3+igI7wTDcuUn2dflylA6Ph9ycihPeyYqo4yP6kLWLpAe4AjV2+ZIRtUi+M2MNJuTyvz3URQ3t44d7rVT4QVaqUdQUYyudNNI/OCdJLG5a4prIa1ckJwRcOjOaH0vr83zEZ9tJNhGgjhpdlk01TUFVgEF4Z2gyAhgIVEsVplt/X4psXH71+wBKI2OYnRcQenIb2e1QdA4K/ls1yxbprVBaZ0P6cm2vPlQ7ECCkvfmHloik68/c2PJ12qIqj1YK9jUr0Nrk5NaVqBovX7muqIhaO58BVlQXn6iDCAo53BAVCqR5zQLrBoKwiuwLP3aUiR36Ov7QoEyEUpHKG1AewJU2gAKa52feZGlyMRHgF7w6583ofneWT/HNvs+EwMv5z5za6fc+8R3QXT19+dzaSUXIUuz1UxoGyAEvrCRpmn+c+aK4mWF3hTUWl/1bWsrK1hv1VAQXoGDsJTUJDu+8NzW55tfPbFIIIIojkHA5TMuBFHN4wrvKKKMIbOB92o7AyqMOww7WgOhgkvQEXdmkVbNYOsISJRkVnYoLXQNrkXEeumIaFxo2801deICKQetXth+K8/mPtrThqwp2DmFQ2ND90V402xVP+e/mQIvFAXhnQA4VOS2Utda7Hgm3dBx7LeoOkIwOdEJnsyaFvBZBCXBK0B54a9IcBMmzKv1kZcx0aIat1ahdMuzPrJr+b6nf9i/t/Vb0DS1oVPC+vWlTQv4vBCjFL6HN9xD6BpR4g0HfCRrUOh8y94i1imwCigIr0AbFiO0xYoWnX5ebHtcqnZjBZwyODSpFT+5MOT7vBGAt4ASUXnFNGvUV8oP3s6soVzQvSlTCuMQF887tn0Nz2f1AxFN/9AarFOkaYIV59/fCmliEQc2bJ2zflsr2UyNbIuriEwU1hjcUbSfXiZhK13w3OqjILwTGMvJ2S0WsS08b+H37fDUUuruwxObwolv6/JOIgalTB7dhY6tPL/nnIQtrs7sA3wRRPvCAOWqb89oKcAs5nacryfkFf3/Nb1rRkhVTJp6G3dNNlejKZ9RKF+xDe4nXpjsiVbCnFrwmsPcuj4UP7KotSC91UVBeCcwWiu0S0o4OuTzllPAUDSjLhGhVO1GRRFpmuLEhaS/JbUOCe1ivmjhPLEpX63VJkIZEyQqGh0IUmn/MNUeRC+dT1y4sozL/FFHta8fXe31ZGsdWMldkLOpar5A4dvZJOTwmkOEPEGryEBkiEolRBSptSyzplLgKKAgvBMQh1Nt7RTpHer1bTq85rNEpTJxuYLB+v7ZVicSrVHGWy4prfIh1kprtGn+mvrnmrkzhyLu6mG5jNK21mbFg0pvL5X+AZI0zZTJwcnYR3zZyxSQJt7WyqYJaaOBSxIvT1EKE5eaA7i1jwytZNEqtNZNChx9FIR3AmOp7epSouKljrf9DDmhgQIt6FKJSBqUlKAysXGwgNImQkUGbbxshSDlMMFIMxUH2ldytfEtYFbHlHv6W3ezhyTo1hV6AhKUiRlat4FG4sMxhy+QGB2HrXMQFac2WM9bbKOBa9TB+ufQodIs3rpKhdBOiBaYfxbh3mqhILwCh42lCgILzvTyj/xHTVSpepdgcb5dVSlfxQyzKrKcmA+pfK5MgvxEK4OIQpsYZSJSATExcffBEd5igurs+4MIW2lGTz2bpOGwjRQrzguSw1Y6G/CTZjNqUxfIT0jTFJukuCQF54iMwSgNFiS1aBMTlyrZJ1LQ3SqiILwTBIcjPzlUHuxQnnrNH7J/ZNZPmnJXL6IMohRRHHk/PBOBaF+htYJYQVmH8uNiSVPnj+cFD1BKo1SMKndhKtXDvl9ZUOAQYOSUMxBTImk0SJ3f2joJM2YJOTqlsdZRr9ep1xteLB0KGoggOJTxW/TIRL7nVhlMuezfK/9HgdVAQXgnKJbfJdG+hV1u4SLbxraeIUpTqnSjogqpU1gBHUWh5awEyuAsSOqQ1IZRiM1rtF1LBEShyxV0qdQxajos/aEI/aOjlAbX0khs2Jr6ogpZT28QTNsw8CdNU2ya+j5hY9AKXKNOMj8HzmKMHwdp0ZhSJf8PQMF3q4eC8E5wHE7VdbHXL5Uzy1r9cwKMSzgMTrTvolAaZSJEG5wxYAyiwrFAYxowC94jG5ajTMnPe12CRTqtsbVFLENUqrBm06kkqUIS/5zPvSm0jvIeX5d3dqhcy5cmCWm9jtQb2Nl50tk5xFo0CmUib1mfh3cF5a0WCsIrkGPxLolD5L+WuoYEH8+guxXne0vjckxUKmOiMOtBZd0LvscWE3l1SBI6HFRo4A/XBO299uISiihzezpoDdn3nXJ67RVbQCnWnHIaaSqkwZRUa4N3oQqV4UDETppavszRxSUJjfk6jXpCvVZnbmaGeq1OqdwVCLPAaqMgvBMIy9LOLbJlXaoq23qsXdtHmzxFAc6maJdQjgxxbDCRr7iaUJ3VujluUSmFigyiNaIMplQiLpdRkfHiX6MwcZxr/RbDcmZvePKCtetPIsm6KazN+2Vt6LKwoWihaHdeFue85i64HzjrSBoNnCkRVbp8nrKgvFVH4XhcoCOUWv7An4UCZmgWAlTb84I2mlKkUViMDj6cJmjwUH5WbajSKlFoo30jfuhk0Mpvb62zaGNQJgrM6gVuy4lSO0HE28p39fZjs8FBwcVFmcyq3XdXOLwLc0Z2mWOKCyYDvhkO4nKVOKqSKo2OorABLuq0q4mC8AosiYXFiuxxqKhJqRAyKcklJnnrvdZEUYQoT2g+qvOCY1999eeBL84KOtgzZbMk/HvkImQ6U8jhVKb9m0Gp2oMuV7HWBQOAptOKtdYTc/Y5aBN6bjNjAW/hbkUQbZDYYOIKFsGYFh2eav1PQYGjiWJLW2D5+bhFji+29e34chFEaUyphI4jjInQSqFbCTV0XYSknidF47e/GcEZo3M7+GZer7l9Xbi2xWZuLFgaUblCqbs7NGG01oizqNZ/NcZgopg4LuX9sja1JI0EK2BNRGpibJqgXJJ3WtCy1gJHHwXhFWjDcrZ/2XnLea7tuPdy8jNog+DEOecrtbr5yFtwVbuEIyM+ZYyPkrRGXIr3XOm8hsWs3zueowhdHBGIJXXWu7kE6ykXBm8r5fONfq3BHkpU0O15HzylDGIdKq3n90LmCFMw3qqhILwTDMvVpi2mv+vUXrZ0i1rTxESUwkQxFmjU69g0BXFI6I313pl+y6q1Dm1moZ825NVUZgOl/UNciq8DH2odS993NnoRDSYqtWydFXGp5M1LW7fyqjlnw2UTy5TyRKg0CoeyCcqlRKXYr1tU23a9wNFHQXgFFsXhdmd0Jr+Q8wthmymXsQ5qtcTPo82IRuF7aJUCbSCL5iKDMr4o4au8vpk/r5Q26n449jKIfGnSCzQU2t8Sa3Hi0GELrY0hLpXyqDMzNiDkH1EKMYBWaANKCXlLsDEHLW9ZleMCK44VJzxrLR/60IfYvHkz1WqVU089lf/0n/7TQa0811xzDevWraNarbJlyxYef/zxtuvs27ePK664gr6+PgYGBnj3u9/NzMzMSi+3wDLRKbLLvu90LmSbVpVboAsKpbzoOLXZVLAUaxOysYwStG3+3LDNNQatFSqQnk+vKe+OZxNUy/SwTutYLrKiiie2MB4S72QMNA0OAin7GR1NmyptSpjIP1CGKJtUVkR0xwxWnPA+/vGP86lPfYr/+l//Kw8//DAf//jH+cQnPsGNN96Yn/OJT3yCG264gZtuuomtW7fS3d3NZZddRq1Wy8+54oor+OlPf8qtt97KV7/6Vb773e9y5ZVXrvRyCwQsVbhYrItiqfP8k7meN/vRSzlUcAAOEZRRuWl6i4deVpENsy2U9lGi0fk212iFS2vYtLHoWpZfkFGABnEktTm08uTbFrlmcpncm6/5s++z9QQn+Hyj78MljxILrD5WXJby/e9/n7e+9a285S1vAWDTpk383d/9HXfffTfgo7vrr7+eD37wg7z1rW8F4POf/zyjo6N8+ctf5vLLL+fhhx/mlltu4Z577uGiiy4C4MYbb+TNb34zn/zkJxkfHz/ofX0zdz3/eWpqaqVv7bhEJ4nJcmUnC1u1Ol4j08flGjRBQp9pHEW+autc6FhQKKNC36oKDsjOb10BF6QgJlI4LDZtIEkdmzSIDvYPyNe17EIMYOt1atMH6FG+myKbWiYIRhsfXTpC5BqG+ORxpw2cKKCCiYANVeJsPYASaf5XoMBRxYpHeK985Su57bbbeOyxxwD48Y9/zJ133smb3vQmALZt28bExARbtmzJX9Pf38/FF1/MXXfdBcBdd93FwMBATnYAW7ZsQWvN1q1bO77vddddR39/f/7YsGHDSt/acYPlFC46dVUsr0jRURFHNh4xcy1GOUR53znCyEWl8BXcrFlfeyLxnRgxWvmeWesELRo0RDjSWs2TzBJYKEs5iKBDd+z89DS1qb2gdHBltj6KU14nGMUGE0e+K8QEi3myAd14gkcwGqJwvhNL7ltfYFWx4hHen/7pnzI1NcVZZ52FMQZrLR/5yEe44oorAJiYmABgdHS07XWjo6P5sYmJCUZGRtoXGkUMDQ3l5yzEBz7wAf7oj/4o/3lqaqogvReIxbopWo9DO5m0nq+CH55IU80WlcreLMBKXmxV+K9IGJ2oVB4SZbNos/dRWvucHU2SbMxPs0iA13Gti5EyCJO7J6BWR6rdiA0jGp0fD6nCNlYrCSSWFYyDS7MyJGmCaIXREVoLrgG1NEXaCK+I7lYLK054f//3f8/NN9/MF77wBc4991zuv/9+3ve+9zE+Ps473/nOlX67HOVymXLwHCtweFiK2Dq1ji3VbXHQ8wI+fnIogahc9YTnILU2CI59M1ZkDDry7sIoX6hAbN7FYLK5tuLHJqJCzm12ym+Bl6FvW6wfWML3u555EiMObRRxFKOVxgWzAucEHfk8o3dhFsTp/B7RiigyKPxoxshoGlZCp0b+Ls3N/ZKDhgocCaw44f3xH/8xf/qnf8rll18OwPnnn8/TTz/Nddddxzvf+U7GxsYA2LlzJ+vWrctft3PnTl7ykpcAMDY2xq5du9qum6Yp+/bty19f4IVhOXm6hed3Ir+Ff7StP/vCRBDcKkGJEJWrEJXA+u2gtSmRlMiLFNpvX310CC4YbKZZlKV8J4YDv310Fjs31YwMO6x7Offun7Ps3/4Uxvh71UHr5wf5OJQDa73lvDLa+6aIF0/7EY5hC641OIcTSFIJBY1sHUX+bjWx4jm8ubm5XJCZwRjjFfXA5s2bGRsb47bbbsuPT01NsXXrVi655BIALrnkEiYnJ7nvvvvyc26//Xacc1x88cUrveQCC9BJYNzpnMWONZGRisr/qUslolIZa/3vgwtfxQlasoptJlb2w6y1joii2FdHowjRGhNFaO2t1O3cTFPdvAx0jqwUNkmYev5ZTGyIIo02Km97QzmgpQASolIvhvbrMZGPSr0lvCF14ESjTWnRQeEFji5WPML71V/9VT7ykY+wceNGzj33XH70ox/x53/+5/zO7/wO4P9A3ve+9/HhD3+Y008/nc2bN/OhD32I8fFx3va2twFw9tln88Y3vpH3vOc93HTTTSRJwtVXX83ll1/esUJb4MihNRpauJ09VI4vV98pB/iWMROXiLu6Sfcr0sRBxQVvuWZF10dywRxUaZ8v06Ccj/ayKMoP8lG4hpemqFK5bd2HJ5yG2an9zB3Yw0DZYOISJo786MiWaxrj+2aVgEIjKiuqRH7CWQKkKRiFtt45uVQuo/C5SJUFvQVWBStOeDfeeCMf+tCH+P3f/3127drF+Pg4v/u7v8s111yTn/P+97+f2dlZrrzySiYnJ3n1q1/NLbfcQqVSyc+5+eabufrqq7n00kvRWvP2t7+dG264YaWXW2ABFtvuLTdnt+i2N8tdKUW5u5eGaJwjkF3mlOIHWmciD600Di8NAVA6TAPTGmVAi8bhcGkD25gnKi2dw126Ag3Tu3fjkgZxT5dvJcMbgJqWLWnWT+snmbnc+QWlwuQyjZimu4sTqHT1oLXxur6iwWJVseKE19vby/XXX8/111+/6DlKKa699lquvfbaRc8ZGhriC1/4wkovr8AiWK72Lvt6KKJb6prV/iGmTBSS974YoEILmnUOLQ6jo/waXpeXOScLPhPj82ZaGbAWV6+heuGFVED37njGFyy0wYT+XWNaLKiUz9dFkUFHwcxPNTsxEN+CloqDMH4yRVHqH1p0WUXh4uiiSCwUWBSdtoWdui46fV3sj1jwhCc6wgWXYLLOsLyLwYDS3lfOtrR1ae1tmXTkOy60RhRo7bD1ORZjlUMRilIKJTC9eweR0aHIsGDdbdKbEOHpMNMiiJNRXs1njI/mrAMVl+gaWENzWKXywuMCq4KC8E5gLEeA3Onnw82PtcIJlHsGUFHJa9zEk4Q39ySYbvr8nNHa/4w3DcgH7wQBnzaeADUO15h/Qeo25yxTe3ZSLsXenVipNg+7LHcHBDNQhygdqrh+2+urs0JaT7CpNwsVY+gaGCbfp/srvICVFnghKAjvBMdSEVun5xfrwOh0vcVQ7u5Flyq+myG0aIF4ca7ysyJcvtVzuWYtJzsI7Wp+lq0SSBuNFyj3EOanDxDFhlIpxkQRcRznYmMX3t85F352fgurvC+ejiJvjQJhe25w+ElolZ4+MgurTItXYHVQEF6BZaFThLdcomvtchAR4kqVqNrrp5JZwVkfEYlzPpLDeZ+8ENU1jUGDpi8ESIIOJAnOJmTGJD9P9Clpim3UMZGmVCphosh7e+rQRREMP5Xy7XCSdYgQjkURSrXYWylf6IirfUTVblq7aYsq7eqhILwCS2Kpre3Cn5dLNCqKqQ4M+wjPpqRJSmKdnwKGJxHPau1Oxk0ZjPEzIkJOj6znFdos2Q9nTTZNkDTxOrrIEJdLgPJGBzpEoVmU6bzg2Auq/ejGzAVGmZgojlEKUitE3f0++iuiumMCBeEVWBTLzeMttdXtRIiiFD1r12G1wTrric6Jdw4OpKIyXR7Naq+INE0FlN82ZlIXcXbJHW2n7Xrr904EhaCNwopDGZNLUySMXlQEuWB4zrupePlx9rMoFUTGglWG8tBIkLA0NSkLl1mYgR49FIRX4JB5vKWKFMtt52qFiNC3Zh0urvoYToRIK//LmImLRQOmWQGlWan11VAJjgPKV3KlvdnicCNPpRRovy3Vykd0SivK5XLbZ6LzPB15S1lmIaVEcA7fYqYiLDHdQ8EEI9uGK9VSsS1wtFEQXoFl43BazhaDBGbq7h/CdPdhXZjnmiaIs37wjVJNV+MQLeXXVaBU1t0RrinkU89a13A4ubwoijFxCXEQ68gLpRVkQ3qyAeGOrMAiKK0wUdP004l3anYipNaBiekZHA5ufgsYucCqoCC8AktiMVJbuJ09FBFCtjUEpxy61EX38Dqs81tIm9rgPecNQsU530oWihJNsgjDfER7mlEajO+zzaOoRYhlycJKFFGqdOW2dWKi3H1Zh04Lv532gyqyexEJ2sEoRoI2z4kmTRy60ku5dzAUV4DM0bnAqqEgvALA0lvAw90eHjInJQqnoG/dOpSOg6bNOx8DeTUUCBq9rMIL4hxasvdQRFEM6FwSstQaF3aGtN+jpqtvAIeiUW/gUocOfx6+b9eTnjHGa/9ynz7Jp5Zl56TWYoHqwDBxS7tktoZOqyvyeEcHBeEVWBSLEd1yI7qlrmcRetauQ8VdOPH5MJvavGiRe8hlW0oToZSPrPzkMofWfhi3yooCy1xLx3OUpmdwmNT61jIviWkKoRHJ+3q9fUG4jmRqmBbJjFKIiugaHs3Fy0X72LGBgvAKLBvLaTVbigilNekGlHoGMX1DpNabAog4T3TONc8NujYXqp9KKVDBM08FIa94ojwUp7TqATscpGtwmCRNscEgQJQnUvHM136vC/5yRJrdIBqwSlMdHqWQoxxbKAivQEccqgBwKOFxJiXpBG/+K2hdobp2jHoiJImPqNIkocl1qs1LyVdsBYIQ2VdvHQpHmtTb30S1XGMZEKDa3YuIkKYp2jSrwVmHhwq6v9ZrZm1mzrq8Pc5ah45iegbXckgWLnBUURBegRzLlZ4sJVlZjpxFCVjlyaF/dBNOR1hrSdMkSDxs0LU5/Gxavy30NlGZ00DgtGx+bNIIzwWzdxXOaCXPRZARc6najaBxzmLTBOdSsg0rEq4s3pnZef0xVnzhxDlPeKqRYsURdfVT7upZlu18gaOHgvAKLBuHIrqlorr216ugwXX0DI9gunpDHk/yjgnnmp0NKhQEsvYuREgbiY/28I7ErtFY4ELSZMVDrSuDzgwCVCiShFxhttXOt7d4MwEThellOrt/T4aJFSrDo5h4xd3XCrxAFIRX4LBwuFvb1ueaeTl8pRVQXRVK/WtIbXhCBGdT31frsi4GcKGVyw/L9pVTJPwCi9fx4Syims35mQRkoefcYhGrTRo+f2gdmRrGV2HD/NkW4tVat2n/RPxkslQJSpfoGx0PbWcHk+2iW/2iUnvEURBegTYspbtbKq+33M6M1j9qQRBt6B3bSOo0aTAQwDnEhdGGmSWTokUA7Id2RzobBJTgkgY4m1dLNYeWqbSuSSE0GvWQWwxE5ZpiYaUyE4EFkpmWLgtv4QxEFbqHRrOLL/neBY4uCsIrsCiWitYOR5vXKcLx4mFAFD0j67EmIrUWseLbuoLY2BNfiNqyqqzy3QwuRH9GKT/M2zk/2BuWZTm3cDh3o17Dd3Xo7ITQQ9E8NxMQZ9KU/LxQPXZOoardlLr7/VoKwjumUBBegcPGwi6L1u87PdcJroX0qn0DmK4+0tSSJEkzb+asL0A41xzUTVN+4pxFB/smcSkuDM1pKkGWJpv29SmSRt3Hh1oFNYzyRRPlOyW8wDgYlAZjUt+B4a3frQipE7oG1/oZuoQbpNiuHisoCK/AQVhOtXa5fbSd5lz4KCqIi3HoUkxpeJwkcdg0IU1SrLPhl9PLTrDOGwuoMCxHHDqYhgqgnCWxNvBLaDnLrE2WddMgScNbU6kwEc2oFlVMID+C5q5lKJFW3vhAnCMFeofH/fa7LQwscCygILwCy8Lh6vEWI0dZIC3JyKN/7RiJKGxqsUnqv4qEubUZsdlQMW3ZFoe8noifXtbUvYXt6iL3ka0l39YKuMT74TVJ2v95tLeSNXepzXv3HRZKNEqX6Fq71jumZEWWAscMin8bBZbESuXx2iCqLfARNL1rxrBxlUaakqapr8Aq71Dic3qSW0Mpo8MAnYxUFIgNWrzQ6JXl3Dq9/YLcnT9PSOv1fFpZ9uK82T/LI4rKSVaHzg9rLdY5rBXi7gHi3r6W6xbC42MJBeEV6IhDEdhSUd0hixgdnjPd/ZR6vTzFOYdLHEma+u4FaRoLINqTSCAcr2QRDGAbNZopPE9QGWEtKoLOnhNftFD+0rkdVFaFzWbrZsPB83cRwVqLVgaHojI8honK4bAr6O4YQ0F4BQ4Li8lPDs9IoDVq8oUIF5XoWTtGPXU06ilJvYFNU1xo27LO4WxTiKzwDiaZ+FchpI1ai/2Sar4PC7avndYojkajHubQevcVpRZq7UKfb9jCZltbY7zzciqKnpGxIInJ7OmLJN6xhILwChwSh6q+LkVynY9JnoPztnIKROgeXY+l3IzyfJ3C20M5CUUIAQndFQJg0ErjXEpaq6GVDTvPxZ3nOhdSoD4/g1IOEe3fqmVeBSFXl2nusnVnOUkrgouqdK3J9HeqoLpjEAXhFVgWOskqFmsvW0iQy7m2IHQNrMFUunFJQr3ewCUpzoatqWQDsLMB3FkeTYXZsApbr+W6ORVIdLF1d1gEjflZosgQxVGzENJmVyW+/SwrogTpjE1T0sQR9Q5Q6c76Z0NUuMhHUMhUVgcF4RVYFIcTxR1W0aIFzT98hS53URlc6zserJeiKJGQxyOQnqc0rXyyLnV+3qsTIZmdAtcUDi+klKVyeDapY+tz6MgQxd65GKXyKFQr306mjB8ipHWIIJ1DnCKxQs/IOrSOW963yOAdaygIr8BhYbE2s4XfL3zukG4l4v3netZtxCqDb13128WmnTq5XZMEIlTgNXMK7Px0y7jGTEC8+Pu3PteozeNq816Dp/34RXGC1sZbvLe4HocXBz2g5+ZURfSOrPd27nm/x3KMFAocTRSEV+DnxmKV2uX03h4MwYmle2QcV+7C2pTEpsF2yeUuJirk5jLSU+AH/wCSzGOThGaddvnbxkathm3UUdoXIPwcC58fzOQnzaKH9lZRThDrJ5VF1W66B9eEzWwoxggslskrtrSrg4LwCiyJ5URwy5ajLOIakldQRYh6eij3jWDDcG1xKeB8rk55E848S6c1TethhWvUSWpz7YNyDkGy4Uqkc3Mocd7gMz+kvQef8jlDwvtJaHdT4v3wrLVUBkaJytV8Lbn8+TB5rSDCI4uC8AocNjrl7BaL8BZrN2t9QFbxBNGGntEN1K1DbBoExCG6C4SjtfGtW9rLUgjuJkqEZH7Gb4eX3M42uzEyo9D5mSm0CJGJvDVVmrZIT1S+tUWy1wd34zTBiaJr7XggXyH7s5IXkNsscGRQEF6BnxvLrcYe3FrWAb4qgTjoXreexEXU6zVv+W59S5nKz2sh1NyNGBBHbWqS1ta1xdcUvuKNAubn5ny3hjhU8NzzPnje1h1p7dAwKAxWvG+f1SX6RsZyEfRiKKK31UdBeAVWBIcivkM7qYiP5kRR7hvCdA95M4EkJU0Sb85pU8TanM+c84Lk3BVZQePAZLs0ZUnmUyF4FNKk3nQ/MdnsWdd8fSCrzPhTEFyaYq1gegYp9w38nJ9cgaOJgvAKHBKH45KyHB1eZx1c1hsrqCiie91GEqdp1BMfRaUJadLApZ7wmk37LUSkoTF9ACc2l5R0hiz4zufh/BwNQYLRp4m8wDmnzdBmZm2KTVOUKKyO6F47jo5LzWu+gEiu2AIfWRSEV+CwsHCY9eIR22FeN+TT/OxX6BvfQMNpktT73GXRnWSzLpzkImTIIi9IazPYpBGaMpZaj8q3xgLBS8+3iWXrQTUdjqNg6+4POrT47XeKpm/dOBn5vtBta7HtPbIoCK/AsrBYkWKxY4fbbaGUQzsv91AC1cG1qGofjTSlUa/hGnVwqW/9wuILDj4qzEhLUODqpHOzQYgnLBzz6N8s/CNsZ0VpnE1bahn+asZE/i8ka2nLzEcFrE0RbYiqPXStGUGQw77nAkcfBeEVOCws5TpyqOrtoRC6a/3XqEz/upOxiSVJ6n6KWdIAW8fahjcODc7ERgdjTucgTanPzmTv3qZKaa5BWlJz/j2TRj2Miky967IIJo68RVV2bREk9dtZnM/hlYfWEVW624oabXM7iojtmEJBeAWWjcUiuU7ntT6yreDC89sH+nh34UCPWDT9G04hQfvcnbUoSVGSEGnBKHxFNUhUlIjPt1lLbXrSW8K3vE8b8WRkF7o5FJCmXuTsrAvBnDQVKBmc753FWZy1NJzQv/4UoBjH+IuCgvAKHDYWcxs5VKGi0/Gm6BjIm+4ViKMyuAbpHSJJgvzDJkhSxzXqSJqE7guvkfNkqVA4GtMHDimIlvZFeBMAa8OIRj+q0SZJXulVQeOcDQyy1pLGFXpGx/PING+DK3DMoiC8AoeNQ/XMLvbo9Jp2wiOYAwhKLBJF9G88DWsVSZKQ2JTGfJ10roZLEqLQ8qWMb+r3r0956pEH+X8++Un27927aESpskeQmNTma2jwzieNJIxdDPIWbVAhZBSxOJvinKVrzXriahUVemfbgshiW3tMoiC8AoeF5RoELNZnu+j18kNNcnAiDG04lYYpYRNHUk+xjQTXaGDr89ikgTbGS0lcihP/ePaJR/j4Rz/C5z7z331L2gL4LKEXGDuBT//1f+fLf/8lUAqXJtjE4iSbpWHBpSGi9FZQkjpqDvrWbwryF9VR7neogdsFER59FIRX4OfCYsYAi0lVOm1lF14rKyBIZrDphFJvH+W1J1FLEtK0gbMpNm0gLiVt1EL7GRhtUA5opExP7kOJY36+hh+z2B7lZZGdUprHHnuMa6/9Tzy1bRtJmoAL0peQe1QCkqT5A+sQa7GmRN/oST4RGALUTgS2kNyWQ3IFER45FIRX4OfGcvppF/6caecyB5JDbXkdmsFTz2M+SUiTGo1GHZs2cEkdbAJhNKNCo4yQujqzs3OMD6+lK+4ibSQHR1/is31aa+747nfZs3cP+6enaDQaOHGkaQJ4nR1WgtDZV4KdtTgc5f5hSj39oLy9fOuwnkMR1rJa7QocERSEV+CwsVxZSkZwS+X1Drn1Fc3A2Mm4rkHmZuZo1GqktVkkqSFJHY1Dae3nXjQSpqZneWb/DGvGNjC5bz+1+VqnGwAF8/Nz3Hf33SiEer3O9GyNRpoGDwCHMcpLU+LIe+4h3kpeHHfe82N+8pOfkDuuLHB+WU60V+DooyC8Ai8Yi7WWdXqu9Vg2DewQF0eXyvStP4351LuT2MY8LqlTn5slrdfxzsfCdM1xx4PPcsdPtzGb1DkwPcXevXvzS2Xv5Le08Ld/8zfcfddd9FQqCDA1V6OR2nByONsJaHDOYhsNsAkOzZPPTvDR//vPuPfurQi6o+9dQXDHHgrCK/CCsFiEdqiIbrEtb/Zc1qSvtGLPvn189ktfYe9MjenZGWbnZqnPzZHMz1OfnyNJUnZP1fjbW7/PZ265gz0zdVytRr1WZ+eOiTbxccZ2e/ft47/9t78kbdTp7ekmrlSpNSyzc3WsdWitibRBlJesuEbDC5/TlAMz83znrnuY3reL/+9/fIFaff6ofNYFXjgKxWSBnwuZpGThz52eb83fiUguRG7d/nWs9GqFpCl/+9lP8+07v0/X1EZedcYIpruCYwYdO/bNKR7a8TR//y/f5ZHtz1IPUpKZeo2Z2Vme2b6di17xMt86K007qO/dcSdPPv44a4b6Set10lIXU1YxNdNgdm4eK2BdShRVUMaQ6DrOOCwp23bs5slnn+Ok2hzn/O//G5HxounWIK/1ngocOygIr8CKo5XgvKdc584LID/HZYO2W66hteH6P/8vlJml1pjntnseot9YThtdgzMNtu3byQ+f28sz++vMzifoUgXdaODShPlajZmZaZ55+hlq8w3iSkzOSErxvTvuAJcyPz+H0ZrJRsodT+9m90yCdPWwZnye/u4ev1V1vsvDuZRaI+XBR7dRTyzT9QQigzaxr+YKeU9thsWIryDE1UFBeAV+biyM6hZub/Oh2csUImevy7+K8MzT2zj75DV0lSpMz9b49mMTbN02yZ6ZOWbF0N0/RKXahQD1RoXEWrAJjaTOnn272Tmxk8n9k4yOj3i7dsCmlicee4Secuxb1qIIpTUJhgd3TfLEt3/E9x5/ll968dm86PSTGOypEItlfnaWp5/dxQ8fegoTxawfX895552PUtpbUi2BguCODRSEV2BFsNhW9lCk10oEreeC39Ku37CBrqrm9a97Lffd/wA7ZmaoT88DGhUZdJpQ6eqmJFXiuXlM4gfvWGvZvXcPT217kq/84z/wnvf+bn7t2ZkZ9kw8T1+1xHStgVYQoTBisEYzh+Mnz+7hoR13MXBnN4NdVXqUYHDMpilTusp555zDKSeN0N3b0zIyaPmEtpwosMDKoyhaFFhxLLd40arJM8b4ATptuTzNps2nAcLm0zdyYOYAxhgiYwBBOUdaS0iThCgylEoRsdF5rnBubobHH/kp/+OLf8cz27eH9zI88/R26rOzVOOIktFoQGsBbVEKDAajI5QxNMSwa97y8P45frJniqem6iQmZmhggPrMPvr7+/zs3A5tZYeSphR28EcfBeEVeEFYjvi49blWkltMgNwqWznl1M3Y2hTnn7OJem0ejUJrP7cnEwnXajW0MVQrVbSJvEhYhN7YcNEZ63n+Z9u49ZZvsnvnLkRg6x13YLQhMopSXEKHXliXGQUo8VGfiYiMwYT309pQiiK6u3qIjTA80EMcV0lt6nWA1ub5yExyI8EludPgolYUBHd0UBBegaOC1m3rUn/4GbJjZ511LiquMjo0yPj4epy1PhpEE4nD2YT5+XlcaqlWKpi4hALGukpc9Suv4nXnnUJJGvz4hz/km1/7Z+rzs9z5g+8RGSiriL5ymd7YIKJRLjgZo3G6BMpHndlatdbEUUR/Txf1mQP80usupW94TShWdO4j7pTbLLB6KHJ4BV4wOuXvoP2P/VA5vGwb2lqtFXF09ZSorj2FfZM1XvnKC/nS//onuroGEeOwYsE5GvV5ZuZnGFm7lrWNHl461se//Vcvo0fV2V9PGR/u50c/+iHPPfs0Z551Fs8++zypc8TlmEFr2bRuPff8bCdz1mC0BS1o0RgMWisk8QOGtNaUopie7i56+iMufNVrUaaEc01jq4WfQUFwxxaKCK/AiqHT9vZQHRdLvUaA1MLLXvkq7v7RT3nNa15GpDVarN/SKkhFSJOE+oEDrCtp3v9vf42P/Pt/y+aNa4lKhkrZ8KKzNrN3904eevgRvva1r6O0oZakKCVE2nLGWD9veelZdKmUJBGsVYhYxHg3ldSmuEB45UqJuFJh7bqNrDvlLGzu49eOF2IPVWxvjxyKCK/AimIp4fHCKuwhEQbxbDh5Pb1Do1RNiYGuKrOJI3IgaUo1cpx/8hiXXXIRF1/4EgbXjVI7sJdkyufRIuClZ57Mt370GM8fmOW++39IV28PU7Pz9HXFVKpVlMAFGwcYH3wR39+2mx9v340TjcHPrkjSxA/u0Zruri5+84orePkrLkbF1eDlJxxGgbaI+lYRhx3hffe73+VXf/VXGR8fRynFl7/85bbjIsI111zDunXrqFarbNmyhccff7ztnH379nHFFVfQ19fHwMAA7373u5mZmWk75yc/+Qmvec1rqFQqbNiwgU984hOHf3cFjhqWitQWq9AuFu3lhCkOh8WJ4m2/8a857fSX0N/bx2BseNtFZ3LV217Nx3/vHfzpO3+FX3rpKXSVHZLUMFph4jLaRFjrGOnvZfPYMEpr9k9OUu0qMzM/j4nLlEsRIoJNUzYOlXn/b7yGv/w/L+edv3wx568bYCBydNFgMBLOHO6mN7aouES1d8Dn7sJalxoKebgEVxDikcNhR3izs7O8+MUv5nd+53f4jd/4jYOOf+ITn+CGG27gc5/7HJs3b+ZDH/oQl112GQ899BCVSgWAK664gh07dnDrrbeSJAnvete7uPLKK/nCF74AwNTUFG94wxvYsmULN910Ew888AC/8zu/w8DAAFdeeeULvOUCRxOdtHitx1qjv4XIdovKgVWO8uAa/vVv/x98+IP/F6e84ZWsX1NiuLfiXYhFg3UkjTo6iil391Hp7WN6vo44y3BvD6UoYnJyktNPP5VGIyFVxs++cCk2sZR6u1HOcdq6Ic498xQaqWV+bpq52hyJtZRNme8/9iw2Sb0tvLj2dYZbO5SurtDdrR4Om/De9KY38aY3vanjMRHh+uuv54Mf/CBvfetbAfj85z/P6OgoX/7yl7n88st5+OGHueWWW7jnnnu46KKLALjxxht585vfzCc/+UnGx8e5+eabaTQafPrTn6ZUKnHuuedy//338+d//ucF4f0CoFMHxmIFjE6vhZbIKbRrWeXt31996aVEH/8kDz0/wUkjp+IUOCtYK2AtNknQXV2YahfdA4NMT0/j6o51I8OUzVPMz84xODAIKCYbNdZXu0lTbynfVa1QqlYRbbD1GsY16O0q09tbYX5+jvp8nc3r1/re2ZYq82LSnALHHla0aLFt2zYmJibYsmVL/lx/fz8XX3wxd911FwB33XUXAwMDOdkBbNmyBa01W7duzc/5pV/6JUql5jT3yy67jEcffZT9+/d3fO96vc7U1FTbo8DRxVJ/6IfS5y32WqeCuYAXyjHQO8iFF72cmhW0KeFMhCqVSfERlndFFnSpSqmrh0p3D4IwNrKGOIpQKPr7BtDGMDkzRw3NXOowUYlSpZrr7ixgowpSKqNKVUSXQUX09nQxNNi35OdQFB2OXawo4U1MTAAwOjra9vzo6Gh+bGJigpGRkbbjURQxNDTUdk6na7S+x0Jcd9119Pf3548NGza88Bsq8HNjsehtIdlZ6yeAtSLX6bVMqnXOIc5/fekFL6G3txfREalTqDjGIVhnUc56gtQRcalKtbsfY2KMAq2gu7eHoeEhUEJqLTumZzlQS9EmplzuBjRKG0ylDHEEWVeH8qN6YmPo7+1d8c+oiAqPDo4bWcoHPvABDhw4kD+eeeaZ1V7SCYnFiheLEaAxps095aD5sWGimYQCRuJSzj77LMrlCk4pxHkzgEgpP+8izLB1zqGMoVLtwcRlTKkEShgaXsPo6CgqdFI8t28/042EUhxhlJe7eKsnRaRjNMpvl43CKnBRlSiuLLqdXey5wzle4MhhRQlvbGwMgJ07d7Y9v3PnzvzY2NgYu3btajuepin79u1rO6fTNVrfYyHK5TJ9fX1tjwLHFjpFeK09tJ0rtfmrQRTihE2bNuGsoBCcTWikfs6E9vN0cNaPWYxMiXK5RKXaBcoP5DnjjNM5af16Iq1BhPmGsGs+paZLOG0AjUtSFJ4wlShIvfA4jmJm5uo8te2Jg3ONBYn9QmBFCW/z5s2MjY1x22235c9NTU2xdetWLrnkEgAuueQSJicnue+++/Jzbr/9dpxzXHzxxfk53/3ud0mSJD/n1ltv5cwzz2RwcPCw1lTkU1YPnbZsi0lROhHGQvFu9ugZHqRhIt/+JcpHdSI4DMZE4BwaQZkIF1WZtoYfP/4z5hoJZ55xBt29PURRjFYacY4Hn9nJX3z1Tj75P7/N//z2j/jhwz9j1+Qc1lRwpoIToVFP2b5rltu3/hQVVQ5qHzuc37Pid3L1cNhV2pmZGZ544on8523btnH//fczNDTExo0bed/73seHP/xhTj/99FyWMj4+ztve9jYAzj77bN74xjfynve8h5tuuokkSbj66qu5/PLLGR8fB+Df/Jt/w3/8j/+Rd7/73fzJn/wJDz74IH/xF3/Bf/kv/2Vl7rrAEUUn8XGnP/KFZLcU6bW+PopKTNcd9z76MzaO9LGmr4tKWWGNQ0eKRtpgcnIPjzz1NN/dej8//MlDPLNrHw00o+PrqVSqlOIySdrAkTCbaJ7cP8uT+6f51qNP01utsnagnwvPO4+u7i52TTzLgQOT7J6q0TM4wqtmF7d0P2xxdYGjisMmvHvvvZfXv/71+c9/9Ed/BMA73/lOPvvZz/L+97+f2dlZrrzySiYnJ3n1q1/NLbfckmvwAG6++WauvvpqLr30UrTWvP3tb+eGG27Ij/f39/PNb36Tq666igsvvJA1a9ZwzTXXFJKUX0C0kl2nboulvrZGTtn3xhiUs4xu2MTtt32T6alJcEJXHIPy3RV9Q6MMDK/jxz95gP2Te5irz+OMQjuHpHUqlTKVUolGWkeU9hPK0GhliLRCTMy0jZmih6e27+SJbduxSUqlUiFKauzbs6ttXYvd48LPodP3S31uBVYeh014r3vd65YMyZVSXHvttVx77bWLnjM0NJSLjBfDi170Iu64447DXV6BYxwLyWEx8ltIdtlX5xxWwdi6cZSuULea2bl5dtSnsGkDoyPGdC+nnbOOtWt2MD0zQ2wdadIgcQl79+6nXKlSrZSZm5tFqRjQiHY4ZUAblI4wkaZUiZifn8eKQisFLiWxjl179nXslV24zX0hpFWIk48Mjpsq7WIofmlWB0tVLw+lw2sd37jQSso5hxUYXrMGY7I2NRV+kRUoQ71WR6EYXjuCMgZtDFobtDLs3rOHcrlCucvbwpucXAHEa1cAJUJtfp75Wg2xDgdYBy6x7N+zF5xr6589nN+zIoe3ejjuCa/AsYHlbGEX9tm2oq0wIML83ByRMSitUMpvR728RdFo1KnX64yMjhJHEWTRkvJ93JVyme4gSPaVXS998RPNvClpXIoxxpCmDRTB1FMczlr27d9LvTaf3diR//AKrBiOe8Ir/mt67OFQkV4niUpbpCfC/n37MMbkROVJUqMUNBoNpqamGB0ZpVKtBPEygGL/vn0opeju6fFR3YLcWvYoxWXSNMVamz8n4mikKTNT08xMz3iCXHBfRX7u2MZxT3gFjk10IrelCBCaxOdEmJqaahZAtEKFqFDE4Zzl2We3Y4yhv68fbQxKawTYs3cvSZLS3dMNiM/NdVhXd3f3giILOBFSmzI3P8+ePXtapj52NkUocOyhILwCRxyLkUHr19Zui6WQWTlNHZjy0ZcOxkw5OQIIBw4cYHZ2luHhYUqlUsjj+YllSaNBb08PPuengvNJVgWOiOOYwcEhPzPXZvlEhYhvhavX6+zaubOdLDvw23JJryDHo4eC8AocNSwlRG79/lARkk1T5manqFsXZCWZCacCJTixzM7PMjExwfj4enq7ezBKAYq9e/ZQr8/T090L6DDDwmHDLAulNJVylb7eXvbu3Yt1DhHCYB6hkaRYZ3n+uef8HAt/2U6mx8u6/+WcW2DlUBBegSOG5fxRLyxWLPZoPa+ROmpzc74FTABxJLaBddZ3XIij0aiz7WfbmJ6ewWiDNjGiDQcOTLF9+3YGhwZwypI4SyQC1mHCGnp7elm/fj2lShnrHDa1JElCkiRY5+iODYNqDuVSNArQizoed9ruFlg9FBbvBY4JHCqn16rfazTqzMzOUk8SbFpHxxEkMWIbpGmKVgqNY9eunTlRGaXYND7KxRecR3+1zLnnvohSHBE1Ev71a19M7+g4X7/nYQ7MJ+w/MMXDjz5GvV7Pq7vZljuKYlSaMF51zO/fQffwON5IPs3vY7H7OxwUOrwjgyLCK7Cq6BTlLVbA8BB6KoYzTxpipCpUlKBSRzmKUEoRRRFxFGOMJknqzMxO4STFKJjat4/TTj4Jnc5z4Usv4JWvfA39lRIvv/AcJudn2bFrJ/U0IYpj9u3ey64dO8AJ3V3d+dqMNkxMzXHHj59i9zPPIsqASlHotrUWhYtjE0WEV+CooDVCW+ocIB/Z2DrIOiMQ54Rkbo6XbF7HqWt7uePBp/jRI9up1eo450iSBGdT0jTxImWbAIJFMZsIn7zpc6jeEX7z3/w21/xff8bff3qEv//2Vr7z40eJy2WqcYzYlOnpKWq1GqlLSBs+esvWUrfwvYeeoW/0Xi4//2VEJYVynRN5Ra7u2EIR4RU4KlhIdot1Wiy6nQ1kopVi9/an2Xr/43zmK3fww0d+Rqo0GB/ZxZEhiiKUBggzJ8ShxBJpRWotn7/5ZnbueA6HY8NZ53DHTx5HiNAqRkSo12eZmZtiLm2Q2sxQVHvvPqNRNqFRq/P9Hz7Ad779HU92qnOPcOv3nY51QhEdHjkUhFfgmEDHzosFwl4RYWpqhr/5X1/h1vsfY9bFxFEF5bxsxCiFoHJLeK0VRmuU1jgE8DKW5597jn/55jeY3LOPr33jX6hbh38l2NRRTxKSRg1JrOdM5berxhiM1lgUqQi1uTq3fv0b7Hh2YkmCO9Q9Fzh6KAivwFHHoRL7+XGV/wMRIU1TPveZz/LY4z8jrnSj4xgXigpowWqHZygJujwN2qFEwIGgUCrCOeG7d97Brt27uTfzZVRgSUlTiySCSywKC0owGkxWiBXxvbWpRSmYOzDDTx94EHWIP6WCBI8NFIRX4Kjg5/1D9i7vvtd14rnnefqpbZSjiJJRaOXJTTnBWEE5wDksacjaCUp0/rxzFrAgjgcfepDv3nkHk/sn0WHL7JzFuoSUBKscVilPqMFq3gFWNPXUMVevk2CJqiUeeOQBbFrvWKw4nKptQXZHHkXRosAxjlC8sA4lEBuNKcVef5c4T22qQaJSRBzDsaHa1U8jtczN1RntraJixd4Djp37Ew7UUhyayekpvv7Nb4T3UDgUEOMEXNIgVkKsI7TR9Maa0YFuuiolSCxr+wfZeWCW+V3bmY0U8wcG2LdvirWja/NVH+4WtyC7o4OC8AocMayccYNgbYLR0N1VYXZ2lrn5GmiFsnXWmoQzNo2za8dzvOOXX8fpZ5/GvT99nP/xP/+Z3339y9mwbi3KCjum6vz327/Dtx58mFQ0Oyae9zbvIYLEJQwaxSvPOo2LzjqFkeFBHv3ZczSmD/Dmf/VLRGXDMz97hk0nnczs/CzPTExxx0+f4MlHHmb3np2MjK1tW/Vyo7uC7I4eCsIrcNSxmP37Qj2eiOSdFs759rGxkbU8v2MCtCJpzLNezfO2V57Pps1n8OBP7mfj6AhdpRLKWioaKpVunts9y2BPmU09Jf7Dr72BPVNT/PiZnWgNRmvqjRTEcMpgP//Hay/gwnPOxLmUhjNUTt3Ed+64k5KtM9AzQu+pmxFVZq6eMt2wzDXmeXjb40xM7OTc885blMxWSpBc4IWhyOEVOOroRAqdZCkZ2UUmolIuYyLDGWedQRxFSD2hOjfLpS87kz0H5ph4dgfnnraJnkoMtYRHHniEF515Kv+89RE+fPO3ufGftlIvVVnb08W6gUGq5QrDg0P+ukZRrUS8ZPN6Tl+zhvl9M+xOq/y3bzzA33z9x0SlAWZ27mZueo7Hn93Ngfk5/v5ffsD6DWsYHh1k7/793h10wf0sJVEpsDooIrwCq4pWAsiiulYYYxARSuUyXd1dDK8ZZu3YGnZNPM0F69ZyzumnMrPPIfYA++cd//ilb3H5G17FgBY2rlvL+NrNvPll56FIGBqoInHJD+22lpG1Y8zXZmD3buqNBjt37EYclCslZGqKDd2GytAAF529gUqPZe/kNE8/s5ux0X4uf/sv88933c3/uu37/MY7/jde/brXAEtPYyu2squPgvAKHFVkW9hOf/xZJ8PCwdxRFAFCuVKhXK1y3tln8ZMf/5A7n3yanu9UWFMu86JTNnLymrVc/utv5MDkPp7bN8lrXn4up6wdom/dGvRQhE0c3/rRY/xo29NsOvlkzj/vJeyf3EtPdw9PbnuKn+0/wJ75Bt09DTb1RLzkLRfTtWaURBzP7XseNTPHG9/8Szy143n+nxs/zTMH5jlp3cm89tWvo1SqsJC/CrI79lAQXoFVw2Jb26yVLOurdc5vF0uVLtaMjPGSlxp6qxVuvf27fOfJp0nrc9z64BNceNrJ9A8M8OMnnuLBJ55Hf+s+tlzoOKVLkSaGu3/yJP/v391CqWeAX/7lf8WGkzZTr8+za9ezDA8M8tMHHuDGb36Xf/uaV/CyUzegS1Xm65adk/v5/Dfu5O6Hn6B7sJf5WsKM01S6e3n5Ky5ieGQNzlmiqNR2H4vdY4HVg5Lj1AN9amqK/v5+Jicn6e/vX+3lnJBY+KvVadJXp9e0DuxpfUxNHmBuZoZavUFtapLndu7ge3feyT333Mfe3buxSQ2RQJJYIoSBripaCdO1OrUkobdvkNdfuoWXXvgKqpUetIZ6fZZ9e3fz8MMP8N3vfY/JnbsY7+vn5DX9pJKy40CNeQwNhIa1aNGsWbuWX379a7jk1a9ldGwdZ5x9NlEcFVHdKiH7ez9w4AB9fX2LnlcQXoEjhk6/Wsv5dVs4w8I5hzjH/Nw8tbkZkkZC2khoJA1qc7Ps3DnBI488zKOPPcqOHTuYmpoiTVPiuMTQ4BpqtTniSLNp0yZe8uKXsP6kk6lUqr7nVnlb+CRNmZ+fZd/ePTz++OM8+sij7JyYYL4xT7WrytDQGjTQP9DHGaefwZlnncPI6DhdPX30Dw6wbny8OfGsKFIcdRSEVxDequNwf7U6RYA58TmHTVPm52ZIk4Q0SX3kl1qStEHaaNCo16nV5pmvzVOv1UhtitYRUWSoVKqUyxWiuExkTD7iMYNzvpBhbUraaJCkKY1GgzRJAEUUxZTKJSrlMnG5RLlSoVTpIopLdPf20j/QR24ZT0F0RxvLJbwih1fgmEFGEguLGkopJOTFKtUqidFEUYS1DnFCWarY1HoCtOGrWBBBE8YwhnGOmaGnavFl9w7xgjgAH01mw4KUQDa6Ea0xUUQURURxhI4itImodlfb+n4Lsjt2cdwSXhYhTE1N5c8Vv4hHHy90A9Ga0/OklJIkDU9wVvyUMuv7Y30/bCArZwEwZLNrPdGJEiS4qyx8DxGFUt6ISlRGhy78z4Qda4o4cAkoa4nLQm2+Rq1WB4rfsdVC9nd+qN+345bw9u7dC8DGjRtXeSUFChQ4Wpienl4yhXXcEt7Q0BAA27dvP+FzeFNTU2zYsIFnnnlmyfzGiYDis2jiePosRITp6WnGx8eXPO+4JbxMvNrf3/8L/y9zpdDX11d8FgHFZ9HE8fJZLCewKXppCxQocMKgILwCBQqcMDhuCa9cLvNnf/ZnlMvl1V7KqqP4LJooPosmTsTP4rgVHhcoUKDAQhy3EV6BAgUKLERBeAUKFDhhUBBegQIFThgUhFegQIETBgXhFShQ4ITBcUt4f/mXf8mmTZuoVCpcfPHF3H333au9pBXFddddx8te9jJ6e3sZGRnhbW97G48++mjbObVajauuuorh4WF6enp4+9vfzs6dO9vO2b59O295y1vo6upiZGSEP/7jPyZN06N5KyuOj33sYyileN/73pc/dyJ9Fs899xy/9Vu/xfDwMNVqlfPPP5977703Py4iXHPNNaxbt45qtcqWLVt4/PHH266xb98+rrjiCvr6+hgYGODd7343MzMzR/tWVh5yHOKLX/yilEol+fSnPy0//elP5T3veY8MDAzIzp07V3tpK4bLLrtMPvOZz8iDDz4o999/v7z5zW+WjRs3yszMTH7O7/3e78mGDRvktttuk3vvvVde8YpXyCtf+cr8eJqmct5558mWLVvkRz/6kXzta1+TNWvWyAc+8IHVuKUVwd133y2bNm2SF73oRfKHf/iH+fMnymexb98+Ofnkk+W3f/u3ZevWrfLUU0/JN77xDXniiSfycz72sY9Jf3+/fPnLX5Yf//jH8mu/9muyefNmmZ+fz8954xvfKC9+8YvlBz/4gdxxxx1y2mmnyW/+5m+uxi2tKI5Lwnv5y18uV111Vf6ztVbGx8fluuuuW8VVHVns2rVLAPnOd74jIiKTk5MSx7F86Utfys95+OGHBZC77rpLRES+9rWvidZaJiYm8nM+9alPSV9fn9Tr9aN7AyuA6elpOf300+XWW2+V1772tTnhnUifxZ/8yZ/Iq1/96kWPO+dkbGxM/vN//s/5c5OTk1Iul+Xv/u7vRETkoYceEkDuueee/Jyvf/3ropSS55577sgt/ijguNvSNhoN7rvvPrZs2ZI/p7Vmy5Yt3HXXXau4siOLAwcOAE2XmPvuu48kSdo+h7POOouNGzfmn8Ndd93F+eefz+joaH7OZZddxtTUFD/96U+P4upXBldddRVvectb2u4ZTqzP4itf+QoXXXQR73jHOxgZGeGCCy7gr//6r/Pj27ZtY2Jiou2z6O/v5+KLL277LAYGBrjooovyc7Zs2YLWmq1btx69mzkCOO4Ib8+ePVhr235xAUZHR5mYmFilVR1ZOOd43/vex6te9SrOO+88ACYmJiiVSgwMDLSd2/o5TExMdPycsmO/SPjiF7/ID3/4Q6677rqDjp1In8VTTz3Fpz71KU4//XS+8Y1v8N73vpc/+IM/4HOf+xzQvJel/j4mJiYYGRlpOx5FEUNDQ79Qn0UnHLf2UCcSrrrqKh588EHuvPPO1V7KquCZZ57hD//wD7n11lupVCqrvZxVhXOOiy66iI9+9KMAXHDBBTz44IPcdNNNvPOd71zl1a0+jrsIb82aNRhjDqrA7dy5k7GxsVVa1ZHD1VdfzVe/+lW+9a1vcdJJJ+XPj42N0Wg0mJycbDu/9XMYGxvr+Dllx35RcN9997Fr1y5e+tKX+nkTUcR3vvMdbrjhBqIoYnR09IT5LNatW8c555zT9tzZZ5/N9u3bgea9LPX3MTY2xq5du9qOp2nKvn37fqE+i0447givVCpx4YUXctttt+XPOee47bbbuOSSS1ZxZSsLEeHqq6/mH/7hH7j99tvZvHlz2/ELL7yQOI7bPodHH32U7du355/DJZdcwgMPPND2y33rrbfS19d30B/NsYxLL72UBx54gPvvvz9/XHTRRVxxxRX59yfKZ/GqV73qIHnSY489xsknnwzA5s2bGRsba/sspqam2Lp1a9tnMTk5yX333Zefc/vtt+Oc4+KLLz4Kd3EEsdpVkyOBL37xi1Iul+Wzn/2sPPTQQ3LllVfKwMBAWwXuFx3vfe97pb+/X7797W/Ljh078sfc3Fx+zu/93u/Jxo0b5fbbb5d7771XLrnkErnkkkvy45kU4w1veIPcf//9csstt8jatWt/4aQYndBapRU5cT6Lu+++W6Ioko985CPy+OOPy8033yxdXV3yt3/7t/k5H/vYx2RgYED+8R//UX7yk5/IW9/61o6ylAsuuEC2bt0qd955p5x++umFLOVYxo033igbN26UUqkkL3/5y+UHP/jBai9pRYEfqnXQ4zOf+Ux+zvz8vPz+7/++DA4OSldXl/z6r/+67Nixo+06P/vZz+RNb3qTVKtVWbNmjfyH//AfJEmSo3w3K4+FhHcifRb/9E//JOedd56Uy2U566yz5K/+6q/ajjvn5EMf+pCMjo5KuVyWSy+9VB599NG2c/bu3Su/+Zu/KT09PdLX1yfvete7ZHp6+mjexhFB4YdXoECBEwbHXQ6vQIECBRZDQXgFChQ4YVAQXoECBU4YFIRXoECBEwYF4RUoUOCEQUF4BQoUOGFQEF6BAgVOGBSEV6BAgRMGBeEVKFDghEFBeAUKFDhhUBBegQIFThj8/zj2kPwh/YmsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 读取词典和验证集\n",
    "with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "vocab_idx2word = {idx:word for word,idx in vocab.items()}\n",
    "with open('../data/deepfashion/val_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 展示第12张图片，其对应的文本描述序号是60到64\n",
    "content_img = Image.open(data['IMAGES'][3])\n",
    "plt.imshow(content_img)\n",
    "\n",
    "print(' '.join([vocab_idx2word[word_idx] for word_idx in data['CAPTIONS'][16]]))\n",
    "print(content_img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch数据类，用于PyTorch DataLoader来按批次产生数据\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, vocab_path, split, captions_per_image=1, max_len=70, transform=None):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            dataset_path：json格式数据文件路径\n",
    "            vocab_path：json格式词典文件路径\n",
    "            split：train、val、test\n",
    "            captions_per_image：每张图片对应的文本描述数\n",
    "            max_len：文本描述包含的最大单词数\n",
    "            transform: 图像预处理方法\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'val', 'test'}\n",
    "        self.cpi = captions_per_image\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 载入数据集\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        # 载入词典\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.data['CAPTIONS'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 第i个文本描述对应第(i // captions_per_image)张图片\n",
    "        img = Image.open(self.data['IMAGES'][i // self.cpi]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caplen = len(self.data['CAPTIONS'][i])\n",
    "        caption = torch.LongTensor(self.data['CAPTIONS'][i]+ [self.vocab['<pad>']] * (self.max_len + 2 - caplen))\n",
    "        \n",
    "        return img, caption, caplen\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 批量读取数据\n",
    "\n",
    "利用刚才构造的数据集类，借助DataLoader类构建能够按批次产生训练、验证和测试数据的对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mktrainval(data_dir, vocab_path, batch_size, workers=4):\n",
    "    \"\"\"\n",
    "    data_dir: 数据集文件夹\n",
    "    vocab_path: 词典文件路径\n",
    "    batch_size: 批次大小\n",
    "    workers: 多线程加载数据的线程数\n",
    "    \"\"\"\n",
    "    train_tx = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_tx = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_set = ImageTextDataset(os.path.join(data_dir, 'train_data.json'), \n",
    "                                 vocab_path, 'train',  transform=train_tx)\n",
    "    valid_set = ImageTextDataset(os.path.join(data_dir, 'val_data.json'), \n",
    "                                 vocab_path, 'val', transform=val_tx)\n",
    "    test_set = ImageTextDataset(os.path.join(data_dir, 'test_data.json'), \n",
    "                                 vocab_path, 'test', transform=val_tx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_set, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "tensor([[112,   1,   2,   3,   4,  42,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,   1,  15,  16,  17,  18,  75,  14,   1,  20,   3,  18,  16,   5,\n",
      "          21,  14,   1,   9,  18,   8,  10,  17,   4,  44,  13,  14,  53,  18,\n",
      "          27,  55,  35,  36,  56,  14,   1,  31,  18,  32,  33,  34,  35,  36,\n",
      "          37,  14,  53,  18,  27,  55,  62,  63,  36,  64,  14, 113,   0,   0,\n",
      "           0,   0],\n",
      "        [112,   1,  40,  41,  80,  31,  26,   4,   6,  87,  88,  10,  67,   9,\n",
      "          18,   8,  14,   1,  86,  16,  17,  18,  11,  12,  14,  45,   4,  33,\n",
      "          71,  15,  14,  23,  31,  26,  33,  60,  95,   7,  29,   8,   9,  10,\n",
      "          11,  12,  13,  14,  53,  18,  27,  55,  35,  36,  56,  14, 113,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [112,   1,  57,  80,  47,  26,   4,   5,   6,   7,  67,   9,  18,  90,\n",
      "           7,  10,  17,   4,  11,  12,  13,  14,   1,  57,   4,  33,  46,  15,\n",
      "          14,  23,  47,  26,  33,   5,  52,   7,  29,  22,   9,  10,  30,  13,\n",
      "          14,  53,  18,  27,  55,  62,  63,  36,  64,  14,  53,  18,   3,  35,\n",
      "          36,  84,  14,  23,  47,  18,  32,  33,  34,  35,  36,  37,  14, 113,\n",
      "           0,   0],\n",
      "        [112, 113,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader   = mktrainval('../data/deepfashion', '../data/deepfashion/vocab.json', 4, 4)\n",
    "for i, (img, caption, caplen) in enumerate(train_loader):\n",
    "    print(img.shape)\n",
    "    print(caption)\n",
    "    print(caplen.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 定义模型\n",
    "\n",
    "ARCTIC模型是一个典型的基于注意力的编解码模型，其编码器为图像网格表示提取器，解码器为循环神经网络。解码器在每生成一个词时，都利用注意力机制考虑当前生成的词和图像中的哪些网格更相关。\n",
    "\n",
    "![ARCTIC的模型结构示意图](img/mt-cnn-attn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 图像编码器\n",
    "\n",
    "ARCTIC原始模型使用在ImageNet数据集上预训练过的分类模型VGG19作为图像编码器，VGG19最后一个卷积层作为网格表示提取层。而我们这里使用ResNet-101作为图像编码器，并将其最后一个非全连接层作为网格表示提取层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet101_Weights\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, finetuned=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        model = torchvision.models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        # ResNet-101网格表示提取器\n",
    "        self.grid_rep_extractor = nn.Sequential(*(list(model.children())[:-2]))\n",
    "        for param in self.grid_rep_extractor.parameters():\n",
    "            param.requires_grad = finetuned\n",
    "        \n",
    "    def forward(self, images):\n",
    "        out = self.grid_rep_extractor(images)   # 提取的是图片特征一堆数字\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "v = ImageEncoder()\n",
    "img = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "preds = v(img) # (1, 1000)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        \"\"\"\n",
    "        image_size: 图片大小\n",
    "        patch_size: 分块大小\n",
    "        num_classes: 分类数\n",
    "        dim: 每个分块的维度\n",
    "        depth: 深度\n",
    "        heads: 多头注意力的头数\n",
    "        mlp_dim: MLP的维度\n",
    "        pool: 池化方式\n",
    "        channels: 通道数\n",
    "        dim_head: 头的维度\n",
    "        dropout: dropout率\n",
    "        emb_dropout: 嵌入层的dropout率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 返回一个元组，图片大小分块大小\n",
    "        image_height, image_width = pair(image_size)  # 图片大小 224*224\n",
    "        patch_height, patch_width = pair(patch_size)  # 分块大小 16*16\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'  # 判断必须可分\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)  # 计算个数\n",
    "        patch_dim = channels * patch_height * patch_width  # 每一块多少维 3*16*16\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),  # 图分割 N 14*14 16*16*3\n",
    "            nn.LayerNorm(patch_dim),  # 层正则化\n",
    "            nn.Linear(patch_dim, dim), # 映射 16*16*3 到 \n",
    "            nn.LayerNorm(dim),  # 层正则化\n",
    "        )  # 分块\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.to_512 = nn.Linear(num_patches + 1, 512)\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        # print(x.shape)\n",
    "        b, n, _ = x.shape  # b is batch_size, n is the number of image patches\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.to_512(x)\n",
    "        x = x.transpose(1,2)\n",
    "        # print(x.shape)\n",
    "        # x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        # x_resized = F.interpolate(x.unsqueeze(0), size=(512, 1024), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # x = self.to_latent(x)\n",
    "        # return self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5099,  0.3826,  0.1037,  ...,  0.1582,  0.2710, -0.2528],\n",
      "         [ 0.3272, -0.4506, -0.1825,  ...,  0.0987, -0.1653, -0.5722],\n",
      "         [-0.1642,  0.1276,  0.9757,  ...,  0.2729, -0.8012, -0.6368],\n",
      "         ...,\n",
      "         [-0.4331, -0.4967,  0.6419,  ...,  0.4726, -0.4635, -0.0268],\n",
      "         [ 0.7185, -1.0780, -0.3117,  ..., -0.3616, -0.6201,  1.0325],\n",
      "         [ 0.3129,  0.5149,  0.1772,  ...,  0.7647,  0.4680,  0.0912]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 视觉Transformer作为图像编码器\n",
    "v = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 16,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 8,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "preds = v(img) # (1, 257, 1024)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 文本解码器\n",
    "\n",
    "ARCTIC原始模型使用结合注意力的LSTM作为文本解码器，我们这里使用结合注意力的GRU作为文本解码器，注意力评分函数采用的是加性注意力。下面给出加性注意力和解码器的具体实现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "加性注意力评分函数的具体形式为 $W_2^T{\\rm tanh}(W_1 [\\mathbf{q}_i; \\mathbf{k}_j])$ 。\n",
    "\n",
    "- 首先将权重 $W_1$ 依照查询q和键k的维度，相应地拆成两组权重，分别将单个查询和一组键映射到到注意力函数隐藏层表示空间；\n",
    "- 然后将二者相加得到一组维度为attn_dim的表示，并在经过非线性变换后，使用形状为(attn_dim, 1) 的权重 $W_2$ 将其映射为一组数值；\n",
    "- 再通过softmax函数获取单个查询和所有键的关联程度，即归一化的相关性分数；\n",
    "- 最后以相关性得分为权重，对值进行加权求和，计算输出特征。这里的值和键是同一组向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def  __init__(self, query_dim, key_dim, attn_dim):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            query_dim: 查询Q的维度\n",
    "            key_dim: 键K的维度\n",
    "            attn_dim: 注意力函数隐藏层表示的维度\n",
    "        \"\"\"\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.attn_w_1_q = nn.Linear(query_dim, attn_dim)\n",
    "        self.attn_w_1_k = nn.Linear(key_dim, attn_dim)\n",
    "        self.attn_w_2 = nn.Linear(attn_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        Q K V：Q和K算出相关性得分，作为V的权重，K=V\n",
    "        参数：\n",
    "            query: 查询 (batch_size, q_dim)\n",
    "            key_value: 键和值，(batch_size, n_kv, kv_dim)\n",
    "        \"\"\"\n",
    "        # （2）计算query和key的相关性，实现注意力评分函数\n",
    "        # -> (batch_size, 1, attn_dim)\n",
    "        queries = self.attn_w_1_q(query).unsqueeze(1)\n",
    "        # -> (batch_size, n_kv, attn_dim)\n",
    "        keys = self.attn_w_1_k(key_value)\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.attn_w_2(self.tanh(queries+keys)).squeeze(2) \n",
    "        # （3）归一化相关性分数\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.softmax(attn) \n",
    "        # （4）计算输出\n",
    "        # (batch_size x 1 x n_kv)(batch_size x n_kv x kv_dim)\n",
    "        # -> (batch_size, 1, kv_dim)\n",
    "        output = torch.bmm(attn.unsqueeze(1), key_value).squeeze(1)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "解码器前馈过程的实现流程如下：\n",
    "\n",
    "（1）将图文数据按照文本的实际长度从长到短排序，这是为了模拟pack_padded_sequence函数的思想，方便后面使用动态的批大小，以避免<pad>参与运算带来的非必要的计算消耗。\n",
    "\n",
    "![pack_padded_sequence函数的作用的示例图](img/cr-pack_padded_sequence-example.png)\n",
    "    \n",
    "\n",
    "（2）在第一时刻解码前，使用图像表示来初始化GRU的隐状态。\n",
    "\n",
    "（3）解码的每一时刻的具体操作可以分解为如下4个子操作：\n",
    "    \n",
    "- （3.1）获取实际的批大小；\n",
    "\n",
    "- （3.2）利用GRU前一时刻最后一个隐藏层的状态作为查询，图像表示作为键和值，获取上下文向量；\n",
    "\n",
    "- （3.3）将上下文向量和当前时刻输入的词表示拼接起来，作为GRU该时刻的输入，获得输出；\n",
    "\n",
    "- （3.4）使用全连接层和softmax激活函数将GRU的输出映射为词表上的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_featurestorch.Size([2, 512, 512])\n",
      "torch.Size([2, 9])\n",
      "[[52, 62, 33, 76, 5, 1, 95, 97], [84, 97, 13, 20, 39, 76, 33, 62]]\n",
      "There his ring sleeveless long The gentleman mixed\n",
      "trousers, mixed neckline denim tank sleeveless ring his\n",
      "Predictions:\n",
      "torch.Size([2, 8, 123])\n",
      "Targets:\n",
      "tensor([[2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [8, 7, 6, 5, 4, 3, 2, 1]])\n",
      "Lengths:\n",
      "tensor([8, 7])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term2 = torch.pow(torch.tensor(10000000.0), torch.arange(0, d_model, 2).float() / d_model)\n",
    "        div_term1 = torch.pow(torch.tensor(10000000.0), torch.arange(1, d_model, 2).float() / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term2)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term1)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, word_dim, num_layers, num_heads, mlp_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: 词汇表大小\n",
    "        word_dim: 词嵌入维度\n",
    "        num_layers: 解码器层数\n",
    "        num heads: 多头注意力的头数\n",
    "        mlp_dim: MLP的维度\n",
    "        dropout: dropout率\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, word_dim)\n",
    "        self.pos_embedding = nn.Embedding(vocab_size, word_dim)  # positional embedding\n",
    "        self.positional_encoding = PositionalEncoding(word_dim)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=word_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(word_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, image_features, captions, lengths):\n",
    "        batch_size, image_code_dim = image_features.size(0), image_features.size(2)\n",
    "        # -> (batch_size, grid_height*grid_width+1, image_code_dim) \n",
    "        # （1）按照caption的长短排序\n",
    "        sorted_cap_lens, sorted_cap_indices = torch.sort(lengths, 0, True)\n",
    "        captions = captions[sorted_cap_indices]\n",
    "        image_code = image_code[sorted_cap_indices]\n",
    "        return image_code, captions, sorted_cap_lens, sorted_cap_indices\n",
    "    \n",
    "    def forward_step(self, image_features, captions_emb, lengths, step):\n",
    "        \"\"\"\n",
    "        image_features: 图像特征 (batch_size, patch_num+1, dim)\n",
    "        captions: 当前生成的部分标注序列 (batch_size, current_length)\n",
    "        lengths: 标注序列的实际长度 (batch_size, )\n",
    "        step: 当前解码的步骤\n",
    "        \"\"\"\n",
    "        # 与 forward 方法类似，但只处理当前步骤的信息\n",
    "        cap_embeds = captions_emb[:, :step]  # 使用传入的嵌入表示\n",
    "        # cap_embeds = self.positional_encoding(cap_embeds)\n",
    "\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(cap_embeds.size(1)).to(cap_embeds.device)\n",
    "        \n",
    "        output = cap_embeds.transpose(0, 1)\n",
    "        memory = image_features.transpose(0, 1)\n",
    "        for i in range(len(self.decoder_layers)):\n",
    "            output = self.decoder_layers[i](output, memory, mask)\n",
    "        \n",
    "        output = output[-1, :, :]\n",
    "        output = self.fc(self.dropout(output))\n",
    "        return output\n",
    "    \n",
    "    def forward(self, image_features, captions, lengths):\n",
    "        \n",
    "        # 同样是将图文数据按照文本的实际长度从长到短排序\n",
    "        # image_features,captions,sorted_cap_lens, sorted_cap_indices = self.init_hidden_state(image_features, captions, lengths)\n",
    "        # batch_size = image_features.size(0)\n",
    "        # lengths = sorted_cap_lens.cpu().numpy() - 1\n",
    "        # # 初始化变量：模型的预测结果和注意力分数\n",
    "        # predictions = torch.zeros(batch_size, lengths[0], self.fc.out_features).to(captions.device)\n",
    "        # image_features: (batch_size, patch_num    ,dim)\n",
    "        # captions: (batch_size, num_steps) 图片标注\n",
    "        # lengths: (batch_size, ) 图片标注的长度\n",
    "        cap_embeds = self.embed(captions) # (batch_size, num_steps, word_dim) num_steps是caption的长度,就是有多少单词,编码\n",
    "        # print(cap_embeds)\n",
    "        # cap_embeds = self.positional_encoding(cap_embeds)\n",
    "        # print(f'cap_embeds{cap_embeds.shape}')\n",
    "        # for step in range(lengths[0]):\n",
    "            \n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(cap_embeds.size(1)).to(captions.device)  # 对角掩码\n",
    "        # print(f'mask{mask}')\n",
    "        output = cap_embeds.transpose(0,1)\n",
    "        memory = image_features.transpose(0,1)\n",
    "        for layer in self.decoder_layers:\n",
    "            # print(f'output{output.shape}')\n",
    "            # print(f'mask{mask.shape}')\n",
    "            # print(f'memory{memory.shape}')\n",
    "            output = layer(output, memory, mask)  # Use image_features as memory\n",
    "        output = output.transpose(0, 1)\n",
    "        softmax_predictions = torch.softmax(output, dim=-1)\n",
    "        _, predicted_indices = torch.max(softmax_predictions, dim=-1)\n",
    "        # print(predicted_indices)\n",
    "        predictions = self.fc(self.dropout(output))  # [1,5,123] 5个预测，每个123个维度\n",
    "        # print(f'predictions{predictions.shape}')\n",
    "        return predictions[:, :-1, :], captions[:, 1:], lengths - 1  # 不要最后一个\n",
    "\n",
    "\n",
    "# Create a TransformerDecoder instance\n",
    "with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "decoder = TransformerDecoder(vocab_size=len(vocab), word_dim=512,  num_layers=1, num_heads=8,mlp_dim=2048, dropout=0.0)\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 16,\n",
    "    num_classes = 1000,\n",
    "    dim = 512,\n",
    "    depth = 8,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "# 测试\n",
    "img = torch.randn(2, 3, 224, 224)  \n",
    "image_features = v(img)\n",
    "print(f'image_features{image_features.shape}')\n",
    "captions = torch.tensor([[1,2,3,4,5,6,7,8,9],[9,8,7,6,5,4,3,2,1]])# Assume there are two samples, each with 5 words\n",
    "print(captions.shape)\n",
    "lengths = torch.tensor([9,8])  # Actual lengths of each sample is 5\n",
    "\n",
    "# Perform forward propagation\n",
    "predictions, targets, lengths = decoder(image_features, captions, lengths)\n",
    "softmax_predictions = torch.softmax(predictions, dim=-1)\n",
    "_, predicted_indices = torch.max(softmax_predictions, dim=-1)\n",
    "predicted_indices_list = predicted_indices.tolist()\n",
    "print(predicted_indices_list)\n",
    "generated_sentences(predicted_indices_list)\n",
    "# Print the results\n",
    "print(\"Predictions:\")\n",
    "print(predictions.shape)\n",
    "print(\"Targets:\")\n",
    "print(targets)\n",
    "print(\"Lengths:\")\n",
    "print(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "# img = torch.randn(2, 3, 224, 224)  \n",
    "# image_features = v(img)\n",
    "# memory = torch.rand(2, 10, 512)\n",
    "# tgt = torch.rand(2, 20, 512)\n",
    "# mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(captions.device)  # 对角掩码\n",
    "# predictions = decoder_layer(tgt, image_features,mask)\n",
    "# softmax_predictions = torch.softmax(predictions, dim=-1)\n",
    "# _, predicted_indices = torch.max(softmax_predictions, dim=-1)\n",
    "# print(predicted_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 10, 512]) torch.Size([2, 3, 512]) torch.Size([2])\n",
      "Predictions shape: tensor([[[ 0.2157, -0.5912,  0.8828,  0.3853, -1.3461],\n",
      "         [ 0.0052,  0.5477,  0.8303, -0.7688, -0.0792]],\n",
      "\n",
      "        [[ 0.5313, -1.8097, -0.3040,  0.7992, -1.5627],\n",
      "         [ 1.9759, -1.1722, -0.4999,  1.1196, -1.0108]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "decoder = TransformerDecoder(vocab_size=vocab_size, word_dim=512, num_layers=6, num_heads=8, mlp_dim=2048, dropout=0.1)\n",
    "\n",
    "# 创建一个简单的模拟数据\n",
    "batch_size = 2\n",
    "max_seq_length = 3\n",
    "word_dim = 512\n",
    "\n",
    "image_features = torch.randn(batch_size, 10, word_dim)  # (batch_size, num_patches+1, word_dim)\n",
    "captions = torch.randint(1, vocab_size, (batch_size, max_seq_length))  # (batch_size, max_seq_length)\n",
    "lengths = torch.randint(2, max_seq_length, (batch_size,))  # (batch_size,)\n",
    "print(captions.shape)\n",
    "# 将captions嵌入\n",
    "captions_emb = decoder.embed(captions)\n",
    "print(image_features.shape,captions_emb.shape,lengths.shape)\n",
    "# 进行前向传播\n",
    "predictions = []\n",
    "for i in range(max_seq_length - 1):  # 逐步预测每个词\n",
    "    output = decoder.forward_step(image_features, captions_emb, lengths, i+1)  # 第i+1步的预测\n",
    "\n",
    "    predictions.append(output.unsqueeze(1))  # 将每步的预测添加到列表中\n",
    "\n",
    "predictions = torch.cat(predictions, dim=1)  # (batch_size, max_seq_length-1, vocab_size)\n",
    "print(\"Predictions shape:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ARCTIC模型\n",
    "\n",
    "在定义编码器和解码器完成之后，我们就很容易构建图像描述模型ARCTIC了。仅需要在初始化函数时声明编码器和解码器，然后在前馈函数实现里，将编码器的输出和文本描述作为解码器的输入即可。\n",
    "\n",
    "这里我们额外定义了束搜索采样函数，用于生成句子，以计算BLEU值。下面的代码详细标注了其具体实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class VitTransformerDecoder(nn.Module):\n",
    "    def __init__(self, image_code_dim, vocab, word_dim, num_layers):\n",
    "        super(VitTransformerDecoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder = ViT(\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                num_classes=1000,\n",
    "                dim=image_code_dim,\n",
    "                depth=num_layers,\n",
    "                heads=4,\n",
    "                mlp_dim=2048,\n",
    "                dropout=0.0,\n",
    "                emb_dropout=0.0\n",
    "            )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=len(vocab),\n",
    "            word_dim=word_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=4,\n",
    "            mlp_dim=2048,\n",
    "            dropout=0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, image, captions, lengths):\n",
    "        \"\"\"\n",
    "        image: (batch_size, 3, 224, 224)\n",
    "        captions: (batch_size, num_steps) 图片标注\n",
    "        lengths: (batch_size, ) 图片标注的长度\n",
    "        \"\"\"\n",
    "        image_features = self.encoder(image)\n",
    "        predictions, targets, lengths = self.decoder(image_features, captions, lengths)\n",
    "        return predictions, targets, lengths\n",
    "    \n",
    "    # 束搜索\n",
    "    def generate_by_beamsearch(self, images, beam_k, max_len):\n",
    "        vocab_size = len(self.vocab)\n",
    "        image_codes = self.encoder(images)  # 图片特征\n",
    "        # print(f'image_codes{image_codes.shape}')  #([2, 512, 1024])\n",
    "        texts = []\n",
    "        device = images.device\n",
    "        # 对每个图像样本执行束搜索\n",
    "        for image_code in image_codes:  # 对于每张图片\n",
    "            # 将图像表示复制k份\n",
    "            image_code = image_code.unsqueeze(0).repeat(beam_k,1,1,1)\n",
    "            # 生成k个候选句子，初始时，仅包含开始符号<start>\n",
    "            cur_sents = torch.full((beam_k, 1), self.vocab['<start>'], dtype=torch.long).to(device)  # 初始状态\n",
    "            # print(f'cur_sents{cur_sents.shape}')\n",
    "            cur_sent_embed = self.decoder.embed(cur_sents)  # embed编码\n",
    "            sent_lens = torch.LongTensor([1]*beam_k).to(device)  # 当前长度只有1\n",
    "            # 存储已生成完整的句子（以句子结束符<end>结尾的句子）\n",
    "            end_sents = []\n",
    "            # 存储已生成完整的句子的概率\n",
    "            end_probs = []\n",
    "            # 存储未完整生成的句子的概率\n",
    "            probs = torch.zeros(beam_k, 1).to(device)\n",
    "            k = beam_k\n",
    "            step= 1\n",
    "            while True:\n",
    "                image_code = image_code.squeeze(1)\n",
    "                # print(f'image_codes[k]{image_code[:k].shape}')\n",
    "                # print(f'cur_sent_embed{cur_sent_embed.shape}')\n",
    "                preds = self.decoder.forward_step(image_code[:k], cur_sent_embed,0,step=step)  # image_features, captions, lengths, i+1\n",
    "                # print(f'print{preds.shape}') # 只预测一个词[5, 123]\n",
    "                step+=1\n",
    "                # -> (k, vocab_size)\n",
    "                preds = nn.functional.log_softmax(preds, dim=1)  # 转化成概率分布\n",
    "                \n",
    "                # 对每个候选句子采样概率值最大的前k个单词生成k个新的候选句子，并计算概率\n",
    "                # -> (k, vocab_size)\n",
    "                probs = probs.repeat(1,preds.size(1)) + preds\n",
    "                # print(f'prob{probs.shape}')\n",
    "                if cur_sents.size(1) == 1:\n",
    "                    # 第一步时，所有句子都只包含开始标识符，因此，仅利用其中一个句子计算topk\n",
    "                    values, indices = probs[0].topk(k, 0, True, True)\n",
    "                    # print(f'values{values.shape},indices{indices.shape}')\n",
    "                else:\n",
    "                    # probs: (k, vocab_size) 是二维张量\n",
    "                    # topk函数直接应用于二维张量会按照指定维度取最大值，这里需要在全局取最大值\n",
    "                    # 因此，将probs转换为一维张量，再使用topk函数获取最大的k个值\n",
    "                    values, indices = probs.view(-1).topk(k, 0, True, True)\n",
    "                    # print(f'values{values.shape},indices{indices.shape}')\n",
    "                # 计算最大的k个值对应的句子索引和词索引\n",
    "                sent_indices = torch.div(indices, vocab_size, rounding_mode='trunc') \n",
    "                # print(f'sent_indices{sent_indices.shape}')\n",
    "                word_indices = indices % vocab_size \n",
    "                # 将词拼接在前一轮的句子后，获得此轮的句子\n",
    "                # print(f'原来cur_sent{cur_sents.shape}')\n",
    "                cur_sents = torch.cat([cur_sents[sent_indices], word_indices.unsqueeze(1)], dim=1)\n",
    "                # print(f'后来cur_sent{cur_sents.shape}')\n",
    "                # 查找此轮生成句子结束符<end>的句子\n",
    "                end_indices = [idx for idx, word in enumerate(word_indices) if word == self.vocab['<end>']]\n",
    "                if len(end_indices) > 0:\n",
    "                    end_probs.extend(values[end_indices])\n",
    "                    end_sents.extend(cur_sents[end_indices].tolist())\n",
    "                    # 如果所有的句子都包含结束符，则停止生成\n",
    "                    k -= len(end_indices)\n",
    "                    if k == 0:\n",
    "                        break\n",
    "                # 查找还需要继续生成词的句子\n",
    "                cur_indices = [idx for idx, word in enumerate(word_indices) \n",
    "                               if word != self.vocab['<end>']]\n",
    "                if len(cur_indices) > 0:\n",
    "                    cur_sent_indices = sent_indices[cur_indices]\n",
    "                    cur_word_indices = word_indices[cur_indices]\n",
    "                    # 仅保留还需要继续生成的句子、句子概率、隐状态、词嵌入\n",
    "                    cur_sents = cur_sents[cur_indices]\n",
    "                    probs = values[cur_indices].view(-1,1)\n",
    "                    cur_sent_embed = self.decoder.embed(\n",
    "                        cur_sents)\n",
    "                # 句子太长，停止生成\n",
    "                if cur_sents.size(1) >= max_len:\n",
    "                    break\n",
    "            if len(end_sents) == 0:\n",
    "                # 如果没有包含结束符的句子，则选取第一个句子作为生成句子\n",
    "                gen_sent = cur_sents[0].tolist()\n",
    "            else: \n",
    "                # 否则选取包含结束符的句子中概率最大的句子\n",
    "                gen_sent = end_sents[end_probs.index(max(end_probs))]\n",
    "            texts.append(gen_sent)\n",
    "        return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "[[121, 114, 20, 104, 109, 107, 81, 109, 107, 81, 109, 107, 81, 118, 99, 10, 16, 10, 16, 10, 16, 10, 16, 10, 16, 80, 71, 53, 44, 105], [121, 114, 20, 104, 109, 107, 81, 109, 107, 81, 109, 107, 81, 109, 107, 81, 118, 99, 10, 16, 10, 16, 109, 107, 81, 109, 107, 81, 118, 91], [121, 114, 4, 114, 20, 104, 109, 95, 80, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 64, 116, 91, 59, 10, 16, 10], [121, 114, 27, 4, 114, 20, 104, 109, 95, 113, 20, 20, 104, 109, 95, 80, 79, 20, 20, 20, 20, 20, 104, 109, 107, 81, 109, 107, 81, 109]]\n",
      "Generated text 1: solid is denim It belt short-sleeve pair three-quarter cotton, stand man square leggings glasses knitting, <start>\n",
      "Generated text 2: solid is denim three-quarter off, cotton, stand square leggings glasses knitting, <start>\n",
      "Generated text 3: has solid is denim clothing, three-point long-sleeve pair off, gentleman stand leggings glasses or <start>\n",
      "Generated text 4: has denim clothing, this pair three-quarter gentleman stand square leggings His glasses <start>\n",
      "<start> glasses denim stand leggings square three-quarter leggings square three-quarter leggings square three-quarter knitting, cotton, solid is solid is solid is solid is solid is pair short-sleeve belt It man\n",
      "<start> glasses denim stand leggings square three-quarter leggings square three-quarter leggings square three-quarter leggings square three-quarter knitting, cotton, solid is solid is leggings square three-quarter leggings square three-quarter knitting, off,\n",
      "<start> glasses has glasses denim stand leggings gentleman pair clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, clothing, long-sleeve or off, three-point solid is solid\n",
      "<start> glasses clothing, has glasses denim stand leggings gentleman His denim denim stand leggings gentleman pair this denim denim denim denim denim stand leggings square three-quarter leggings square three-quarter leggings\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 生成随机图像和标注\n",
    "batch_size = 4\n",
    "image_size = 224\n",
    "num_channels = 3\n",
    "num_classes = 1000\n",
    "images = torch.randn(batch_size, num_channels, image_size, image_size)\n",
    "\n",
    "# 假设你的字典是一个包含一些随机单词的词汇表\n",
    "with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# 假设最大长度为10\n",
    "max_len = 30\n",
    "\n",
    "# 创建模型实例\n",
    "model = VitTransformerDecoder(image_code_dim=1024, vocab=vocab, word_dim=1024, num_layers=6)\n",
    "\n",
    "# 使用束搜索生成文本\n",
    "beam_k = 4  # 选择束宽度\n",
    "print(images.shape)\n",
    "generated_texts = model.generate_by_beamsearch(images, beam_k, max_len)\n",
    "print(generated_texts)\n",
    "# 打印生成的文本\n",
    "for i, text in enumerate(generated_texts):\n",
    "    words = [key for key, value in vocab.items() if value in text]\n",
    "    print(f\"Generated text {i + 1}: {' '.join(words)}\")\n",
    "generated_sentences(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[81, 48, 67], [81, 48, 67], [81, 48, 67], [81, 48, 67]]\n",
      "[[], [1, 73, 79, 50, 24, 16, 14, 59, 19], [1, 51, 58, 22, 24, 16, 14, 5, 19], [1, 40, 16, 28, 7, 8, 9, 15, 4, 32, 17, 13]]\n",
      "[[81, 48, 67], [81, 48, 67], [81, 48, 67], [81, 48, 67], [81, 48, 67], [81, 48, 67], [81, 48, 67], [81, 48, 67]]\n",
      "[[], [1, 73, 79, 50, 24, 16, 14, 59, 19], [1, 51, 58, 22, 24, 16, 14, 5, 19], [1, 40, 16, 28, 7, 8, 9, 15, 4, 32, 17, 13], [21, 50, 24, 32, 39, 39, 49, 28, 10, 11, 12], [1, 40, 16, 28, 7, 8], [52, 16, 25, 54, 34, 35, 55], [1, 39, 49, 16, 28, 7, 8]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, caps, caplens) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(imgs.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(caps)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(caplens)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# 通过束搜索，生成候选文本\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         texts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_by_beamsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# 候选文本\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         cands\u001b[38;5;241m.\u001b[39mextend([filter_useless_words(text, filterd_words) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts])\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mVitTransformerDecoder.generate_by_beamsearch\u001b[0;34m(self, images, beam_k, max_len)\u001b[0m\n\u001b[1;32m     61\u001b[0m image_code \u001b[38;5;241m=\u001b[39m image_code\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# print(f'image_codes[k]{image_code[:k].shape}')\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print(f'cur_sent_embed{cur_sent_embed.shape}')\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_code\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_sent_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# image_features, captions, lengths, i+1\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(f'print{preds.shape}') # 只预测一个词[5, 123]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 70\u001b[0m, in \u001b[0;36mTransformerDecoder.forward_step\u001b[0;34m(self, image_features, captions_emb, lengths, step)\u001b[0m\n\u001b[1;32m     68\u001b[0m memory \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers)):\n\u001b[0;32m---> 70\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]\n\u001b[1;32m     73\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output))\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/transformer.py:577\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m--> 577\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    578\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/transformer.py:594\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    593\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 594\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/functional.py:5066\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5065\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5066\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/functional.py:4753\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4752\u001b[0m             b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m-> 4753\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (linear(q, w_q, b_q),) \u001b[38;5;241m+\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_kv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4755\u001b[0m     w_q, w_k, w_v \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def filter_useless_words(sent, filterd_words):\n",
    "    # 去除句子中不参与BLEU值计算的符号\n",
    "    return [w for w in sent if w not in filterd_words]\n",
    "\n",
    "model = VitTransformerDecoder(image_code_dim=1024, vocab=vocab, word_dim=1024, num_layers=6)\n",
    "model.eval()\n",
    "# 存储候选文本\n",
    "cands = []\n",
    "# 存储参考文本\n",
    "refs = []\n",
    "# 需要过滤的词\n",
    "filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']})\n",
    "cpi = 5 # 每张图片的标注数\n",
    "device = next(model.parameters()).device\n",
    "for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "    # print(imgs.shape)\n",
    "    # print(caps)\n",
    "    # print(caplens)\n",
    "    with torch.no_grad():\n",
    "        # 通过束搜索，生成候选文本\n",
    "        texts = model.generate_by_beamsearch(imgs.to(device), 4, 20)\n",
    "        # 候选文本\n",
    "        cands.extend([filter_useless_words(text, filterd_words) for text in texts])\n",
    "        print(cands)\n",
    "        # 参考文本\n",
    "        refs.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()])\n",
    "        print(refs)\n",
    "print(\"Candidate texts:\")\n",
    "for text in cands:\n",
    "    print(text)\n",
    "\n",
    "print(\"\\nReference texts:\")\n",
    "for text in refs:\n",
    "    print(text)\n",
    "# 实际上，每个候选文本对应cpi条参考文本\n",
    "multiple_refs = []\n",
    "for idx in range(len(refs)):\n",
    "    multiple_refs.append(refs[(idx//cpi)*cpi : (idx//cpi)*cpi+cpi])\n",
    "# 计算BLEU-4值，corpus_bleu函数默认weights权重为(0.25,0.25,0.25,0.25)\n",
    "# 即计算1-gram到4-gram的BLEU几何平均值\n",
    "bleu4 = corpus_bleu(multiple_refs, cands, weights=(0.25,0.25,0.25,0.25))\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 定义损失函数\n",
    "\n",
    "这里采用了最常用的交叉熵损失作为损失函数。由于同一个训练批次里的文本描述的长度不一致，因此，有大量的不需要计算损失的<pad>目标。为了避免计算资源的浪费，这里先将数据按照文本长度排序，再利用pack_padded_sequence函数将预测目标为\\<pad\\>的数据去除，最后再利用交叉熵损失计算实际的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_featurestorch.Size([2, 512, 512])\n",
      "torch.Size([2, 9])\n",
      "[[110, 78, 111, 83, 34, 4, 47, 54], [73, 54, 96, 4, 34, 83, 111, 78]]\n",
      "chiffon, hat furry sleeves on has pants accessory\n",
      "shorts accessory other has on sleeves furry hat\n",
      "Predictions:\n",
      "torch.Size([2, 8, 123])\n",
      "Targets:\n",
      "tensor([[2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [8, 7, 6, 5, 4, 3, 2, 1]])\n",
      "Lengths:\n",
      "tensor([8, 7])\n"
     ]
    }
   ],
   "source": [
    "# Create a TransformerDecoder instance\n",
    "with open('../data/deepfashion/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "decoder = TransformerDecoder(vocab_size=len(vocab), word_dim=512,  num_layers=1, num_heads=8,mlp_dim=2048, dropout=0.0)\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 16,\n",
    "    num_classes = 1000,\n",
    "    dim = 512,\n",
    "    depth = 8,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "# 测试\n",
    "img = torch.randn(2, 3, 224, 224)  \n",
    "image_features = v(img)\n",
    "print(f'image_features{image_features.shape}')\n",
    "captions = torch.tensor([[1,2,3,4,5,6,7,8,9],[9,8,7,6,5,4,3,2,1]])# Assume there are two samples, each with 5 words\n",
    "print(captions.shape)\n",
    "lengths = torch.tensor([9,8])  # Actual lengths of each sample is 5\n",
    "\n",
    "# Perform forward propagation\n",
    "predictions, targets, lengths = decoder(image_features, captions, lengths)\n",
    "softmax_predictions = torch.softmax(predictions, dim=-1)\n",
    "_, predicted_indices = torch.max(softmax_predictions, dim=-1)\n",
    "predicted_indices_list = predicted_indices.tolist()\n",
    "print(predicted_indices_list)\n",
    "generated_sentences(predicted_indices_list)\n",
    "# Print the results\n",
    "print(\"Predictions:\")\n",
    "print(predictions.shape)\n",
    "print(\"Targets:\")\n",
    "print(targets)\n",
    "print(\"Lengths:\")\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8215, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            predictions：按文本长度排序过的预测结果\n",
    "            targets：按文本长度排序过的文本描述\n",
    "            lengths：文本长度\n",
    "        \"\"\"\n",
    "        predictions = pack_padded_sequence(predictions, lengths, batch_first=True)[0]\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True)[0]\n",
    "        return self.loss_fn(predictions, targets)\n",
    "loss = PackedCrossEntropyLoss()\n",
    "loss = loss(predictions, targets, lengths)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 123]) torch.Size([2, 8]) tensor([8, 7])\n",
      "torch.Size([2, 8, 123])\n",
      "tensor([[2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [8, 7, 6, 5, 4, 3, 2, 1]])\n",
      "tensor([8, 7])\n",
      "tensor(5.8215, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UnsortedPackedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnsortedPackedCrossEntropyLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            predictions：未排序的预测结果\n",
    "            targets：未排序的文本描述\n",
    "            lengths：文本长度\n",
    "        \"\"\"\n",
    "        print(predictions.shape)\n",
    "        print(targets)\n",
    "        print(lengths)\n",
    "        batch_size = predictions.size(0)\n",
    "        max_length = predictions.size(1)\n",
    "\n",
    "        # Flatten predictions and targets\n",
    "        predictions_flat = predictions.reshape(-1, predictions.size(2))  # [batch_size * max_length, vocab_size]\n",
    "        targets_flat = targets.reshape(-1)  # [batch_size * max_length]\n",
    "\n",
    "        # Mask out padding positions\n",
    "        mask = torch.arange(max_length, device=predictions.device)[None, :] < lengths[:, None]\n",
    "        mask_flat = mask.view(-1)  # [batch_size * max_length]\n",
    "        # Apply the mask to predictions and targets\n",
    "        predictions_masked = predictions_flat[mask_flat]\n",
    "        targets_masked = targets_flat[mask_flat]\n",
    "\n",
    "        # Calculate loss for each position\n",
    "        loss = self.loss_fn(predictions_masked, targets_masked)\n",
    "\n",
    "        return loss.mean()  # Return mean loss over non-padded elements\n",
    "loss = UnsortedPackedCrossEntropyLoss()\n",
    "print(predictions.shape,targets.shape,lengths)\n",
    "loss = loss(predictions, targets, lengths)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 选择优化方法\n",
    "\n",
    "这里选用Adam优化算法来更新模型参数，由于数据集较小，训练轮次少，因此，学习速率在训练过程中并不调整。但是对编码器和解码器采用了不同的学习速率。具体来说，预训练的图像编码器的学习速率小于需要从头开始训练的文本解码器的学习速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "def get_optimizer(model, config):\n",
    "    return torch.optim.Adam([{\"params\": filter(lambda p: p.requires_grad, model.encoder.parameters()), \n",
    "                              \"lr\": config.encoder_learning_rate},\n",
    "                             {\"params\": filter(lambda p: p.requires_grad, model.decoder.parameters()), \n",
    "                              \"lr\": config.decoder_learning_rate}])\n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    \"\"\"\n",
    "        每隔lr_update个轮次，学习速率减小至当前十分之一，\n",
    "        实际上，我们并未使用该函数，这里是为了展示在训练过程中调整学习速率的方法。\n",
    "    \"\"\"\n",
    "    optimizer.param_groups[0]['lr'] = config.encoder_learning_rate * (0.1 ** (epoch // config.lr_update))\n",
    "    optimizer.param_groups[1]['lr'] = config.decoder_learning_rate * (0.1 ** (epoch // config.lr_update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 评估指标\n",
    "\n",
    "这里借助nltk库实现了图像描述中最常用的评估指标BLEU值，需要注意的是，再调用计算BLEU值之前，要先将文本中人工添加的文本开始符、结束符和占位符去掉。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def filter_useless_words(sent, filterd_words):\n",
    "    # 去除句子中不参与BLEU值计算的符号\n",
    "    return [w for w in sent if w not in filterd_words]\n",
    "\n",
    "def evaluate(data_loader, model, config):\n",
    "    model.eval()\n",
    "    # 存储候选文本\n",
    "    cands = []\n",
    "    # 存储参考文本\n",
    "    refs = []\n",
    "    # 需要过滤的词\n",
    "    filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']})\n",
    "    cpi = config.captions_per_image # 每张图片的标注数\n",
    "    device = next(model.parameters()).device\n",
    "    for i, (imgs, caps, caplens) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            # 通过束搜索，生成候选文本\n",
    "            texts = model.generate_by_beamsearch(imgs.to(device), config.beam_k, config.max_len+2)\n",
    "            print(texts)\n",
    "            # 候选文本\n",
    "            cands.extend([filter_useless_words(text, filterd_words) for text in texts])\n",
    "            # 参考文本\n",
    "            refs.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()])\n",
    "\n",
    "    # 实际上，每个候选文本对应cpi条参考文本\n",
    "    multiple_refs = []\n",
    "    for idx in range(len(refs)):\n",
    "        multiple_refs.append(refs[(idx//cpi)*cpi : (idx//cpi)*cpi+cpi])\n",
    "    # 计算BLEU-4值，corpus_bleu函数默认weights权重为(0.25,0.25,0.25,0.25)\n",
    "    # 即计算1-gram到4-gram的BLEU几何平均值\n",
    "    bleu4 = corpus_bleu(multiple_refs, cands, weights=(0.25,0.25,0.25,0.25))\n",
    "    model.train()\n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 2; 11.91 GiB total capacity; 10.94 GiB already allocated; 86.94 MiB free; 11.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m caplens \u001b[38;5;241m=\u001b[39m caplens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# 2. 前馈计算\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m predictions, targets, lengths \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaplens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 3. 计算损失\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# captions从第2个词开始为targets\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 33\u001b[0m, in \u001b[0;36mVitTransformerDecoder.forward\u001b[0;34m(self, image, captions, lengths)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mimage: (batch_size, 3, 224, 224)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mcaptions: (batch_size, num_steps) 图片标注\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mlengths: (batch_size, ) 图片标注的长度\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(image)\n\u001b[0;32m---> 33\u001b[0m predictions, targets, lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, targets, lengths\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 101\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, image_features, captions, lengths)\u001b[0m\n\u001b[1;32m     96\u001b[0m memory \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# print(f'output{output.shape}')\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# print(f'mask{mask.shape}')\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# print(f'memory{memory.shape}')\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use image_features as memory\u001b[39;00m\n\u001b[1;32m    102\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m softmax_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/transformer.py:577\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m--> 577\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    578\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/transformer.py:594\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    593\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 594\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/functional.py:5066\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5065\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5066\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/functional.py:4753\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4752\u001b[0m             b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m-> 4753\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (linear(q, w_q, b_q),) \u001b[38;5;241m+\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_kv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4755\u001b[0m     w_q, w_k, w_v \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 2; 11.91 GiB total capacity; 10.94 GiB already allocated; 86.94 MiB free; 11.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 单卡训练\n",
    "# 设置模型超参数和辅助变量\n",
    "config = Namespace(\n",
    "    max_len = 30,\n",
    "    captions_per_image = 5,\n",
    "    batch_size = 32,\n",
    "    image_code_dim = 1024,\n",
    "    word_dim = 1024,\n",
    "    hidden_size = 512,\n",
    "    num_layers = 8,\n",
    "    encoder_learning_rate = 0.001,\n",
    "    decoder_learning_rate = 0.005,\n",
    "    num_epochs = 10,\n",
    "    grad_clip = 5.0,\n",
    "    alpha_weight = 1.0,\n",
    "    evaluate_step = 1000, # 每隔多少步在验证集上测试一次\n",
    "    checkpoint = None, # 如果不为None，则利用该变量路径的模型继续训练\n",
    "    best_checkpoint = '../model/Transformer/best_flickr8k.ckpt', # 验证集上表现最优的模型的路径\n",
    "    last_checkpoint = '../model/Transformer/last_flickr8k.ckpt', # 训练完成时的模型的路径\n",
    "    beam_k = 5\n",
    ")\n",
    "\n",
    "# 设置GPU信息\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# 数据\n",
    "data_dir = '../data/deepfashion/'\n",
    "vocab_path = '../data/deepfashion/vocab.json'\n",
    "train_loader, valid_loader, test_loader = mktrainval(data_dir, vocab_path, config.batch_size)\n",
    "\n",
    "# 模型\n",
    "with open(vocab_path, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# 随机初始化 或 载入已训练的模型\n",
    "start_epoch = 0\n",
    "checkpoint = config.checkpoint\n",
    "if checkpoint is None:\n",
    "    model = VitTransformerDecoder(image_code_dim=config.image_code_dim, vocab=vocab, word_dim=config.word_dim, num_layers=config.num_layers)\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model = checkpoint['model']\n",
    "\n",
    "# 优化器\n",
    "optimizer = get_optimizer(model, config)\n",
    "\n",
    "# 将模型拷贝至GPU，并开启训练模式\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = loss_function()\n",
    "\n",
    "best_res = 0\n",
    "print(\"开始训练\")\n",
    "fw = open('logtrans.txt', 'w')\n",
    "\n",
    "for epoch in range(start_epoch, config.num_epochs):\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # 1. 读取数据至GPU\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "\n",
    "        # 2. 前馈计算\n",
    "        predictions, targets, lengths = model(imgs, caps, caplens)\n",
    "        # 3. 计算损失\n",
    "        # captions从第2个词开始为targets\n",
    "        print(predictions.shape)\n",
    "        print(targets.shape)\n",
    "        loss = loss_function(targets,predictions)\n",
    "        # 重随机注意力正则项，使得模型尽可能全面的利用到每个网格\n",
    "        # 要求所有时刻在同一个网格上的注意力分数的平方和接近1\n",
    "\n",
    "        loss.backward()\n",
    "        # 梯度截断\n",
    "        if config.grad_clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        # 4. 更新参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('epoch %d, step %d: loss=%.2f' % (epoch, i+1, loss.cpu()))\n",
    "            fw.write('epoch %d, step %d: loss=%.2f \\n' % (epoch, i+1, loss.cpu()))\n",
    "            fw.flush()\n",
    "\n",
    "        state = {\n",
    "                'epoch': epoch,\n",
    "                'step': i,\n",
    "                'model': model,\n",
    "                'optimizer': optimizer\n",
    "                }\n",
    "        if (i+1) % config.evaluate_step == 0:\n",
    "            bleu_score = evaluate(valid_loader, model, config)\n",
    "            # 5. 选择模型\n",
    "            if best_res < bleu_score:\n",
    "                best_res = bleu_score\n",
    "                torch.save(state, config.best_checkpoint)\n",
    "            torch.save(state, config.last_checkpoint)\n",
    "            fw.write('Validation@epoch, %d, step, %d, BLEU-4=%.2f\\n' % \n",
    "                  (epoch, i+1, bleu_score))\n",
    "            fw.flush()\n",
    "            print('Validation@epoch, %d, step, %d, BLEU-4=%.2f' % \n",
    "                  (epoch, i+1, bleu_score))\n",
    "checkpoint = torch.load(config.best_checkpoint)\n",
    "model = checkpoint['model']\n",
    "bleu_score = evaluate(test_loader, model, config)\n",
    "print(\"Evaluate on the test set with the model that has the best performance on the validation set\")\n",
    "print('Epoch: %d, BLEU-4=%.2f' % \n",
    "      (checkpoint['epoch'], bleu_score))\n",
    "fw.write('Epoch: %d, BLEU-4=%.2f' % \n",
    "      (checkpoint['epoch'], bleu_score))\n",
    "fw.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 训练模型\n",
    "\n",
    "训练模型过程仍然是分为读取数据、前馈计算、计算损失、更新参数、选择模型五个步骤。\n",
    "\n",
    "模型训练的具体方案为一共训练30轮，编码器和解码器的学习速率分别为0.0001和0.0005。\n",
    "\n",
    "<!-- 模型训练的具体方案为一共训练30轮，初始编码器和解码器的学习速率分别为0.0001和0.0005，每10轮将学习速率变为原数值的1/10。 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码完成训练，最后一行会输出在验证集上表现最好的模型在测试集上的结果，具体如下：\n",
    "\n",
    "```\n",
    "Epoch: 4, BLEU-4=0.23\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  },
  "vscode": {
   "interpreter": {
    "hash": "9c90bab175f6ebea40ddbaa491ed6726e7c53d0208272849c54b7fb296566795"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
